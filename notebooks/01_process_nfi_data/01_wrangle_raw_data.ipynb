{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling NFI Raw Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú Libraries and Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NFI Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NFI Data\n",
    "nfi_raw = get_latest_nfi_raw_data()\n",
    "nfi_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df to change\n",
    "nfi_final_data = nfi_raw.copy()\n",
    "\n",
    "# Get variable information from excel sheet\n",
    "sheet_nfi_org = pd.concat(\n",
    "    [\n",
    "        get_feature_database_sheet(\"NFI Original Variables\"),\n",
    "        get_feature_database_sheet(\"NFI Derivatives\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fixing specific variables by hand -------------------------------------------\n",
    "# Replace where integr is in [4L, 6A, 6H] with [4, 6, 6]\n",
    "nfi_final_data[\"integr\"] = (\n",
    "    nfi_final_data[\"integr\"].astype(str).replace([\"4L\", \"6A\", \"6H\"], [\"4\", \"6\", \"6\"])\n",
    ")\n",
    "\n",
    "# * Fix order of, 0 is lowest and 1 is highest, 4 second lowest)\n",
    "nfi_final_data[\"dc_1\"] = nfi_final_data[\"dc_1\"].replace(\n",
    "    [0, 1, 2, 3, 4], [0, 4, 3, 2, 1]\n",
    ")\n",
    "nfi_final_data[\"dc_2\"] = nfi_final_data[\"dc_2\"].replace(\n",
    "    [0, 1, 2, 3, 4], [0, 4, 3, 2, 1]\n",
    ")\n",
    "\n",
    "# * For dep, remove leading 0s\n",
    "nfi_final_data[\"dep\"] = (\n",
    "    nfi_final_data[\"dep\"].astype(str).replace(r\"^0+\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# * tpespar variables...\n",
    "# For tpespar1_1, remove leading 0s\n",
    "nfi_final_data[\"tpespar1_1\"] = (\n",
    "    nfi_final_data[\"tpespar1_1\"].astype(str).replace(r\"^0+\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# For tpespar1_2, remove leading 0s\n",
    "nfi_final_data[\"tpespar1_2\"] = (\n",
    "    nfi_final_data[\"tpespar1_2\"].astype(str).replace(r\"^0+\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# For tpespar2_1, remove leading 0s\n",
    "nfi_final_data[\"tpespar2_1\"] = (\n",
    "    nfi_final_data[\"tpespar2_1\"].astype(str).replace(r\"^0+\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# For tpespar2_2, remove leading 0s\n",
    "nfi_final_data[\"tpespar2_2\"] = (\n",
    "    nfi_final_data[\"tpespar2_2\"].astype(str).replace(r\"^0+\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# * For dcespar1, remove leading 0s\n",
    "nfi_final_data[\"dcespar1\"] = (\n",
    "    nfi_final_data[\"dcespar1\"].astype(str).replace(r\"^0+\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# * For dcespar2, remove leading 0s\n",
    "nfi_final_data[\"dcespar2\"] = (\n",
    "    nfi_final_data[\"dcespar2\"].astype(str).replace(r\"^0+\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# * Fixes for 2022 data update\n",
    "# * For pentexp, pcalc, pox, videplant\n",
    "nfi_final_data[\"pentexp\"] = nfi_final_data[\"pentexp\"].replace(\"X\", -9999).astype(float)\n",
    "\n",
    "nfi_final_data[\"videplant\"] = (\n",
    "    nfi_final_data[\"videplant\"].replace(\"X\", -9999).astype(float)\n",
    ")\n",
    "nfi_final_data[\"pox\"] = (\n",
    "    nfi_final_data[\"pox\"].replace([\"X\", \"\"], [-9999, -9999]).astype(float)\n",
    ")\n",
    "nfi_final_data[\"pcalc\"] = (\n",
    "    nfi_final_data[\"pcalc\"].replace([\"X\", \"\"], [-9999, -9999]).astype(float)\n",
    ")\n",
    "\n",
    "# Fix variable formatting and do gapfilling of categorical variables ----------\n",
    "vec_cats = []\n",
    "vec_ords = []\n",
    "vec_dates = []\n",
    "vec_nums = []\n",
    "\n",
    "for col in sorted(nfi_final_data.columns):\n",
    "    if col in sheet_nfi_org[\"var\"].tolist():\n",
    "        if sheet_nfi_org.query(f\"var == @col\")[\"type\"].iloc[0] == \"cat\":\n",
    "            # Fix weird behaviour of some values being x and x.0\n",
    "            nfi_final_data[col] = (\n",
    "                nfi_final_data[col].astype(str).replace(r\"\\.0\", \"\", regex=True)\n",
    "            )\n",
    "            # Gapfilling missing values\n",
    "            nfi_final_data[col] = nfi_final_data[col].replace(\"nan\", \"Missing\")\n",
    "            # Set to category\n",
    "            nfi_final_data[col] = nfi_final_data[col].astype(\"category\")\n",
    "            vec_cats.append(col)\n",
    "\n",
    "        elif sheet_nfi_org.query(f\"var == @col\")[\"type\"].iloc[0] == \"date\":\n",
    "            nfi_final_data[col] = pd.to_datetime(nfi_final_data[col], errors=\"coerce\")\n",
    "            vec_dates.append(col)\n",
    "\n",
    "        elif sheet_nfi_org.query(f\"var == @col\")[\"type\"].iloc[0] == \"ord\":\n",
    "            # nfi_final_data[col] = nfi_final_data[col].fillna(-9999)\n",
    "            vec_ords.append(col)\n",
    "\n",
    "        else:\n",
    "            nfi_final_data[col] = pd.to_numeric(nfi_final_data[col], errors=\"coerce\")\n",
    "            vec_nums.append(col)\n",
    "\n",
    "\n",
    "# Print information ----------------------------------------------------------\n",
    "display(\"--- Variable Encoding: ---\")\n",
    "print(f\" - Set to category (plus setting NA to 'missing'): \\t{vec_cats}\")\n",
    "print(f\" - Set to ordinal (plus setting NA to -9999): \\t\\t{vec_ords}\")\n",
    "print(f\" - Set to datetime: \\t\\t\\t\\t\\t{vec_dates}\")\n",
    "print(f\" - Set to numeric: \\t\\t\\t\\t\\t{vec_nums}\")\n",
    "\n",
    "print(\"Shape of nfi_final_data:\", nfi_final_data.shape)\n",
    "print(\"Number of invidiual sites:\", nfi_final_data[\"idp\"].nunique())\n",
    "\n",
    "display(\"--- Columns in nfi_final_data: ----\")\n",
    "for col in nfi_final_data.columns:\n",
    "    print(f\" - {col}\")\n",
    "\n",
    "display(f\"--- Shape of nfi_final_data: {nfi_final_data.shape} ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split tree and site variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NFI variables that are on the tree level\n",
    "tree_vars = (\n",
    "    get_feature_database_sheet(\"NFI Original Variables\")\n",
    "    .query(\"level == 'tree'\")[\"var\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "site_vars = (\n",
    "    get_feature_database_sheet(\"NFI Original Variables\")\n",
    "    .query(\"level == 'location'\")[\"var\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Make sure idp is in there and remove duplicates through set\n",
    "tree_vars = list(set([\"idp\", \"tree_id\", \"campagne_1\"] + tree_vars))\n",
    "site_vars = list(set([\"idp\", \"campagne_1\"] + site_vars))\n",
    "\n",
    "# TODO: ADDING VARIABLES HERE THAT SHOULD BE ADDED SEPARATELY WHEN MOVING FROM R TO PYTHON\n",
    "site_vars = site_vars + [\"lat_fr\", \"lon_fr\", \"lat\", \"lon\"]\n",
    "tree_vars = tree_vars + [\n",
    "    \"tree_state_1\",\n",
    "    \"tree_state_2\",\n",
    "    \"tree_state_change\",\n",
    "    \"species_lat\",\n",
    "    \"genus_lat\",\n",
    "    \"espar_red\",\n",
    "    \"shadow_growth\",\n",
    "    \"tree_class\",\n",
    "    \"family_lat\",\n",
    "    \"order_lat\",\n",
    "]\n",
    "\n",
    "# Subset vars that are actually in the data (removing automatically added _1 _2 suffixes)\n",
    "tree_vars = [x for x in tree_vars if x in nfi_final_data.columns]\n",
    "site_vars = [x for x in site_vars if x in nfi_final_data.columns]\n",
    "\n",
    "# Separate tree and location variables\n",
    "nfi_tree_raw = nfi_final_data[tree_vars].reset_index(drop=True)\n",
    "nfi_site_raw = nfi_final_data[site_vars].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Move idp to first position\n",
    "nfi_tree_raw.insert(0, \"idp\", nfi_tree_raw.pop(\"idp\"))\n",
    "nfi_site_raw.insert(0, \"idp\", nfi_site_raw.pop(\"idp\"))\n",
    "\n",
    "# Report shapes\n",
    "print(\"\\n --- Tree Variables ---\")\n",
    "print(f\" - Number of variables: {len(tree_vars)}\")\n",
    "print(f\" - Number of observations: {nfi_tree_raw.shape[0]}\")\n",
    "print(f\" - Number of unique trees: {nfi_tree_raw['tree_id'].nunique()}\")\n",
    "print(f\" - Number of unique sites: {nfi_tree_raw['idp'].nunique()}\")\n",
    "print(\"\\n --- Site Variables ---\")\n",
    "print(f\" - Number of variables: {len(site_vars)}\")\n",
    "print(f\" - Number of observations: {nfi_site_raw.shape[0]}\")\n",
    "print(f\" - Number of unique sites: {nfi_site_raw['idp'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data has site duplicates that need to be cleaned up\n",
    "# Set idp to integer for easier handling\n",
    "nfi_site_raw[\"idp\"] = nfi_site_raw[\"idp\"].astype(\"int32\")\n",
    "# Get idp duplicates\n",
    "idp_dupes = (\n",
    "    nfi_site_raw[nfi_site_raw.duplicated(\"idp\", keep=False)]\n",
    "    .sort_values(\"idp\")\n",
    "    .replace(\"Missing\", np.nan)\n",
    ")\n",
    "print(f\" - Number of unique sites: {nfi_site_raw['idp'].nunique()}\")\n",
    "print(f\" - Number of observations: {nfi_site_raw.shape[0]}\")\n",
    "print(f\" - Number of duplicate sites: {idp_dupes['idp'].nunique()}\")\n",
    "print(f\" - Number of observations in duplicate sites: {idp_dupes.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often, lat lon NA are cause for duplicates. Check this:\n",
    "dupes = idp_dupes[\"idp\"].nunique()\n",
    "coord_na_dupes = idp_dupes[idp_dupes[\"lat\"].isna()].shape[0]\n",
    "\n",
    "if dupes == coord_na_dupes:\n",
    "    print(\n",
    "        \"‚úÖ Duplicates are caused by lat/lon NA, merging them in the next cells, so no problem!\"\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"‚ùó‚ùó‚ùó Duplicates are not caused by lat/lon NA, check this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Most duplicates are due to missing values, so we can just merge them\n",
    "# Define function to merge\n",
    "def merge_rows(group):\n",
    "    merged_values = []\n",
    "\n",
    "    for col in group.columns:\n",
    "        unique_values = group[col].dropna().unique()\n",
    "\n",
    "        if len(unique_values) == 0:\n",
    "            merged_values.append(pd.NA)\n",
    "        elif len(unique_values) > 1:\n",
    "            print(\n",
    "                f\"\\nWarning: Multiple non-NA values in {col} for Group {group['idp'].iloc[0]} - Values: {unique_values} - Chosen value: {unique_values[0]} \",\n",
    "            )\n",
    "            merged_values.append(unique_values[0])\n",
    "        else:\n",
    "            merged_values.append(unique_values[0])\n",
    "\n",
    "    return pd.Series(merged_values, index=group.columns)\n",
    "\n",
    "\n",
    "# Apply function\n",
    "idp_dupes_cleaned = idp_dupes.groupby(\"idp\").apply(merge_rows).reset_index(drop=True)\n",
    "nfi_site_nodupes = nfi_site_raw[~nfi_site_raw[\"idp\"].isin(idp_dupes[\"idp\"])]\n",
    "nfi_site_nodupes = pd.concat([nfi_site_nodupes, idp_dupes_cleaned]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "# Check if done correctly\n",
    "print(f\" - Number of unique sites: {nfi_site_nodupes['idp'].nunique()}\")\n",
    "print(f\" - Number of observations: {nfi_site_nodupes.shape[0]}\")\n",
    "# Set idp back to category\n",
    "nfi_site_nodupes[\"idp\"] = nfi_site_nodupes[\"idp\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Tree Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set growth threshold below which ba_1 and ba_2 are set to NA\n",
    "growth_threshold = -5  # percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW DF\n",
    "nfi_tree_growth = nfi_tree_raw.copy()\n",
    "\n",
    "# DBH, BA, CHANGE -------------------------------------------------------------\n",
    "# Get plot area\n",
    "plot_area = 25**2 * np.pi / 10**5  # (25m)^2 * pi / 10000 [m^2/ha] = [ha]\n",
    "\n",
    "# C13\n",
    "nfi_tree_growth[\"c13_change_abs_yr\"] = (\n",
    "    nfi_tree_growth[\"c13_2\"] - nfi_tree_growth[\"c13_1\"]\n",
    ") / 5\n",
    "nfi_tree_growth[\"c13_change_perc_yr\"] = (\n",
    "    nfi_tree_growth[\"c13_change_abs_yr\"] / nfi_tree_growth[\"c13_1\"] * 100\n",
    ")\n",
    "\n",
    "# For trees with unrealistic growth, set c13_2 to NA\n",
    "# Print number of trees with unrealistic growth\n",
    "df_trees_unreal_growth = nfi_tree_growth.query(\"c13_change_perc_yr < @growth_threshold\")\n",
    "df_trees_real_growth = nfi_tree_growth.query(\"c13_change_perc_yr >= @growth_threshold\")\n",
    "ntrees_unrealistic_growth = df_trees_unreal_growth.shape[0]\n",
    "ptrees_unrealistic_growth = round(\n",
    "    ntrees_unrealistic_growth / nfi_tree_growth.shape[0] * 100\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\" - Number of trees with growth less than {growth_threshold}%: {ntrees_unrealistic_growth} ({ptrees_unrealistic_growth}%)\",\n",
    "    f\"\\n   For these, c13_1 and c13_2 are set to NA‚ùó\",\n",
    ")\n",
    "\n",
    "# Set c13_1, c13_2 and c13_change_perc_yr to NA if growth is below threshold\n",
    "nfi_tree_growth.loc[\n",
    "    nfi_tree_growth[\"c13_change_abs_yr\"] < growth_threshold, \"c13_1\"\n",
    "] = np.nan\n",
    "nfi_tree_growth.loc[\n",
    "    nfi_tree_growth[\"c13_change_abs_yr\"] < growth_threshold, \"c13_2\"\n",
    "] = np.nan\n",
    "nfi_tree_growth.loc[\n",
    "    nfi_tree_growth[\"c13_change_abs_yr\"] < growth_threshold, \"c13_change_perc_yr\"\n",
    "] = np.nan\n",
    "\n",
    "# Diameter\n",
    "nfi_tree_growth[\"dbh_1\"] = nfi_tree_growth[\"c13_1\"] / np.pi\n",
    "nfi_tree_growth[\"dbh_2\"] = nfi_tree_growth[\"c13_2\"] / np.pi\n",
    "nfi_tree_growth[\"dbh_change_abs_yr\"] = (\n",
    "    nfi_tree_growth[\"dbh_2\"] - nfi_tree_growth[\"dbh_1\"]\n",
    ") / 5\n",
    "nfi_tree_growth[\"dbh_change_perc_yr\"] = (\n",
    "    nfi_tree_growth[\"dbh_change_abs_yr\"] / nfi_tree_growth[\"dbh_1\"] * 100\n",
    ")\n",
    "\n",
    "# Basal Area\n",
    "nfi_tree_growth[\"ba_1\"] = np.pi * (nfi_tree_growth[\"dbh_1\"] / 2) ** 2 / plot_area\n",
    "nfi_tree_growth[\"ba_2\"] = np.pi * (nfi_tree_growth[\"dbh_2\"] / 2) ** 2 / plot_area\n",
    "nfi_tree_growth[\"ba_change_abs_yr\"] = (\n",
    "    nfi_tree_growth[\"ba_2\"] - nfi_tree_growth[\"ba_1\"]\n",
    ") / 5\n",
    "nfi_tree_growth[\"ba_change_perc_yr\"] = (\n",
    "    nfi_tree_growth[\"ba_change_abs_yr\"] / nfi_tree_growth[\"ba_1\"] * 100\n",
    ")\n",
    "\n",
    "# Growth rates\n",
    "nfi_tree_growth[\"avg_growth_height_meter_per_yr\"] = (\n",
    "    nfi_tree_growth[\"htot\"] / nfi_tree_growth[\"age13\"]\n",
    ")\n",
    "nfi_tree_growth[\"avg_growth_ba_per_year\"] = (\n",
    "    nfi_tree_growth[\"ba_1\"] / nfi_tree_growth[\"age13\"]\n",
    ")\n",
    "\n",
    "# Replace inf with NA\n",
    "nfi_tree_growth = nfi_tree_growth.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age correction\n",
    "\n",
    "- age13 is actually the age estimated at 1.30m and not the true age of the tree. To account for this, the IGN provides the following table.\n",
    "- I am overwriting age13 instead of creating a new age variable so that down-stream code does not need to be changed and that age13 is the only true age variable.\n",
    "\n",
    "| Tree Name           | ESPAR Code     | Years |\n",
    "| ------------------- | -------------- | ----- |\n",
    "| ch√™ne p√©doncul√©     | 02             | 7     |\n",
    "| ch√™ne sessile       | 03             | 8     |\n",
    "| ch√™ne pubescent     | 05             | 7     |\n",
    "| ch√™ne vert          | 06             | 6     |\n",
    "| h√™tre               | 09             | 9     |\n",
    "| ch√¢taignier         | 10             | 3     |\n",
    "| charme              | 11             | 7     |\n",
    "| bouleau verruqueux  | 12V            | 4     |\n",
    "| fr√™ne commun        | 17C            | 5     |\n",
    "| autres feuillus     | other_feuillus | 5     |\n",
    "| pin maritime        | 51             | 4     |\n",
    "| pin sylvestre       | 52             | 7     |\n",
    "| pin noir d'Autriche | 54             | 8     |\n",
    "| sapin pectin√©       | 61             | 9     |\n",
    "| √©pic√©a commun       | 62             | 8     |\n",
    "| douglas             | 64             | 4     |\n",
    "| autres r√©sineux     | other_resineux | 6     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tree_years dictionary\n",
    "tree_years_dict = {\n",
    "    \"02\": 7,\n",
    "    \"03\": 8,\n",
    "    \"05\": 7,\n",
    "    \"06\": 6,\n",
    "    \"09\": 9,\n",
    "    \"10\": 3,\n",
    "    \"11\": 7,\n",
    "    \"12V\": 4,\n",
    "    \"17C\": 5,\n",
    "    \"other_feuillus\": 5,\n",
    "    \"51\": 4,\n",
    "    \"52\": 7,\n",
    "    \"54\": 8,\n",
    "    \"61\": 9,\n",
    "    \"62\": 8,\n",
    "    \"64\": 4,\n",
    "    \"other_resineux\": 6,\n",
    "    \"Missing\": 0,\n",
    "}\n",
    "\n",
    "# Add correction for unspecified species\n",
    "for spec in nfi_tree_growth[\"espar\"].unique():\n",
    "    if spec in tree_years_dict or spec == \"Missing\":\n",
    "        # print(f\"Species: {spec} - Years: {tree_years_dict[spec]}\")\n",
    "        continue\n",
    "    else:\n",
    "        # Extract first two letters and turn into int\n",
    "        first_two = int(spec[0:2])\n",
    "        # Check if it is a broadleaf (<50) or needleleaf (>=50)\n",
    "        if first_two < 50:\n",
    "            tree_years_dict[spec] = 5\n",
    "        else:\n",
    "            tree_years_dict[spec] = 6\n",
    "\n",
    "# Add to dataframe\n",
    "nfi_tree_growth[\"add_years\"] = nfi_tree_growth[\"espar\"].map(tree_years_dict)\n",
    "\n",
    "# Add years to age13\n",
    "nfi_tree_growth[\"age13\"] = nfi_tree_growth[\"age13\"] + nfi_tree_growth[\"add_years\"]\n",
    "\n",
    "# Remove temporary columns\n",
    "nfi_tree_growth = nfi_tree_growth.drop(columns=[\"add_years\"])\n",
    "\n",
    "# Plot histogram of tree ages\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.histplot(nfi_tree_growth[\"age13\"], bins=30, ax=ax)\n",
    "ax.set_title(\"Histogram of tree ages\")\n",
    "ax.set_xlabel(\"Age\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "nfi_tree_growth[\"age13\"].describe().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGE CLASS -------------------------------------------------------------------\n",
    "nfi_tree_growth[\"tree_age_class\"] = (\n",
    "    pd.cut(\n",
    "        nfi_tree_growth[\"age13\"],\n",
    "        bins=[0, 15, 30, 45, 60, 75, 100, np.Inf],\n",
    "        labels=[\"0-15\", \"15-30\", \"30-45\", \"45-60\", \"60-75\", \"75-100\", \"100+\"],\n",
    "    )\n",
    "    .cat.add_categories(\"Missing\")\n",
    "    .fillna(\"Missing\")\n",
    ")\n",
    "\n",
    "# CIRCUMFERENCE CLASS -  NFI ----------------------------------------------------\n",
    "# Cuts are based on IFN documentation categories\n",
    "nfi_tree_growth[\"tree_circumference_class\"] = (\n",
    "    pd.cut(\n",
    "        nfi_tree_growth[\"c13_1\"],\n",
    "        bins=[\n",
    "            0,\n",
    "            0.705,\n",
    "            1.175,\n",
    "            np.Inf,\n",
    "        ],\n",
    "        labels=[\"small\", \"medium\", \"large\"],\n",
    "    )\n",
    "    .cat.add_categories([\"Missing\", \"recruit\"])\n",
    "    .fillna(\"Missing\")\n",
    ")\n",
    "\n",
    "# Set all recruits to sapling\n",
    "nfi_tree_growth.loc[\n",
    "    nfi_tree_growth[\"tree_state_change\"] == \"new_alive\", \"tree_circumference_class\"\n",
    "] = \"recruit\"\n",
    "\n",
    "# CIRCUMFERENCE CLASS - MANUAL -------------------------------------------------\n",
    "\n",
    "# nfi_tree_growth[\"tree_circumference_class_small_and_large\"] = (\n",
    "#     pd.cut(\n",
    "#         nfi_tree_growth[\"c13_1\"],\n",
    "#         bins=[\n",
    "#             0,\n",
    "#             0.705,\n",
    "#             1.175,\n",
    "#             np.Inf,\n",
    "#         ],\n",
    "#         labels=[\"small\", \"medium\", \"large\"],\n",
    "#     )\n",
    "#     .cat.add_categories([\"Missing\", \"recruit\"])\n",
    "#     .fillna(\"Missing\")\n",
    "# )\n",
    "\n",
    "# # Set all recruits to sapling\n",
    "# nfi_tree_growth.loc[\n",
    "#     nfi_tree_growth[\"tree_state_change\"] == \"new_alive\", \"tree_circumference_class\"\n",
    "# ] = \"recruit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative circumference of living trees\n",
    "df_tmp = nfi_tree_growth.copy()\n",
    "df_tmp = df_tmp.query(\"tree_state_1 == 'alive'\")\n",
    "df_tmp[\"c13_rel\"] = df_tmp.groupby(\"idp\", observed=False)[\"c13_1\"].transform(\n",
    "    lambda x: (x / x.mean())\n",
    ")\n",
    "\n",
    "# Separate into social status - suppressed (0) and dominant (1)\n",
    "df_tmp[\"social_status\"] = df_tmp[\"c13_rel\"].apply(lambda x: 1 if x > 1.0 else 0)\n",
    "\n",
    "# Dataframe for suppressed trees\n",
    "df_sup = df_tmp.query(\"social_status != 1\").copy()\n",
    "df_sup[\"c13_rel_dominant_median\"] = np.nan\n",
    "\n",
    "# Dataframe for dominant trees\n",
    "# Calculate median values within dominant trees\n",
    "df_dom = df_tmp.query(\"social_status == 1\").copy()\n",
    "df_dom[\"c13_rel_dominant_median\"] = df_dom.groupby(\"idp\", observed=False)[\n",
    "    \"c13_1\"\n",
    "].transform(lambda x: (x / x.median()))\n",
    "\n",
    "# Separate into social status - suppressed (0), small-dominant (1) and large-dominant (2)\n",
    "df_dom[\"social_status\"] = df_dom[\"c13_rel_dominant_median\"].apply(\n",
    "    lambda x: 2 if x > 1.0 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back to original dataframe\n",
    "df_both = pd.concat([df_sup, df_dom])\n",
    "df_fin = nfi_tree_growth.merge(\n",
    "    df_both[[\"tree_id\", \"social_status\", \"c13_rel\", \"c13_rel_dominant_median\"]],\n",
    "    on=\"tree_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Check if rows are the same\n",
    "if df_fin.shape[0] != nfi_tree_growth.shape[0]:\n",
    "    raise ValueError(\n",
    "        f\"‚ùó‚ùó‚ùó Rows are not the same, check this! df_fin = {df_fin.shape[0]} and nfi_tree_growth = {nfi_tree_growth.shape[0]}\"\n",
    "    )\n",
    "\n",
    "# Check change in columns\n",
    "print(\"New columns:\")\n",
    "for c in df_fin.columns:\n",
    "    if c not in nfi_tree_growth.columns:\n",
    "        print(f\" - {c}\")\n",
    "\n",
    "# Overwrite nfi_tree_growth\n",
    "nfi_tree_growth = df_fin.copy()\n",
    "\n",
    "# Plot circumference relative to social status\n",
    "sns.violinplot(data=df_both, x=\"social_status\", y=\"c13_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Filter Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Level\n",
    "\n",
    "Results:\n",
    "\n",
    "- It looks like that for trees with substantial negative growth, that these trees had much larger c13_1 measurements compared to trees that had no negative growth. The distribution of the c13_2 measurements look similar for trees with negative growth and trees without negative growth. But there is also a tendency that c13_2 are super small. So, it is probably a mix between measurement errors of c13_1 being too large and c13_2 being too small.\n",
    "  - Decision: trees below above-defined growth-threshold are set to have na for their c13_x measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFORMATION ON UNREALISTIC GROWTH -------------------------------------------\n",
    "# Create a 2x2 plot grid\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 12))\n",
    "\n",
    "# Plot distribution of growth rates\n",
    "sns.histplot(df_trees_unreal_growth[\"c13_change_perc_yr\"], ax=axs[0, 0], stat=\"count\")\n",
    "axs[0, 0].set_title(\n",
    "    f\"Distribution of Growth Rates (only growth below {growth_threshold}%)\"\n",
    ")\n",
    "\n",
    "# Plot the mean per campagne as boxplot\n",
    "sns.boxplot(\n",
    "    x=\"campagne_1\", y=\"c13_change_perc_yr\", data=df_trees_unreal_growth, ax=axs[0, 1]\n",
    ")\n",
    "axs[0, 1].set_title(\n",
    "    f\"Mean Growth Rates per Campagne (only growth below {growth_threshold}%)\"\n",
    ")\n",
    "\n",
    "# Add count on top of boxplot\n",
    "sns.countplot(x=\"campagne_1\", data=df_trees_unreal_growth, ax=axs[1, 0])\n",
    "axs[1, 0].set_title(\n",
    "    f\"Count of Trees per Campagne (only growth below {growth_threshold}%)\"\n",
    ")\n",
    "\n",
    "# Compare C13_1 and C13_2\n",
    "axs[1, 1].scatter(\n",
    "    df_trees_real_growth[\"c13_1\"],\n",
    "    df_trees_real_growth[\"c13_2\"],\n",
    "    color=\"green\",\n",
    "    label=f\"Growth above {growth_threshold}%\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "axs[1, 1].scatter(\n",
    "    df_trees_unreal_growth[\"c13_1\"],\n",
    "    df_trees_unreal_growth[\"c13_2\"],\n",
    "    color=\"red\",\n",
    "    label=f\"Growth below {growth_threshold}%\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "axs[1, 1].set_xlabel(\"C13_1\")\n",
    "axs[1, 1].set_ylabel(\"C13_2\")\n",
    "axs[1, 1].legend()\n",
    "axs[1, 1].axline((0, 0), slope=1, color=\"black\", linestyle=\"--\")\n",
    "\n",
    "# Get counts of NAs in C13_1 and C13_2 per campagne\n",
    "df_tmp = nfi_tree_growth.copy()\n",
    "df_tmp[\"c13_1_NA\"] = df_tmp[\"c13_1\"].isna()\n",
    "df_tmp[\"c13_2_NA\"] = df_tmp[\"c13_2\"].isna()\n",
    "\n",
    "# Plot these counts per campagne\n",
    "sns.histplot(\n",
    "    x=\"campagne_1\",\n",
    "    hue=\"c13_1_NA\",\n",
    "    data=df_tmp,\n",
    "    ax=axs[2, 0],\n",
    "    multiple=\"fill\",\n",
    "    stat=\"count\",\n",
    ")\n",
    "\n",
    "axs[2, 0].set_title(f\"Count of NAs in C13_1 per Campagne (all trees)\")\n",
    "\n",
    "sns.histplot(\n",
    "    x=\"campagne_1\",\n",
    "    hue=\"c13_2_NA\",\n",
    "    data=df_tmp,\n",
    "    ax=axs[2, 1],\n",
    "    multiple=\"fill\",\n",
    "    stat=\"count\",\n",
    ")\n",
    "axs[2, 1].set_title(f\"Count of NAs in C13_2 per Campagne (all trees)\")\n",
    "\n",
    "\n",
    "# Plot distribution of c13_1 in df_trees_unreal_growth\n",
    "sns.histplot(\n",
    "    df_trees_unreal_growth[\"c13_1\"], stat=\"density\", color=\"red\", bins=100, ax=axs[3, 0]\n",
    ")\n",
    "sns.histplot(\n",
    "    df_trees_real_growth[\"c13_1\"], stat=\"density\", color=\"green\", bins=100, ax=axs[3, 0]\n",
    ")\n",
    "axs[3, 0].set_title(\"Distribution of C13_1 from unreal and real growth\")\n",
    "axs[3, 0].legend([\"Unreal Growth\", \"Real Growth\"])\n",
    "\n",
    "# Plot distribution of c13_2 in df_trees_unreal_growth\n",
    "sns.histplot(\n",
    "    df_trees_unreal_growth[\"c13_2\"], stat=\"density\", color=\"red\", bins=100, ax=axs[3, 1]\n",
    ")\n",
    "sns.histplot(\n",
    "    df_trees_real_growth[\"c13_2\"], stat=\"density\", color=\"green\", bins=100, ax=axs[3, 1]\n",
    ")\n",
    "axs[3, 1].set_title(\"Distribution of C13_2 from unreal and real growth\")\n",
    "axs[3, 1].legend([\"Unreal Growth\", \"Real Growth\"])\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFORMATION ON TREE STATE ----------------------------------------------------\n",
    "# Drop tree_state_change without any counts\n",
    "nfi_tree_growth[\"tree_state_change\"] = (\n",
    "    nfi_tree_growth[\"tree_state_change\"].astype(str).astype(\"category\")\n",
    ")\n",
    "\n",
    "# Create a 2x1 plot grid\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Distribution of tree_state_change\n",
    "nfi_tree_growth[\"tree_state_change\"].value_counts().plot(kind=\"bar\", ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of Tree State Change\")\n",
    "\n",
    "(\n",
    "    nfi_tree_growth.groupby([\"tree_state_change\", \"campagne_1\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"tree_state_change\", index=\"campagne_1\", values=0)\n",
    "    .plot(kind=\"bar\", stacked=True, ax=axs[1])\n",
    ")\n",
    "axs[1].legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.2), ncol=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site Level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counted trees per campagne\n",
    "nfi_tree_growth.campagne_1.value_counts().sort_index().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counted sites per campagne\n",
    "nfi_site_nodupes.campagne_1.value_counts().sort_index().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if distribution of sites across France is similar in our time-window as in remaining time\n",
    "# Subset tree and site dataframe\n",
    "tmp_tree = nfi_tree_growth[[\"idp\", \"tree_id\", \"campagne_1\"]].reset_index(drop=True)\n",
    "tmp_site = nfi_site_nodupes[\n",
    "    [\"idp\", \"campagne_1\", \"lat\", \"lon\", \"lat_fr\", \"lon_fr\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Make sure idp is numeric for merging...\n",
    "tmp_tree[\"idp\"] = tmp_tree[\"idp\"].astype(\"int32\")\n",
    "tmp_site[\"idp\"] = tmp_site[\"idp\"].astype(\"int32\")\n",
    "\n",
    "df_tmp = pd.merge(tmp_tree, tmp_site, on=[\"idp\", \"campagne_1\"], how=\"left\")\n",
    "\n",
    "df_intime = df_tmp.query(\"campagne_1 >= 2010 & campagne_1 <= 2016\")[\n",
    "    [\"idp\", \"lat\", \"lon\"]\n",
    "].drop_duplicates()\n",
    "df_outsidetime = df_tmp.query(\"campagne_1 < 2010 | campagne_1 > 2016\")[\n",
    "    [\"idp\", \"lat\", \"lon\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "# Plot intime\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.kdeplot(\n",
    "    data=df_intime,\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "    fill=True,\n",
    "    cmap=\"Greens\",\n",
    "    ax=axs[0],\n",
    "    thresh=0,\n",
    "    levels=150,\n",
    "    # cbar=True,\n",
    ")\n",
    "axs[0].set_title(\"Distribution of sites in time window\")\n",
    "axs[0].set_xlabel(\"Longitude\")\n",
    "axs[0].set_ylabel(\"Latitude\")\n",
    "\n",
    "# Plot outsidetime\n",
    "sns.kdeplot(\n",
    "    data=df_outsidetime,\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "    fill=True,\n",
    "    cmap=\"Reds\",\n",
    "    ax=axs[1],\n",
    "    thresh=0,\n",
    "    levels=150,\n",
    "    # cbar=True,\n",
    ")\n",
    "axs[1].set_title(\"Distribution of sites outside time window\")\n",
    "axs[1].set_xlabel(\"Longitude\")\n",
    "axs[1].set_ylabel(\"Latitude\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Investigate Simplified Trees**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplified trees, do they have NA in ba_1 and ba_2?\n",
    "# Get timespan of interest\n",
    "df_timespan = nfi_tree_growth.copy().query(\"campagne_1 >= 2010 & campagne_1 <= 2018\")\n",
    "df_timespan[\"idp\"] = df_timespan[\"idp\"].astype(\"int32\")\n",
    "df_timespan.insert(0, \"tree_id\", df_timespan.pop(\"tree_id\"))\n",
    "# Split into simplified and non-simplified trees\n",
    "df_simplified_trees = df_timespan.query(\"simplif == '1'\")\n",
    "df_nonsimplified_trees = df_timespan.query(\"simplif != '1'\")\n",
    "\n",
    "display(f\"Shape of simplified trees from 2009-2016: {df_simplified_trees.shape}\")\n",
    "display(df_simplified_trees.tree_state_change.value_counts())\n",
    "# display(df_simplified_trees.campagne_1.value_counts().sort_index())\n",
    "# display(\n",
    "#     df_simplified_trees[[\"campagne_1\", \"tree_state_change\"]].value_counts().sort_index()\n",
    "# )\n",
    "df_simplified_trees[[\"ba_1\", \"ba_2\"]].isna().value_counts(dropna=False)\n",
    "# * Only 10 percent of all simplified trees have no ba_2 values. So they can still be useful!\n",
    "\n",
    "# Number of trees per site\n",
    "df_tmp = df_simplified_trees.groupby(\"idp\").size().reset_index()\n",
    "df_tmp.columns = [\"idp\", \"n_trees\"]\n",
    "print(\"Number of trees per site: \")\n",
    "display(df_tmp.n_trees.value_counts().sort_index().plot(kind=\"bar\", figsize=(15, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are these sites with more than 20 trees?\n",
    "# * Looks all good to me...\n",
    "# df_tmp.query(\"n_trees > 20\").sort_values(\"n_trees\", ascending=False).head(10)\n",
    "# df_simplified_trees.query(\"idp == 625259\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are simplified trees not assessed on damage?\n",
    "# * No, they are generally assessed on damage...\n",
    "df_simplified_trees.value_counts(\"acci\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"deggib\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"sfgui_1\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"sfgui_2\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"sfdorge_1\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"sfdorge_2\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"sfcoeur\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"sfgeliv_1\", dropna=False).sort_index()\n",
    "df_simplified_trees.value_counts(\"sfgeliv_2\", dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are some species more simplified than others?\n",
    "# * There are minor differences, but generally the top 10 species are the same.\n",
    "print(f\"Non simplified trees (total = {df_nonsimplified_trees.shape[0]}):\")\n",
    "display(\n",
    "    df_nonsimplified_trees[\"genus_lat\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    "    / df_nonsimplified_trees.shape[0]\n",
    "    * 100\n",
    ")\n",
    "print(f\"Simplified trees (total = {df_simplified_trees.shape[0]}):\")\n",
    "display(\n",
    "    df_simplified_trees[\"genus_lat\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    "    / df_simplified_trees.shape[0]\n",
    "    * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Apriori Filter of Sites and Trees\n",
    "\n",
    "Notes on the filters below:\n",
    "\n",
    "- Filters to apply to reduce computational load + get more representative results when calculating site-level metrics.\n",
    "- Between 2009-2016, peupnr_2 is never 3 (temporarily forested). So, filtering for peupnr_2 =! 3 does not remove any clear-cut sites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SITE FILTER\n",
    "df_aft = nfi_site_nodupes.copy()\n",
    "\n",
    "# Filter for campagne of interest\n",
    "first_year = 2010\n",
    "last_year = 2018\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"campagne_1 >= @first_year and campagne_1 <= @last_year\")\n",
    "filter_report(f\"campagne_1 {first_year}-{last_year}\", df_bef, df_aft, site_level=True)\n",
    "\n",
    "# Filter based on peupnr_x (0 = censusable, 1 = not censusable, 3 = temporarily deforested)\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"peupnr_1 not in ['1', '3']\")\n",
    "filter_report(\"Censuable sites (peupnr_1)\", df_bef, df_aft, site_level=True)\n",
    "\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"peupnr_2 not in ['1', '3']\")\n",
    "filter_report(\"Censuable sites (peupnr_2)\", df_bef, df_aft, site_level=True)\n",
    "\n",
    "# Removing NA coords lat/lon\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.dropna(subset=[\"lat\", \"lon\"])\n",
    "filter_report(\"Removing NA coords lat/lon\", df_bef, df_aft, site_level=True)\n",
    "\n",
    "# Final Report\n",
    "nfi_site_filtered = df_aft.reset_index(drop=True)\n",
    "filter_report(\"-- Final\", nfi_site_nodupes, nfi_site_filtered, site_level=True)\n",
    "\n",
    "# Reset dtypes to remove dropped levels\n",
    "nfi_site_filtered[\"peupnr_1\"] = nfi_site_filtered[\"peupnr_1\"].astype(\"category\")\n",
    "nfi_site_filtered[\"peupnr_2\"] = nfi_site_filtered[\"peupnr_2\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Coordinates for Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Note: Save the idp, census year, and coordinates for these sites.\n",
    "# ! This file is then used for all extractions!\n",
    "# ! Filters down the line, may remove some sites, but keeping them in here makes it more flexible for a general data extraction without going back-and-forth again.\n",
    "\n",
    "sites_with_idp = (\n",
    "    nfi_site_filtered[[\"idp\", \"campagne_1\", \"lat\", \"lon\", \"lat_fr\", \"lon_fr\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "sites_with_idp.columns = [\"SiteID\", \"idp\", \"first_year\", \"y\", \"x\", \"y_fr\", \"x_fr\"]\n",
    "\n",
    "sites_without_idp = (\n",
    "    sites_with_idp.copy()\n",
    "    .drop([\"idp\", \"first_year\", \"SiteID\"], axis=1)\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()  # Needs double index reset for some reason...\n",
    "    .rename(columns={\"index\": \"SiteID\"})\n",
    ")\n",
    "\n",
    "os.makedirs(here(\"data/final/nfi\"), exist_ok=True)\n",
    "sites_with_idp.to_csv(here(\"data/final/nfi/coords_of_sites_with_idp.csv\"), index=False)\n",
    "sites_without_idp.to_csv(\n",
    "    here(\"data/final/nfi/coords_of_sites_without_idp.csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as geojson\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Fixing dtypes\n",
    "sites_with_idp[\"idp\"] = sites_with_idp[\"idp\"].astype(\"int32\")\n",
    "\n",
    "# For EPSG 4326, WGS84\n",
    "geometry = [Point(xy) for xy in zip(sites_with_idp.x, sites_with_idp.y)]\n",
    "sites_epsg4326 = gpd.GeoDataFrame(sites_with_idp, geometry=geometry)\n",
    "sites_epsg4326.crs = \"EPSG:4326\"\n",
    "sites_epsg4326.to_file(\n",
    "    here(\"data/final/nfi/sites_with_idp_epsg4326.geojson\"), driver=\"GeoJSON\"\n",
    ")\n",
    "\n",
    "# For EPSG 2154, RGF93 / Lambert-93\n",
    "geometry = [Point(xy) for xy in zip(sites_with_idp.x_fr, sites_with_idp.y_fr)]\n",
    "sites_epsg2154 = gpd.GeoDataFrame(sites_with_idp, geometry=geometry)\n",
    "sites_epsg2154 = sites_epsg2154[[\"idp\", \"first_year\", \"geometry\"]]\n",
    "sites_epsg2154.crs = \"EPSG:2154\"\n",
    "sites_epsg2154.to_file(\n",
    "    here(\"data/final/nfi/sites_with_idp_epsg2154.geojson\"), driver=\"GeoJSON\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREE FILTER\n",
    "# New df\n",
    "df_bef = nfi_tree_growth.copy()\n",
    "\n",
    "# Reduce to idp that are in nfi_site_filtered\n",
    "nfi_site_nodupes[\"idp\"] = nfi_site_nodupes[\"idp\"].astype(int)\n",
    "\n",
    "df_bef[\"idp\"] = df_bef[\"idp\"].astype(int)\n",
    "df_aft = df_bef[df_bef[\"idp\"].isin(nfi_site_filtered[\"idp\"].tolist())]\n",
    "filter_report(\"Reduce to idp in nfi_site_filtered\", df_bef, df_aft)\n",
    "\n",
    "# Remove where shadow growth is 'yes'\n",
    "# * This was done in R, not done anymore.\n",
    "# * But this does not change anything because it only\n",
    "# * removes trees measured outside our time of interest\n",
    "# df_bef = df_aft.copy()\n",
    "# df_aft = df_bef.query(\"shadow_growth != 'yes'\")\n",
    "# filter_report(\"Remove trees wi. shadow growth\", df_bef, df_aft)\n",
    "\n",
    "# Remove trees with unknown tree status\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"tree_state_1 != 'Missing'\")\n",
    "filter_report(\"Remove tree_state_1 missing\", df_bef, df_aft)\n",
    "\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"tree_state_2 != 'Missing'\")\n",
    "filter_report(\"Remove tree_state_2 missing\", df_bef, df_aft)\n",
    "\n",
    "# Remove trees with non-sensical tree state change\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"tree_state_change != 'dead_alive'\")\n",
    "filter_report(\"Remove state_change dead_alive\", df_bef, df_aft)\n",
    "\n",
    "# Remove recruits that died\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"tree_state_change != 'new_dead'\")\n",
    "filter_report(\"Remove state_change new_dead\", df_bef, df_aft)\n",
    "\n",
    "# Remove trees that were certainly outside of the target stand\n",
    "df_bef = df_aft.copy()\n",
    "df_aft = df_bef.query(\"cible != '0'\")\n",
    "filter_report(\"Remove cible\", df_bef, df_aft)\n",
    "\n",
    "# Remove trees with simplified measurement\n",
    "# ! This removes only 10 sites but ~33% of all trees! This limits our analysis a lot!\n",
    "# df_bef = df_aft.copy()\n",
    "# df_aft = df_bef.query(\"simplif != '1'\")\n",
    "# filter_report(\"Remove simplified trees\", df_bef, df_aft)\n",
    "\n",
    "# Final Report\n",
    "nfi_filter_trees = df_aft.reset_index(drop=True)\n",
    "filter_report(\"-- Final\", nfi_tree_growth, nfi_filter_trees)\n",
    "\n",
    "# Reset dtypes to remove dropped levels\n",
    "# nfi_filter_trees[\"shadow_growth\"] = nfi_filter_trees[\n",
    "#     \"shadow_growth\"\n",
    "# ].cat.remove_unused_categories()\n",
    "nfi_filter_trees[\"tree_state_1\"] = nfi_filter_trees[\n",
    "    \"tree_state_1\"\n",
    "].cat.remove_unused_categories()\n",
    "nfi_filter_trees[\"tree_state_2\"] = nfi_filter_trees[\n",
    "    \"tree_state_2\"\n",
    "].cat.remove_unused_categories()\n",
    "nfi_filter_trees[\"tree_state_change\"] = nfi_filter_trees[\n",
    "    \"tree_state_change\"\n",
    "].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce sites only to sites that were kept after filtering trees\n",
    "nfi_site_filtered = nfi_site_filtered[\n",
    "    nfi_site_filtered[\"idp\"].isin(nfi_filter_trees[\"idp\"].unique())\n",
    "].reset_index(drop=True)\n",
    "\n",
    "nfi_site_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information (after filter to speed things up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get temporary df\n",
    "nfi_tree_addinfo = nfi_filter_trees.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Height Estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for how many trees htot is missing\n",
    "print(\"Dataset lacks HEIGHT for % of trees: \")\n",
    "\n",
    "for state in nfi_tree_addinfo[\"tree_state_change\"].unique():\n",
    "    print(\n",
    "        f\" - {state:<15}: {round(nfi_tree_addinfo.query('tree_state_change == @state')['htot'].isna().sum() / nfi_tree_addinfo.query('tree_state_change == @state').shape[0] * 100, 2)}%\"\n",
    "    )\n",
    "\n",
    "print(\"\\nDataset lacks VOLUME for % of trees: \")\n",
    "\n",
    "for state in nfi_tree_addinfo[\"tree_state_change\"].unique():\n",
    "    print(\n",
    "        f\" - {state:<15}: {round(nfi_tree_addinfo.query('tree_state_change == @state')['v'].isna().sum() / nfi_tree_addinfo.query('tree_state_change == @state').shape[0] * 100, 2)}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest to predict htot\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Get only trees with htot\n",
    "df_tmp = nfi_tree_addinfo.merge(nfi_site_filtered, how=\"left\", on=\"idp\")[\n",
    "    [\n",
    "        \"tree_id\",\n",
    "        \"c13_1\",\n",
    "        \"lat_fr\",\n",
    "        \"lon_fr\",\n",
    "        \"espar\",\n",
    "        \"genus_lat\",\n",
    "        \"simplif\",\n",
    "        \"tree_state_change\",\n",
    "        \"age13\",\n",
    "        \"idp\",\n",
    "        \"ir5\",\n",
    "        \"w\",\n",
    "        \"v\",\n",
    "        \"htot\",\n",
    "        \"tetard\",  # Whether tree is a cut constantly\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Filter down to trees that were alive at first visit\n",
    "# Note: There are many dead trees with weirdly small htot (maybe the fell or something)\n",
    "# So, I am only focusing on alive trees. Should be kept in mind for later calcualtions!\n",
    "df_tmp = df_tmp.query(\"tree_state_change in ['alive_alive', 'alive_cut', 'alive_dead']\")\n",
    "\n",
    "# OHE espar and simplif\n",
    "df_tmp = pd.get_dummies(\n",
    "    df_tmp,\n",
    "    columns=[\"espar\", \"simplif\", \"genus_lat\", \"tree_state_change\", \"tetard\"],\n",
    "    drop_first=True,\n",
    ")\n",
    "\n",
    "# Get dictionary for OHE\n",
    "ohe_dict = {\n",
    "    \"espar\": df_tmp.filter(like=\"espar\").columns,\n",
    "    \"simplif\": df_tmp.filter(like=\"simplif\").columns,\n",
    "    \"genus_lat\": df_tmp.filter(like=\"genus_lat\").columns,\n",
    "    \"tree_state_change\": df_tmp.filter(like=\"tree_state_change\").columns,\n",
    "    \"tetard\": df_tmp.filter(like=\"tetard\").columns,\n",
    "}\n",
    "\n",
    "# Drop NAs\n",
    "df_rf = df_tmp.copy()\n",
    "df_rf = df_rf.dropna()\n",
    "\n",
    "# Split into predictors and response\n",
    "X = df_rf.drop(columns=[\"htot\", \"tree_id\"])\n",
    "y = df_rf[\"htot\"]\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    # stratify=vec_strat,\n",
    ")\n",
    "\n",
    "# Train model with best parameters\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "my_grid = {\n",
    "    \"n_estimators\": [100, 200, 1000],\n",
    "    \"max_depth\": [1, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", 0.1, 0.05],\n",
    "}\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "# grid_search = GridSearchCV(\n",
    "#     rf, param_grid=my_grid, cv=5, scoring=\"neg_mean_squared_error\", verbose=2\n",
    "# )\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get best parameters\n",
    "# grid_search.best_params_\n",
    "\n",
    "# # Report best parameters and score\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best score: {grid_search.best_score_} (RMSE)\")\n",
    "\n",
    "# Do CV with best parameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# best_params = {\"max_depth\": 10, \"max_features\": 0.1, \"n_estimators\": 1000}\n",
    "best_params = {\"n_estimators\": 100}\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    **best_params,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rmse_scores = cross_val_score(\n",
    "    rf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    ")\n",
    "rmse_scores = np.sqrt(-rmse_scores)\n",
    "r2_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring=\"r2\")\n",
    "\n",
    "# Report\n",
    "print(\"Train performance:\")\n",
    "print(f\"RMSE: {rmse_scores.mean():.2f} +/- {rmse_scores.std():.2f}\")\n",
    "print(f\"R2: {r2_scores.mean():.2f} +/- {r2_scores.std():.2f}\")\n",
    "\n",
    "# Fit\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Get variable importance\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "names = X.columns[indices]\n",
    "\n",
    "# Get RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "r2 = rf.score(X_test, y_test)\n",
    "\n",
    "# Report\n",
    "print(\"\\nTest performance:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Make one scatter and one kde plot side by side\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "# Scatter\n",
    "axs[0].scatter(y_pred, y_test, alpha=0.5)\n",
    "axs[0].plot([0, 45], [0, 45], color=\"red\", linestyle=\"--\")\n",
    "axs[0].set_xlim(0, 45)\n",
    "axs[0].set_ylim(0, 45)\n",
    "axs[0].set_xlabel(\"Predicted height (m)\")\n",
    "axs[0].set_ylabel(\"True height (m)\")\n",
    "\n",
    "# KDE\n",
    "sns.kdeplot(\n",
    "    x=y_pred,\n",
    "    y=y_test,\n",
    "    fill=True,\n",
    "    thresh=0.05,\n",
    "    levels=10,\n",
    "    cmap=\"mako\",\n",
    "    ax=axs[1],\n",
    ")\n",
    "axs[1].plot([0, 45], [0, 45], color=\"red\", linestyle=\"--\")\n",
    "axs[1].set_xlim(0, 45)\n",
    "axs[1].set_ylim(0, 45)\n",
    "axs[1].set_xlabel(\"Predicted height (m)\")\n",
    "axs[1].set_ylabel(\"True height (m)\")\n",
    "\n",
    "# Residuals Histogram\n",
    "residuals = y_test - y_pred\n",
    "axs[2].hist(residuals, bins=20)\n",
    "axs[2].plot([0, 0], [0, 6500], color=\"red\", linestyle=\"--\")\n",
    "axs[2].set_xlabel(\"Residuals (m)\")\n",
    "axs[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Give each subplot a letter\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.text(\n",
    "        0.1,\n",
    "        0.9,\n",
    "        string.ascii_uppercase[i],\n",
    "        transform=ax.transAxes,\n",
    "        size=12,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "\n",
    "# fig.suptitle(\n",
    "#     f\"Prediction of Tree Height [m] \\n RMSE: {round(rmse,1)} | MAE: {round(mae,1)} | R2: {round(r2, 2)}\",\n",
    "#     fontsize=14,\n",
    "#     fontweight=\"bold\",\n",
    "# )\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "# Turn\n",
    "# Plot variable importance - too many levels if not aggregating!\n",
    "# importances = rf.feature_importances_\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# names = [X.columns[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "fi = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": X.columns,\n",
    "        \"importance\": rf.feature_importances_,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sum up importance for species and genus\n",
    "agg_vars = []\n",
    "for col in ohe_dict.keys():\n",
    "    agg_vars.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"feature\": col,\n",
    "                \"importance\": fi.query(f\"feature in @ohe_dict[@col]\")[\n",
    "                    \"importance\"\n",
    "                ].sum(),\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "    )\n",
    "agg_vars = pd.concat(agg_vars)\n",
    "agg_vars\n",
    "fi = pd.concat([fi, agg_vars]).reset_index(drop=True)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    data=fi.sort_values(\"importance\", ascending=False).head(10),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Add percentages to the right\n",
    "for i in ax.patches:\n",
    "    ax.text(\n",
    "        i.get_width() + 0.005,\n",
    "        i.get_y() + i.get_height() / 2,\n",
    "        f\"{round(i.get_width()*100, 2)}%\",\n",
    "        fontsize=10,\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Feature Importance for Tree Height Prediction\")\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Use model to predict htot for all trees\n",
    "X_all = df_tmp.drop(columns=[\"htot\", \"tree_id\"])\n",
    "y_all = df_tmp[\"htot\"]\n",
    "\n",
    "# Predict\n",
    "y_pred_all = rf.predict(X_all)\n",
    "\n",
    "# Add to dataframe\n",
    "df_estimated_htot = df_tmp.copy()\n",
    "df_estimated_htot[\"htot_pred\"] = y_pred_all\n",
    "\n",
    "# Calculate difference between predicted and true htot\n",
    "df_estimated_htot[\"htot_diff\"] = (\n",
    "    df_estimated_htot[\"htot_pred\"] - df_estimated_htot[\"htot\"]\n",
    ")\n",
    "\n",
    "# Record where htot is replaced by predicted htot\n",
    "# Replace where htot is missing\n",
    "# Replace where tree_state_change is alive_alive, alive_cut, or alive_dead\n",
    "df_estimated_htot[\"htot_replaced_by_prediction\"] = df_estimated_htot[\"htot\"].isna()\n",
    "\n",
    "\n",
    "# Attach relevant columns from nfi_tree_addinfo\n",
    "df_estimated_htot = df_estimated_htot.merge(\n",
    "    nfi_tree_addinfo[\n",
    "        [\n",
    "            \"tree_id\",\n",
    "            \"tree_state_change\",\n",
    "            \"tree_circumference_class\",\n",
    "            \"tree_class\",\n",
    "            \"genus_lat\",\n",
    "            \"campagne_1\",\n",
    "            \"dbh_1\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"tree_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Plot distribution of difference per tree_state_change\n",
    "for class_ in df_estimated_htot[\"tree_state_change\"].unique():\n",
    "    df_i = df_estimated_htot.query(\"tree_state_change == @class_\")\n",
    "    sns.kdeplot(df_i[\"htot_diff\"], label=class_, fill=True, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print number of trees with htot replaced by prediction\n",
    "display(\n",
    "    round(\n",
    "        df_estimated_htot[[\"tree_state_change\", \"htot_replaced_by_prediction\"]]\n",
    "        .value_counts(normalize=True)\n",
    "        .sort_index()\n",
    "        * 100\n",
    "    )\n",
    ")\n",
    "\n",
    "# Replace and clean -----------------------------------------------------------\n",
    "\n",
    "# Replace htot with predicted htot where htot is missing and where tree is alive_alive, alive_cut, or alive_dead\n",
    "df_estimated_htot[\"htot_final\"] = df_estimated_htot[\"htot\"]\n",
    "conditions = (df_estimated_htot[\"htot_final\"].isna()) & (\n",
    "    df_estimated_htot[\"tree_state_change\"].isin(\n",
    "        [\"alive_alive\", \"alive_cut\", \"alive_dead\"]\n",
    "    )\n",
    ")\n",
    "df_estimated_htot.loc[conditions, \"htot_final\"] = df_estimated_htot.loc[\n",
    "    conditions, \"htot_pred\"\n",
    "]\n",
    "\n",
    "# Reduce df to make easier to read\n",
    "df_biomass = df_estimated_htot.copy()[\n",
    "    [\n",
    "        \"tree_id\",\n",
    "        \"htot\",\n",
    "        \"htot_pred\",\n",
    "        \"htot_diff\",\n",
    "        \"htot_final\",\n",
    "        \"htot_replaced_by_prediction\",\n",
    "        \"tree_state_change\",\n",
    "        \"c13_1\",\n",
    "        \"dbh_1\",\n",
    "        \"genus_lat\",\n",
    "        \"tree_class\",\n",
    "        \"v\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Calculation of Volume not needed because already given in the dataset...\n",
    "# # Calculate biomass for each tree\n",
    "# df_biomass[\"volume_calc\"] = (\n",
    "#     (math.pi / 40.0)\n",
    "#     * df_biomass[\"dbh_1\"]\n",
    "#     * df_biomass[\"dbh_1\"]\n",
    "#     * df_biomass[\"htot_final\"]\n",
    "# )\n",
    "\n",
    "# # Compare volume_calc with measured volume\n",
    "# df_biomass[\"volume_diff\"] = df_biomass[\"v\"] - df_biomass[\"volume_calc\"]\n",
    "\n",
    "# # Plot distribution of difference per tree_state_change\n",
    "# for class_ in df_biomass[\"tree_state_change\"].unique():\n",
    "#     df_i = df_biomass.query(\"tree_state_change == @class_\")\n",
    "#     sns.kdeplot(df_i[\"volume_diff\"], label=class_, fill=True, alpha=0.5)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: There are a few trees with extremely low height values, even for alive trees\n",
    "# I am not sure what is off with these, maybe tetards (nope, checked it...)\n",
    "# Anyways, there is no reason to remove these variables, so I keep them and acknowledge their noise.\n",
    "df_biomass.sort_values(\"htot_diff\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach height information to nfi_tree_addinfo\n",
    "nfi_tree_addinfo = nfi_tree_addinfo.merge(\n",
    "    df_biomass[[\"tree_id\", \"htot_final\", \"htot_replaced_by_prediction\"]],\n",
    "    on=\"tree_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Add height class\n",
    "nfi_tree_addinfo[\"tree_height_class\"] = (\n",
    "    pd.cut(\n",
    "        nfi_tree_addinfo[\"htot_final\"],\n",
    "        bins=[0, 10, 15, 20, 25, np.Inf],\n",
    "        labels=[\"0-10\", \"10-15\", \"15-20\", \"20-25\", \"25+\"],\n",
    "    )\n",
    "    .cat.add_categories(\"Missing\")\n",
    "    .fillna(\"Missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site-level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfi_site_addinfo = nfi_site_filtered.copy()\n",
    "\n",
    "# ! ADD INFORMATION ON GRECO AND SER --------------------------------------------\n",
    "# ! Note: Takes about 2 minutes to run\n",
    "# I am matching all regional information using the noisy coordinates to facilitate this.\n",
    "# Correcting the coordinates should not cause the site to fall into another region. And if so\n",
    "# it would only happen for a small subset of sites. So this is fine.\n",
    "# TODO: Maybe move this up to where I save the coordinates file so that the information is in there too...\n",
    "# Multiprocess to add regional information\n",
    "from utilities import attach_regional_information\n",
    "\n",
    "df_list = split_df_into_list_of_group_or_ns(nfi_site_addinfo, 10)\n",
    "nfi_site_addinfo = run_mp(attach_regional_information, df_list, pd.concat, num_cores=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Share of trees alive at first visit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of alive trees at first visit\n",
    "df_share_alive = share_alive_trees_mp(nfi_tree_addinfo)\n",
    "nfi_site_addinfo = nfi_site_addinfo.merge(df_share_alive, on=\"idp\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Share of small trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of trees with dbh > 7.5cm\n",
    "df_share_small = share_larger75dbh_trees_mp(nfi_tree_addinfo)\n",
    "nfi_site_addinfo = nfi_site_addinfo.merge(df_share_small, on=\"idp\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Fixes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing missing species and genus info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixing missing entries for species_lat\n",
    "nfi_tree_addinfo[\"species_lat\"] = nfi_tree_addinfo[\"species_lat\"].cat.add_categories(\n",
    "    [\"Other\", \"Pinus brutia\", \"Salix alba\"]\n",
    ")\n",
    "\n",
    "nfi_tree_addinfo.loc[nfi_tree_addinfo[\"espar\"] == \"57B\", \"species_lat\"] = \"Pinus brutia\"\n",
    "nfi_tree_addinfo.loc[nfi_tree_addinfo[\"espar\"] == \"25B\", \"species_lat\"] = \"Salix alba\"\n",
    "\n",
    "\n",
    "## Fixing missing entries for genus_lat\n",
    "nfi_tree_addinfo[\"genus_lat\"] = nfi_tree_addinfo[\"genus_lat\"].cat.add_categories(\n",
    "    [\"Aria\", \"Other\"]\n",
    ")\n",
    "\n",
    "nfi_tree_addinfo.loc[\n",
    "    nfi_tree_addinfo[\"species_lat\"] == \"Sorbus aria (Alisier blanc)\", \"genus_lat\"\n",
    "] = \"Aria\"\n",
    "nfi_tree_addinfo.loc[nfi_tree_addinfo[\"species_lat\"] == \"Salix alba\", \"genus_lat\"] = (\n",
    "    \"Salix\"\n",
    ")\n",
    "nfi_tree_addinfo.loc[\n",
    "    nfi_tree_addinfo[\"species_lat\"] == \"Rhamnus alpina (Nerprun des Alpes)\", \"genus_lat\"\n",
    "] = \"Rhamnus\"\n",
    "nfi_tree_addinfo.loc[nfi_tree_addinfo[\"species_lat\"] == \"Pinus brutia\", \"genus_lat\"] = (\n",
    "    \"Pinus\"\n",
    ")\n",
    "nfi_tree_addinfo.loc[\n",
    "    nfi_tree_addinfo[\"species_lat\"] == \"Magnoliopsida\", \"genus_lat\"\n",
    "] = \"Other\"\n",
    "\n",
    "# Reducing species_lat to first two words (or three if it is a hybrid)\n",
    "# Also, attach short name of first three letters of first and second word\n",
    "# (ignore x for hybrids, only first three for not specified species)\n",
    "for i in nfi_tree_addinfo.index:\n",
    "\n",
    "    # Unspecified species\n",
    "    if \" \" not in nfi_tree_addinfo.loc[i, \"species_lat\"]:\n",
    "        nfi_tree_addinfo.loc[i, \"species_lat2\"] = nfi_tree_addinfo.loc[i, \"species_lat\"]\n",
    "        nfi_tree_addinfo.loc[i, \"species_lat_short\"] = (\n",
    "            nfi_tree_addinfo.loc[i, \"genus_lat\"][:3] + \"_genus\"\n",
    "        )\n",
    "\n",
    "    # Hybrid species\n",
    "    elif \" x\" in nfi_tree_addinfo.loc[i, \"species_lat\"]:\n",
    "        first_ = nfi_tree_addinfo.loc[i, \"species_lat\"].split(\" \")[0]\n",
    "        second_ = nfi_tree_addinfo.loc[i, \"species_lat\"].split(\" \")[2]\n",
    "        nfi_tree_addinfo.loc[i, \"species_lat2\"] = \" x \".join([first_, second_])\n",
    "        nfi_tree_addinfo.loc[i, \"species_lat_short\"] = \"x\".join(\n",
    "            [first_[:5].upper(), second_[:5].upper()]\n",
    "        )\n",
    "\n",
    "    # Proper species\n",
    "    else:\n",
    "        first_ = nfi_tree_addinfo.loc[i, \"species_lat\"].split(\" \")[0]\n",
    "        second_ = nfi_tree_addinfo.loc[i, \"species_lat\"].split(\" \")[1]\n",
    "        nfi_tree_addinfo.loc[i, \"species_lat2\"] = \" \".join([first_, second_])\n",
    "        nfi_tree_addinfo.loc[i, \"species_lat_short\"] = \"\".join(\n",
    "            [first_[:5].upper(), second_[:5].upper()]\n",
    "        )\n",
    "\n",
    "nfi_tree_addinfo[\"species_lat2\"] = nfi_tree_addinfo[\"species_lat2\"].astype(\"category\")\n",
    "nfi_tree_addinfo[\"species_lat_short\"] = nfi_tree_addinfo[\"species_lat_short\"].astype(\n",
    "    \"category\"\n",
    ")\n",
    "\n",
    "# Check if number of unique species stayed the same\n",
    "df_xxx = nfi_tree_addinfo.copy()\n",
    "cols = [\n",
    "    \"species_lat\",\n",
    "    \"species_lat2\",\n",
    "    \"species_lat_short\",\n",
    "]\n",
    "cols_count = [f\"{col}_count\" for col in cols]\n",
    "\n",
    "df_xxx = df_xxx[[\"tree_id\"] + cols]\n",
    "\n",
    "# Attach counts to each column\n",
    "for col in cols:\n",
    "    df_tmp = df_xxx[col].value_counts().reset_index()\n",
    "    df_tmp.columns = [col, f\"{col}_count\"]\n",
    "    df_xxx = df_xxx.merge(df_tmp, on=col, how=\"left\")\n",
    "\n",
    "# Filter where counts are not equal\n",
    "print(\"Merged species lat 2 information:\")\n",
    "display(\n",
    "    df_xxx.query(\"species_lat_count != species_lat2_count\")[cols + cols_count]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"species_lat2\")\n",
    ")\n",
    "print(\"Merged species lat short information:\")\n",
    "display(\n",
    "    df_xxx.query(\"species_lat_count != species_lat_short_count\")[cols + cols_count]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"species_lat_short\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sites where there were only dead trees at first visit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sites_without_alive_trees(df):\n",
    "    # Group the dataframe by site ID\n",
    "    grouped = df.groupby(\"idp\")\n",
    "\n",
    "    # Check if there is at least one tree with 'tree_state_1' == 'alive' in each site\n",
    "    alive_trees = grouped.apply(lambda x: any(x[\"tree_state_1\"] == \"alive\"))\n",
    "\n",
    "    # Get the site IDs where there is at least one alive tree\n",
    "    sites_with_alive_trees = alive_trees[alive_trees].index\n",
    "\n",
    "    # Filter the dataframe to keep only the sites with alive trees\n",
    "    filtered_df = df[df[\"idp\"].isin(sites_with_alive_trees)]\n",
    "\n",
    "    # Report\n",
    "    print(f\" - Number of sites before filtering: {df['idp'].nunique()}\")\n",
    "    print(f\" - Number of sites after filtering: {filtered_df['idp'].nunique()}\")\n",
    "    print(\n",
    "        f\" - Number of sites removed: {df['idp'].nunique() - filtered_df['idp'].nunique()}\"\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "nfi_tree_addinfo = remove_sites_without_alive_trees(nfi_tree_addinfo).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge into final df ----------------------------------------------------------\n",
    "# Make sure idp is numeric for merging...\n",
    "nfi_tree_addinfo[\"idp\"] = nfi_tree_addinfo[\"idp\"].astype(\"int32\")\n",
    "nfi_site_addinfo[\"idp\"] = nfi_site_addinfo[\"idp\"].astype(\"int32\")\n",
    "df_final = pd.merge(\n",
    "    nfi_tree_addinfo, nfi_site_addinfo, how=\"left\", on=[\"idp\", \"campagne_1\"]\n",
    ")\n",
    "\n",
    "# Check if no variables have been duplicated, indicated by _x suffix\n",
    "for col in df_final.columns:\n",
    "    if col.endswith(\"_x\"):\n",
    "        print(f\"Warning: {col} has been duplicated!\")\n",
    "\n",
    "print(f\"Shape of nfi_tree_addinfo: \\t{nfi_tree_addinfo.shape}\")\n",
    "print(f\"Shape of nfi_site_addinfo: \\t{nfi_site_addinfo.shape}\")\n",
    "print(f\"Shape of final df: \\t\\t{df_final.shape}\")\n",
    "print(f\"Number of trees in final df: \\t{df_final['tree_id'].nunique()}\")\n",
    "print(f\"Number of sites in final df: \\t{df_final['idp'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving output from 2024-01-06\n",
    "# Shape of nfi_tree_addinfo: \t(549255, 73)\n",
    "# Shape of nfi_site_addinfo: \t(40231, 122)\n",
    "# Shape of final df: \t\t    (549255, 193)\n",
    "\n",
    "# Saving output from 2024-01-12\n",
    "# Shape of nfi_tree_addinfo: \t(617679, 73)\n",
    "# Shape of nfi_site_addinfo: \t(45121, 122)\n",
    "# Shape of final df:     \t\t(617679, 193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged file at tree-level\n",
    "os.makedirs(here(\"data/final/nfi\"), exist_ok=True)\n",
    "df_final.to_feather(here(\"data/final/nfi/nfi_ready_for_analysis.feather\"))\n",
    "df_final.to_csv(here(\"data/final/nfi/nfi_ready_for_analysis.csv\"), index=False)\n",
    "# display(df_final)\n",
    "\n",
    "# Save tree-level file\n",
    "nfi_tree_addinfo.to_feather(here(\"data/final/nfi/nfi_tree_information_raw.feather\"))\n",
    "\n",
    "# Save site-level file\n",
    "nfi_site_addinfo.to_feather(here(\"data/final/nfi/nfi_site_information_raw.feather\"))\n",
    "# display(nfi_site_addinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Years included: {df_final['campagne_1'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime.success()\n",
    "raise ValueError(\n",
    "    f\"Script finished on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read latest version\n",
    "from utilities import get_final_nfi_data_for_analysis\n",
    "\n",
    "df_final = get_final_nfi_data_for_analysis()\n",
    "print(df_final.shape)\n",
    "df_final\n",
    "\n",
    "# Load tree-level file\n",
    "nfi_tree_addinfo = pd.read_feather(\n",
    "    here(\"data/final/nfi/nfi_tree_information_raw.feather\")\n",
    ")\n",
    "\n",
    "# Load site-level file\n",
    "nfi_site_addinfo = pd.read_feather(\n",
    "    here(\"data/final/nfi/nfi_site_information_raw.feather\")\n",
    ")\n",
    "# display(nfi_site_addinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.species_lat_short.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis below\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate of missing values in htot, volume, c13_1\n",
    "display(df_final[[\"htot\", \"v\", \"c13_1\", \"age13\"]].isna().mean().round(3) * 100)\n",
    "\n",
    "# Redo analysis but only for tree that died\n",
    "df_final_dead = df_final.query(\"tree_state_change == 'alive_dead'\").copy()\n",
    "df_final_dead[[\"htot\", \"v\", \"c13_1\", \"age13\"]].isna().mean().round(3) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.campagne_1.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees and sites\n",
    "print(f\" - Number of trees: {df_final.shape[0]}\")\n",
    "print(f\" - Number of sites: {df_final['idp'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a site that has only NA in ba_1? This affects dominance calculation...\n",
    "# Answer: No, probably because I am removing these simplified trees\n",
    "df_final.ba_1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree_state_change\n",
    "display(\n",
    "    \"Subsetting trees that were alive at first visit and see how they survived, died, were cut:\"\n",
    ")\n",
    "display(df_final.query(\"tree_state_1 == 'alive'\").shape)\n",
    "display(\n",
    "    df_final.query(\"tree_state_1 == 'alive'\").tree_state_change.value_counts(\n",
    "        dropna=False\n",
    "    )\n",
    ")\n",
    "display(\n",
    "    round(\n",
    "        df_final.query(\"tree_state_1 == 'alive'\").tree_state_change.value_counts(\n",
    "            dropna=False, normalize=True\n",
    "        )\n",
    "        * 100,\n",
    "        2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree_state_change\n",
    "display(\"Subsetting trees that were alive at first visit and see how they recruited:\")\n",
    "display(df_final.query(\"tree_state_1 == 'new'\").shape)\n",
    "display(\n",
    "    df_final.query(\"tree_state_1 == 'new'\").tree_state_change.value_counts(dropna=False)\n",
    ")\n",
    "display(\n",
    "    round(\n",
    "        df_final.query(\"tree_state_1 == 'new'\").tree_state_change.value_counts(\n",
    "            dropna=False, normalize=True\n",
    "        )\n",
    "        * 100,\n",
    "        2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages of sites that showed mortality, cut, and recruitment\n",
    "display(\n",
    "    df_final.query(\n",
    "        \"tree_state_change in ['alive_alive', 'alive_dead', 'alive_cut', 'new_alive']\"\n",
    "    ).idp.nunique()\n",
    ")\n",
    "\n",
    "sites_mort = df_final.query(\"tree_state_change == 'alive_dead'\").idp.nunique()\n",
    "sites_mort_prc = round(sites_mort / df_final.idp.nunique() * 100, 2)\n",
    "\n",
    "sites_cut = df_final.query(\"tree_state_change == 'alive_cut'\").idp.nunique()\n",
    "sites_cut_prc = round(sites_cut / df_final.idp.nunique() * 100, 2)\n",
    "\n",
    "sites_rec = df_final.query(\"tree_state_change == 'new_alive'\").idp.nunique()\n",
    "sites_rec_prc = round(sites_rec / df_final.idp.nunique() * 100, 2)\n",
    "\n",
    "print(f\" - Number of sites with mortality: {sites_mort} ({sites_mort_prc}%)\")\n",
    "print(\" - Number of sites with cut: \", sites_cut, f\"({sites_cut_prc}%)\")\n",
    "print(\" - Number of sites with recruitment: \", sites_rec, f\"({sites_rec_prc}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of number of trees per site\n",
    "df_tmp = df_final.groupby(\"idp\").size().reset_index()\n",
    "df_tmp.columns = [\"idp\", \"n_trees\"]\n",
    "print(\"Number of trees per site: \")\n",
    "display(df_tmp.n_trees.value_counts().sort_index().plot(kind=\"bar\", figsize=(15, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of species\n",
    "print(\"Distribution of species: \")\n",
    "display(\n",
    "    df_final[\"genus_lat\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .sort_values(ascending=False)\n",
    "    .head(20)\n",
    "    / df_final.shape[0]\n",
    "    * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of species\n",
    "print(\"Distribution of species: \")\n",
    "display(\n",
    "    df_final[\"species_lat\"]\n",
    "    .value_counts(dropna=False)\n",
    "    .sort_values(ascending=False)\n",
    "    .head(20)\n",
    "    / df_final.shape[0]\n",
    "    * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Species Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tree-level file\n",
    "df_tree = get_final_nfi_data_for_analysis()\n",
    "df_site = pd.read_feather(\n",
    "    here(\"data/final/predictor_datasets/nfi_site_information.feather\")\n",
    ")\n",
    "\n",
    "# Merge them by idp\n",
    "df = pd.merge(\n",
    "    df_tree[[\"idp\", \"tree_id\", \"tree_state_change\", \"species_lat\", \"genus_lat\"]],\n",
    "    df_site[[\"idp\", \"ser\", \"gre\", \"reg\", \"dep\", \"hex\"]],\n",
    "    how=\"left\",\n",
    "    on=[\"idp\"],\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get hexagon shapefile\n",
    "which_shape = \"hex\"\n",
    "region_shp = get_shp_of_region(which_shape)\n",
    "\n",
    "# Attach site data to have information on hex\n",
    "top10genus = df[\"genus_lat\"].value_counts().sort_values(ascending=False).head(10).index\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(2, 5, figsize=(24, 8))\n",
    "fig_pos = 0\n",
    "\n",
    "for x in top10genus:\n",
    "    df_x = df.query(\"genus_lat == @x\")\n",
    "\n",
    "    # Group by hex and get number of trees per group\n",
    "    df_x = df_x.groupby(which_shape).size().reset_index()\n",
    "    df_x.columns = [which_shape, \"n_trees\"]\n",
    "\n",
    "    # Attach geometry\n",
    "    df_x = df_x.merge(region_shp, on=which_shape, how=\"outer\")\n",
    "\n",
    "    # Plot geometry filled with n_trees on the first subplot\n",
    "    gdf_plot = gpd.GeoDataFrame(df_x)\n",
    "    gdf_plot.plot(\n",
    "        column=\"n_trees\", legend=True, ax=axs[fig_pos // 5, fig_pos % 5]\n",
    "    )  # Adjusted indexing\n",
    "    axs[fig_pos // 5, fig_pos % 5].set_title(f\"Distribution of {x} per {which_shape}\")\n",
    "\n",
    "    fig_pos += 1\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get hexagon shapefile\n",
    "shp_hex = load_hexmap()\n",
    "\n",
    "# Attach site data to have information on hex\n",
    "df_tmp = pd.merge(nfi_tree_addinfo, nfi_site_addinfo, how=\"left\")\n",
    "\n",
    "# Group by hex and get number of trees per group\n",
    "df_tmp_trees = df_tmp.groupby(\"hex\").size().reset_index()\n",
    "df_tmp_trees.columns = [\"hex\", \"n_trees\"]\n",
    "\n",
    "# Attach hex shapefile\n",
    "df_tmp_trees = pd.merge(df_tmp_trees, shp_hex, how=\"right\")\n",
    "\n",
    "# Group by hex and get number of sites per group\n",
    "df_tmp_sites = nfi_site_addinfo.groupby(\"hex\").size().reset_index()\n",
    "df_tmp_sites.columns = [\"hex\", \"n_sites\"]\n",
    "\n",
    "# Attach hex shapefile\n",
    "df_tmp_sites = pd.merge(df_tmp_sites, shp_hex, how=\"right\")\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot geometry filled with n_trees on the first subplot\n",
    "gpd.GeoDataFrame(df_tmp_trees).plot(column=\"n_trees\", legend=True, ax=axs[0])\n",
    "axs[0].set_title(\"Number of Trees per Hex\")\n",
    "\n",
    "# Plot geometry filled with n_sites on the second subplot\n",
    "gpd.GeoDataFrame(df_tmp_sites).plot(column=\"n_sites\", legend=True, ax=axs[1])\n",
    "axs[1].set_title(\"Number of Sites per Hex\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Thinning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the number of trees per site change with the mean dbh per site?\n",
    "df_tmp = nfi_tree_addinfo.groupby(\"idp\").agg({\"dbh_1\": \"mean\", \"idp\": \"size\"})\n",
    "df_tmp.columns = [\"mean_dbh\", \"n_trees\"]\n",
    "# Make smooth scatterplot\n",
    "sns.kdeplot(\n",
    "    data=df_tmp,\n",
    "    y=\"mean_dbh\",\n",
    "    x=\"n_trees\",\n",
    "    cmap=\"mako\",\n",
    "    thresh=0,\n",
    "    levels=100,\n",
    "    fill=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Get hexagon shapefile\n",
    "shp_hex = load_hexmap()\n",
    "\n",
    "# Attach site data to have information on hex\n",
    "df_tmp = pd.merge(nfi_tree_addinfo, nfi_site_addinfo, how=\"left\")\n",
    "\n",
    "# Group by hex and get number of trees per group\n",
    "df_tmp_trees = df_tmp.groupby(\"hex\").size().reset_index()\n",
    "df_tmp_trees.columns = [\"hex\", \"n_trees\"]\n",
    "\n",
    "# Attach hex shapefile\n",
    "df_tmp_trees = pd.merge(df_tmp_trees, shp_hex, how=\"right\")\n",
    "\n",
    "# Group by hex and get number of sites per group\n",
    "df_tmp_sites = nfi_site_addinfo.groupby(\"hex\").size().reset_index()\n",
    "df_tmp_sites.columns = [\"hex\", \"n_sites\"]\n",
    "\n",
    "# Attach hex shapefile\n",
    "df_tmp_sites = pd.merge(df_tmp_sites, shp_hex, how=\"right\")\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot geometry filled with n_trees on the first subplot\n",
    "gpd.GeoDataFrame(df_tmp_trees).plot(column=\"n_trees\", legend=True, ax=axs[0])\n",
    "axs[0].set_title(\"Number of Trees per Hex\")\n",
    "\n",
    "# Plot geometry filled with n_sites on the second subplot\n",
    "gpd.GeoDataFrame(df_tmp_sites).plot(column=\"n_sites\", legend=True, ax=axs[1])\n",
    "axs[1].set_title(\"Number of Sites per Hex\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_final.groupby([\"simplif\", \"campagne_1\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"simplif\", index=\"campagne_1\", values=0)\n",
    "    .plot(kind=\"bar\", stacked=True)\n",
    ")\n",
    "\n",
    "(\n",
    "    df_final.groupby([\"simplif\", \"tree_state_change\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"simplif\", index=\"tree_state_change\", values=0)\n",
    "    .plot(kind=\"bar\", stacked=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = df_final.query(\"simplif == '1'\")\n",
    "print(xxx.shape)\n",
    "print(xxx.htot.isna().sum())\n",
    "xxx.head(20)[\n",
    "    [\n",
    "        \"idp\",\n",
    "        \"tree_id\",\n",
    "        \"c13_1\",\n",
    "        \"c13_2\",\n",
    "        \"sfgui_1\",\n",
    "        \"lib\",\n",
    "        \"ir5\",\n",
    "        \"v\",\n",
    "        \"htot\",\n",
    "        \"tree_state_change\",\n",
    "        \"cible\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all simplified trees have no cible information?\n",
    "xxx.cible.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees with missing cible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.cible.value_counts(dropna=False, normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_final.groupby([\"cible\", \"campagne_1\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"cible\", index=\"campagne_1\", values=0)\n",
    "    .plot(kind=\"bar\", stacked=True)\n",
    ")\n",
    "\n",
    "(\n",
    "    df_final.groupby([\"cible\", \"tree_state_change\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"cible\", index=\"tree_state_change\", values=0)\n",
    "    .plot(kind=\"bar\", stacked=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplified trees, do they have NA in ba_1 and ba_2?\n",
    "# Get timespan of interest\n",
    "df_timespan = df_final.copy().query(\"campagne_1 >= 2010 & campagne_1 <= 2018\")\n",
    "df_timespan[\"idp\"] = df_timespan[\"idp\"].astype(\"int32\")\n",
    "df_timespan.insert(0, \"tree_id\", df_timespan.pop(\"tree_id\"))\n",
    "\n",
    "# Split into simplified and non-simplified trees\n",
    "df_simplified_trees = df_timespan.query(\"simplif == '1'\")\n",
    "df_nonsimplified_trees = df_timespan.query(\"simplif != '1'\")\n",
    "\n",
    "display(f\"Shape of simplified trees: {df_simplified_trees.shape}\")\n",
    "display(df_simplified_trees.tree_state_change.value_counts())\n",
    "display(df_simplified_trees.campagne_1.value_counts().sort_index())\n",
    "display(\n",
    "    df_simplified_trees[[\"campagne_1\", \"tree_state_change\"]].value_counts().sort_index()\n",
    ")\n",
    "display(\n",
    "    df_simplified_trees[[\"ba_1\", \"ba_2\"]]\n",
    "    .isna()\n",
    "    .value_counts(dropna=False, normalize=True)\n",
    ")\n",
    "# * Only 10 percent of all simplified trees have no ba_2 values. So they can still be useful!\n",
    "\n",
    "# Number of trees per site\n",
    "df_tmp = df_simplified_trees.groupby(\"idp\").size().reset_index()\n",
    "df_tmp.columns = [\"idp\", \"n_trees\"]\n",
    "print(\"Number of trees per site: \")\n",
    "display(df_tmp.n_trees.value_counts().sort_index().plot(kind=\"bar\", figsize=(15, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.plisi.value_counts(dropna=False, normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_final.ir5.isna().sum() / df_final.shape[0] * 100)\n",
    "round(df_final.c13_2.isna().sum() / df_final.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Incidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nincid_dic = {\n",
    "    \"0\": \"None\",\n",
    "    \"1\": \"Fire\",\n",
    "    \"2\": \"Mortality\",\n",
    "    \"3\": \"Landslide\",\n",
    "    \"4\": \"Storm\",\n",
    "    \"5\": \"Other\",\n",
    "    \"Missing\": \"Missing\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_in_final_anlysis = get_species_with_models(\"list\")\n",
    "top9 = df_final.species_lat2.value_counts().head(9).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_final.shape)\n",
    "display(df_final.query(\"species_lat2 in @species_in_final_anlysis\").shape)\n",
    "display(\n",
    "    df_final.query(\"species_lat2 in @species_in_final_anlysis\")[\n",
    "        \"tree_state_change\"\n",
    "    ].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary\n",
    "veget5_dict = {\n",
    "    \"0\": \"alive\",\n",
    "    \"1\": \"dead_thrown\",\n",
    "    \"2\": \"dead_thrown\",\n",
    "    \"5\": \"dead_standing\",\n",
    "    \"6\": \"cut\",\n",
    "    \"7\": \"cut\",\n",
    "    \"A\": \"dead_thrown\",\n",
    "    \"C\": \"dead_thrown\",\n",
    "    \"M\": \"dead_standing\",\n",
    "    \"T\": \"dead_uprooted\",\n",
    "    \"N\": \"NA\",\n",
    "    \"Z\": \"alive_injured\",\n",
    "}\n",
    "\n",
    "# Replace values\n",
    "tmp = df_final.copy().query(\"tree_state_change == 'alive_dead'\")\n",
    "tmp[\"veget5\"] = tmp[\"veget5\"].map(veget5_dict)\n",
    "\n",
    "for subset in [\"all\", \"only_analyzed\", \"top9\"]:\n",
    "\n",
    "    if subset == \"only_analyzed\":\n",
    "        tmp = tmp.query(\"species_lat2 in @species_in_final_anlysis\")\n",
    "    elif subset == \"top9\":\n",
    "        tmp = tmp.query(\"species_lat2 in @top9\")\n",
    "\n",
    "    display(\"--------------------------------------------------------------------\")\n",
    "    display(\"Natural incidence distribution across trees THAT DIED BETWEEN VISITS\")\n",
    "    print(f\"Subset: {subset}\")\n",
    "    print(\"Distribution across all trees:\")\n",
    "\n",
    "    display(\n",
    "        (tmp[\"nincid_2\"].map(nincid_dic).value_counts(normalize=True) * 100)\n",
    "        .round(1)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # How many sites and trees where affected by natural incidences?\n",
    "    print(\"Distribution across all sites:\")\n",
    "    display(\n",
    "        (\n",
    "            tmp[[\"idp\", \"nincid_2\"]]\n",
    "            .drop_duplicates()[\"nincid_2\"]\n",
    "            .map(nincid_dic)\n",
    "            .value_counts(normalize=True)\n",
    "            * 100\n",
    "        )\n",
    "        .round(1)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # Display distribution of veget5\n",
    "    display(\"Recorded state of death for trees THAT DIED BETWEEN VISITS\")\n",
    "    display(\n",
    "        (\n",
    "            # tmp.query(\"tree_state_1 == 'alive'\")[\"veget5\"]\n",
    "            tmp.query(\"tree_state_change == 'alive_dead'\")[\"veget5\"]\n",
    "            .value_counts(normalize=True)\n",
    "            .sort_index()\n",
    "            * 100\n",
    "        ).round(1)\n",
    "    )\n",
    "    (\n",
    "        tmp.query(\"tree_state_change == 'alive_dead' and veget5=='dead_uprooted'\")[\n",
    "            \"campagne_2\"\n",
    "        ]\n",
    "        .value_counts(normalize=True)\n",
    "        .sort_index()\n",
    "        * 100\n",
    "    ).round(1).plot(\n",
    "        kind=\"bar\",\n",
    "        title=\"Percentage of trees that died and were recorded as thrown\",\n",
    "        ylabel=\"Percentage\",\n",
    "        xlabel=\"Year of 2nd Visit\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to create multiple plots for all the natural incident categories and see their intensity over time\n",
    "# I will create a plot for each natural incident category\n",
    "\n",
    "# Pick which visit should be monitored\n",
    "visit_nr = \"2\"\n",
    "\n",
    "# Use final dataset\n",
    "# df_tmp = nfi_site_addinfo.copy()\n",
    "\n",
    "# Use dataset without filtering, needs loading from above\n",
    "df_tmp = nfi_site_nodupes.copy()\n",
    "df_tmp[f\"nincid_{visit_nr}\"] = df_tmp[f\"nincid_{visit_nr}\"].fillna(\"Missing\")\n",
    "\n",
    "# Get all natural incident categories\n",
    "nincid_dic = {\n",
    "    \"0\": \"None\",\n",
    "    \"1\": \"Fire\",\n",
    "    \"2\": \"Mortality\",\n",
    "    \"3\": \"Landslide\",\n",
    "    \"4\": \"Storm\",\n",
    "    \"5\": \"Other\",\n",
    "    \"Missing\": \"Missing\",\n",
    "}\n",
    "nincid_cat = df_tmp[f\"nincid_{visit_nr}\"].unique()\n",
    "nincid_cat = [nincid_dic[str(x)] for x in nincid_cat]\n",
    "\n",
    "# Rename the natural incident categories\n",
    "df_tmp[f\"nincid_{visit_nr}\"] = df_tmp[f\"nincid_{visit_nr}\"].astype(str)\n",
    "df_tmp[f\"nincid_{visit_nr}\"] = df_tmp[f\"nincid_{visit_nr}\"].replace(nincid_dic)\n",
    "display(df_tmp[f\"nincid_{visit_nr}\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axs = plt.subplots(1, len(nincid_cat), figsize=(4 * len(nincid_cat), 5))\n",
    "\n",
    "# Loop over all natural incident categories\n",
    "for i, cat in enumerate(nincid_cat):\n",
    "    # Filter the dataframe for the current natural incident category\n",
    "    df_cat = df_tmp.query(f\"nincid_{visit_nr} == @cat\")\n",
    "\n",
    "    # Make a boxplot of the intensity on the x-axis and the year on the y-axis\n",
    "    sns.violinplot(data=df_cat, y=f\"incid_{visit_nr}\", x=\"campagne_1\", ax=axs[i])\n",
    "    axs[i].set_title(f\"Intensity of {cat}\")\n",
    "    # Set y axis from 0 to 4\n",
    "    axs[i].set_ylim(0, 5)\n",
    "\n",
    "    # Rotate x-axis labels\n",
    "    axs[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Display distribution of intensity\n",
    "    display(f\" - Distribution of INTENSITY for {cat}:\")\n",
    "    display(df_cat[f\"incid_{visit_nr}\"].value_counts(normalize=True).sort_index() * 100)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Divider\n",
    "display()\n",
    "\n",
    "# Check nicnid-campagne distribution\n",
    "df_tmp[[f\"nincid_{visit_nr}\", \"campagne_1\"]].value_counts(\n",
    "    normalize=True,\n",
    "    dropna=False,\n",
    ").sort_index().plot(kind=\"bar\", figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stand densities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.copy().query(\"species_lat2 in @species_in_final_anlysis\")[\n",
    "    [\"idp\", \"share_larger75dbh\"]\n",
    "].drop_duplicates()[\"share_larger75dbh\"].hist(bins=100)\n",
    "\n",
    "df_final.copy().query(\"species_lat2 in @species_in_final_anlysis\")[\n",
    "    [\"idp\", \"share_larger75dbh\"]\n",
    "].drop_duplicates()[\"share_larger75dbh\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"share_alive\"].hist(bins=100)\n",
    "df_final[\"share_alive\"].describe()\n",
    "df_final[\"share_alive\"].quantile([0.01, 0.05, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"tree_state_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
