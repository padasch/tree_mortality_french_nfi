{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Mortality Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Predictor datasets available:\n",
      " 1.\t digitalis_tmax            \t | Created:     3 days ago \t(2025-02-24)\n",
      " 2.\t digitalis_tmin            \t | Created:     3 days ago \t(2025-02-24)\n",
      " 3.\t digitalis_tmoy            \t | Created:     3 days ago \t(2025-02-24)\n",
      " 4.\t forest_biodiversity       \t | Created:   133 days ago \t(2024-10-17)\n",
      " 5.\t forest_carrying_capacity  \t | Created:   133 days ago \t(2024-10-17)\n",
      " 6.\t forest_competition        \t | Created:   133 days ago \t(2024-10-17)\n",
      " 7.\t forest_gini               \t | Created:   133 days ago \t(2024-10-17)\n",
      " 8.\t management                \t | Created:   132 days ago \t(2024-10-18)\n",
      " 9.\t ndvi                      \t | Created:   132 days ago \t(2024-10-18)\n",
      " 10.\t soil                      \t | Created:   133 days ago \t(2024-10-17)\n",
      " 11.\t spei_anom                 \t | Created:   130 days ago \t(2024-10-20)\n",
      " 12.\t topography                \t | Created:   133 days ago \t(2024-10-17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of available species and their percentages\n",
      "Fagus sylvatica           52379                          9.00%\n",
      "Quercus robur             51299                          8.82%\n",
      "Quercus petraea           50662                          8.71%\n",
      "Carpinus betulus          48409                          8.32%\n",
      "Castanea sativa           36756                          6.32%\n",
      "Quercus pubescens         35602                          6.12%\n",
      "Pinus sylvestris          29755                          5.11%\n",
      "Abies alba                29015                          4.99%\n",
      "Picea abies               27050                          4.65%\n",
      "Fraxinus excelsior        23312                          4.01%\n",
      "Quercus ilex              17954                          3.09%\n",
      "Pinus pinaster            17497                          3.01%\n",
      "Pseudotsuga menziesii     14629                          2.51%\n",
      "Betula pendula            13582                          2.33%\n",
      "Pinus nigra               10335                          1.78%\n",
      "Corylus avellana          10196                          1.75%\n",
      "Acer campestre            8430                           1.45%\n",
      "Robinia pseudoacacia      8413                           1.45%\n",
      "Acer pseudoplatanus       6907                           1.19%\n",
      "Alnus glutinosa           6845                           1.18%\n",
      "Populus tremula           6413                           1.10%\n",
      "Prunus avium              6248                           1.07%\n",
      "Salix caprea              5936                           1.02%\n",
      "Populus                   5868                           1.01%\n",
      "Crataegus monogyna        4142                           0.71%\n",
      "Pinus halepensis          3808                           0.65%\n",
      "Larix decidua             3463                           0.60%\n",
      "Sorbus aria               3198                           0.55%\n",
      "Sorbus torminalis         2919                           0.50%\n",
      "Tilia cordata             2825                           0.49%\n",
      "Ulmus minor               2586                           0.44%\n",
      "Arbutus unedo             2099                           0.36%\n",
      "Tilia platyphyllos        2077                           0.36%\n",
      "Pinus mugo                1905                           0.33%\n",
      "Ilex aquifolium           1721                           0.30%\n",
      "Quercus pyrenaica         1653                           0.28%\n",
      "Salix cinerea             1563                           0.27%\n",
      "Acer opalus               1520                           0.26%\n",
      "Quercus rubra             1509                           0.26%\n",
      "Betula pubescens          1359                           0.23%\n",
      "Picea sitchensis          1261                           0.22%\n",
      "Acer monspessulanum       1260                           0.22%\n",
      "Sorbus aucuparia          1204                           0.21%\n",
      "Quercus suber             1072                           0.18%\n",
      "Populus nigra             756                            0.13%\n",
      "Salix alba                720                            0.12%\n",
      "Buxus sempervirens        687                            0.12%\n",
      "Sambucus nigra            668                            0.11%\n",
      "Acer platanoides          654                            0.11%\n",
      "Fraxinus angustifolia     566                            0.10%\n",
      "Malus sylvestris          537                            0.09%\n",
      "Populus x canescens       522                            0.09%\n",
      "Juniperus communis        505                            0.09%\n",
      "Juniperus oxycedrus       489                            0.08%\n",
      "Prunus spinosa            469                            0.08%\n",
      "Phillyrea latifolia       467                            0.08%\n",
      "Larix kaempferi           450                            0.08%\n",
      "Cedrus atlantica          430                            0.07%\n",
      "Abies grandis             377                            0.06%\n",
      "Juglans regia             374                            0.06%\n",
      "Pyrus communis            359                            0.06%\n",
      "Pinus pinea               359                            0.06%\n",
      "Alnus incana              274                            0.05%\n",
      "Pinus strobus             273                            0.05%\n",
      "Ulmus glabra              256                            0.04%\n",
      "Ostrya carpinifolia       248                            0.04%\n",
      "Laburnum anagyroides      247                            0.04%\n",
      "Sorbus domestica          242                            0.04%\n",
      "Fraxinus ornus            241                            0.04%\n",
      "Prunus mahaleb            238                            0.04%\n",
      "Erica arborea             199                            0.03%\n",
      "Salix fragilis            179                            0.03%\n",
      "Quercus cerris            176                            0.03%\n",
      "Populus alba              175                            0.03%\n",
      "Salix atrocinerea         171                            0.03%\n",
      "Olea europaea             166                            0.03%\n",
      "Prunus padus              157                            0.03%\n",
      "Pinus cembra              140                            0.02%\n",
      "Taxus baccata             131                            0.02%\n",
      "Cornus mas                116                            0.02%\n",
      "Rhamnus cathartica        104                            0.02%\n",
      "Abies nordmanniana        101                            0.02%\n",
      "Prunus domestica          99                             0.02%\n",
      "Frangula alnus            87                             0.01%\n",
      "Acer negundo              83                             0.01%\n",
      "Pinus taeda               83                             0.01%\n",
      "Euonymus europaeus        83                             0.01%\n",
      "Prunus serotina           80                             0.01%\n",
      "Aesculus hippocastanum    80                             0.01%\n",
      "Magnoliopsida             76                             0.01%\n",
      "Thuja plicata             66                             0.01%\n",
      "Alnus cordata             58                             0.01%\n",
      "Pinus brutia              54                             0.01%\n",
      "Crataegus laevigata       54                             0.01%\n",
      "Laurus nobilis            53                             0.01%\n",
      "Eucalyptus                49                             0.01%\n",
      "Larix x marschlinsii      46                             0.01%\n",
      "Tsuga heterophylla        45                             0.01%\n",
      "Conifer                   38                             0.01%\n",
      "Platanus occidentalis     38                             0.01%\n",
      "Platanus x hispanica      36                             0.01%\n",
      "Cryptomeria japonica      33                             0.01%\n",
      "Pistacia terebinthus      33                             0.01%\n",
      "Sorbus mougeotii          32                             0.01%\n",
      "Juglans nigra             31                             0.01%\n",
      "Pyrus spinosa             30                             0.01%\n",
      "Ailanthus altissima       30                             0.01%\n",
      "Pinus radiata             30                             0.01%\n",
      "Ulmus laevis              27                             0.00%\n",
      "Sequoia sempervirens      26                             0.00%\n",
      "Chamaecyparis lawsoniana  26                             0.00%\n",
      "Alnus alnobetula          24                             0.00%\n",
      "Cupressus sempervirens    23                             0.00%\n",
      "Ficus carica              22                             0.00%\n",
      "Rhamnus alaternus         22                             0.00%\n",
      "Salix viminalis           20                             0.00%\n",
      "Abies concolor            20                             0.00%\n",
      "Pistacia lentiscus        20                             0.00%\n",
      "Tilia x europaea          19                             0.00%\n",
      "Rhamnus alpina            17                             0.00%\n",
      "Vachellia farnesiana      16                             0.00%\n",
      "Liriodendron tulipifera   16                             0.00%\n",
      "Prunus cerasifera         14                             0.00%\n",
      "Pinus contorta            13                             0.00%\n",
      "Quercus palustris         13                             0.00%\n",
      "Celtis australis          12                             0.00%\n",
      "Prunus cerasus            12                             0.00%\n",
      "Sorbus latifolia          11                             0.00%\n",
      "Cupressus arizonica       9                              0.00%\n",
      "Laburnum alpinum          9                              0.00%\n",
      "Sambucus racemosa         9                              0.00%\n",
      "Juniperus thurifera       8                              0.00%\n",
      "Salix eleagnos            8                              0.00%\n",
      "Cupressus macrocarpa      6                              0.00%\n",
      "Abies cephalonica         6                              0.00%\n",
      "Prunus dulcis             4                              0.00%\n",
      "Taxodium distichum        3                              0.00%\n",
      "Pyrus cordata             3                              0.00%\n",
      "Crataegus azarolus        3                              0.00%\n",
      "Salix x rubens            2                              0.00%\n",
      "Cedrus libani             2                              0.00%\n",
      "Phillyrea angustifolia    2                              0.00%\n",
      "Prunus brigantina         2                              0.00%\n",
      "Salix daphnoides          2                              0.00%\n",
      "Sorbus intermedia         2                              0.00%\n",
      "Rhus typhina              2                              0.00%\n",
      "Tamarix gallica           1                              0.00%\n",
      "Abies pinsapo             1                              0.00%\n",
      "Cydonia oblonga           1                              0.00%\n",
      "Salix triandra            1                              0.00%\n",
      "Morus alba                1                              0.00%\n",
      "Toxicodendron vernicifluum 1                              0.00%\n",
      "Cercis siliquastrum       1                              0.00%\n",
      "Cedrus brevifolia         1                              0.00%\n",
      "Casuarina equisetifolia   1                              0.00%\n",
      "Abies procera             0                              0.00%\n",
      "Pistacia vera             0                              0.00%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()\n",
    "\n",
    "from IPython.display import clear_output  # For clearing the output of a cell\n",
    "import json\n",
    "\n",
    "\n",
    "# List available data\n",
    "tmp = list_predictor_datasets(return_list=False)\n",
    "display(\"--------\")\n",
    "print(\"\\nList of available species and their percentages\")\n",
    "tmp = get_final_nfi_data_for_analysis(verbose=False).query(\n",
    "    \"tree_state_change in ['alive_alive', 'alive_dead']\"\n",
    ")\n",
    "# Get normalized and non normalized counts\n",
    "all_species = tmp[\"species_lat2\"].value_counts()\n",
    "all_species_norm = tmp[\"species_lat2\"].value_counts(normalize=True)\n",
    "for i in all_species.index:\n",
    "    print(f\"{i:25} {all_species[i]:<30} {all_species_norm[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(species, user_input, base_dir=None):\n",
    "\n",
    "    # ! Set and save user input settings ---------------------------------\n",
    "    # Set species\n",
    "    user_input[\"subset_group\"] = [species]\n",
    "\n",
    "    # ! Get current directory (create if non-existent)\n",
    "    if base_dir is None:\n",
    "        user_input[\"current_dir\"] = create_new_run_folder_treemort(\n",
    "            user_input[\"subset_group\"][0]\n",
    "        )\n",
    "    else:\n",
    "        if base_dir[-1] != \"/\":\n",
    "            base_dir += \"/\"\n",
    "        user_input[\"current_dir\"] = base_dir + species + \"/\"\n",
    "        os.makedirs(user_input[\"current_dir\"], exist_ok=True)\n",
    "\n",
    "    current_dir = user_input[\"current_dir\"]\n",
    "\n",
    "    # ! Skip run if already done\n",
    "    ffile = f\"{current_dir}/final_model_performance.csv\"\n",
    "    if os.path.exists(ffile):\n",
    "        print(f\"Skipping {species} as it already exists\")\n",
    "        return None\n",
    "\n",
    "    # ! Write settings to file\n",
    "    file_path = f\"{current_dir}/__user_input.txt\"\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for key, value in user_input.items():\n",
    "            if isinstance(value, list):\n",
    "                file.write(f\"{key}:\")\n",
    "                for v in value:\n",
    "                    file.write(f\"\\n - {v}\")\n",
    "                file.write(\"\\n\\n\")\n",
    "            else:\n",
    "                file.write(f\"{key}:\\n - {value}\\n\\n\")\n",
    "\n",
    "    # ! Get Target Data -------------------------------------------------------\n",
    "    # ! Load NFI Dataset\n",
    "    df_raw = pd.read_feather(here(\"data/final/nfi/nfi_ready_for_analysis.feather\"))\n",
    "\n",
    "    # ! Filter Target Data\n",
    "    df_subset = df_raw.copy()\n",
    "\n",
    "    # Check if species column present\n",
    "    for subset in user_input[\"subset\"]:\n",
    "        if subset not in df_subset.columns:\n",
    "            raise KeyError(f\"{subset} not in columns\")\n",
    "\n",
    "    # Filter out trees that do not belong to the desired species\n",
    "    for subset in user_input[\"subset\"]:\n",
    "        df_subset = df_subset[df_subset[subset].isin(user_input[\"subset_group\"])].copy()\n",
    "\n",
    "    # Keep only trees that survived or died\n",
    "    df_subset = df_subset.query(\n",
    "        \"tree_state_change == 'alive_alive' or tree_state_change == 'alive_dead'\"\n",
    "    ).copy()\n",
    "\n",
    "    # Encode target (1 = dead, 0 = alive)\n",
    "    df_subset[\"target\"] = (\n",
    "        df_subset[\"tree_state_change\"]\n",
    "        .copy()\n",
    "        .apply(lambda x: 1 if x == \"alive_dead\" else 0)\n",
    "    )\n",
    "\n",
    "    # Clean df\n",
    "    df_subset = move_vars_to_front(df_subset, [\"idp\", \"tree_id\", \"target\"])\n",
    "\n",
    "    # Keep target dataset separately\n",
    "    df_target = df_subset[[\"idp\", \"tree_id\", \"target\"]].copy()\n",
    "\n",
    "    # Break function if only alive trees\n",
    "    if df_target.target.value_counts().shape[0] == 1:\n",
    "        display(df_target.target.value_counts())\n",
    "        print(f\" - Skipping because too few dead trees\")\n",
    "        write_txt(f\"{current_dir}/⚠️ too few dead trees.txt\")\n",
    "        return None\n",
    "\n",
    "    # Break function if too little dead trees\n",
    "    # Algorithm usually broke when there were less than 35 dead trees\n",
    "    if df_target.target.value_counts()[1] < 35:\n",
    "        display(df_target.target.value_counts())\n",
    "        print(f\" - Skipping because too few dead trees\")\n",
    "        write_txt(f\"{current_dir}/⚠️ too few dead trees.txt\")\n",
    "        return None\n",
    "\n",
    "    # ! Attach Feature Data -------------------------------------------------------\n",
    "\n",
    "    # Select predictor data\n",
    "    user_input[\"predictor_datasets\"] = [\"\"]  # Not needed anymore\n",
    "\n",
    "    # Initiate dictionary and df\n",
    "    dict_preds = {}\n",
    "    df_preds = df_subset.copy()[[\"idp\", \"tree_id\"]]\n",
    "\n",
    "    #! Tree Properties\n",
    "    # Using df_subset from above to pick variables\n",
    "    voi = [\"htot_final\", \"c13_rel\", \"c13_1\"]\n",
    "    df_tree = df_subset[[\"idp\", \"tree_id\"] + voi]\n",
    "    df_preds = df_preds.merge(df_tree, on=[\"idp\", \"tree_id\"], how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Tree\", df_tree, dict_preds)\n",
    "\n",
    "    #! Stand Properties\n",
    "    # Using df_subset from above to pick variables\n",
    "    df_stand = df_subset[[\"idp\", \"tree_id\", \"social_status\"]]\n",
    "\n",
    "    # Using separately calculated metrics\n",
    "    df_stand = (\n",
    "        df_stand.merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_competition\"),\n",
    "            on=[\"idp\", \"tree_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_biodiversity\"),\n",
    "            on=[\"idp\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_gini\"),\n",
    "            on=[\"idp\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    df_preds = df_preds.merge(df_stand, on=[\"idp\", \"tree_id\"], how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Stand\", df_stand, dict_preds)\n",
    "\n",
    "    #! Carrying Capacity\n",
    "    df_cc = attach_or_load_predictor_dataset(\"forest_carrying_capacity\")\n",
    "    df_preds = df_preds.merge(df_cc, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Carrying Capacity\", df_cc, dict_preds)\n",
    "\n",
    "    #! Topography\n",
    "    df_topo = attach_or_load_predictor_dataset(\"topography\")\n",
    "    # Keep only variables at 1000m resolution (we will use this as the main resolution)\n",
    "    df_topo = df_topo[[\"idp\"] + [var for var in df_topo.columns if \"1000\" in var]]\n",
    "    # Remove dem1000_ and _mean from variable names\n",
    "    df_topo.columns = [\"idp\"] + [\n",
    "        var.replace(\"dem1000_\", \"\").replace(\"_mean\", \"\") for var in df_topo.columns[1:]\n",
    "    ]\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_topo, on=\"idp\", how=\"left\")\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Topography\", df_topo, dict_preds)\n",
    "\n",
    "    #! Soil Conditions\n",
    "    df_soil = attach_or_load_predictor_dataset(\"soil\")\n",
    "    # Clean variable names\n",
    "    df_soil.columns = [var.replace(\"soil_\", \"\") for var in df_soil.columns]\n",
    "    df_soil = df_soil.drop(columns=[\"first_year\"])\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_soil, on=\"idp\", how=\"left\")\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Soil\", df_soil, dict_preds)\n",
    "\n",
    "    #! Temperature\n",
    "    drop_cols = [\"idp\", \"first_year\", \"yrs_before_second_visit\"]\n",
    "    df_temp = pd.concat(\n",
    "        [\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmoy\"),\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmin\").drop(columns=drop_cols),\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmax\").drop(columns=drop_cols),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_temp, on=\"idp\", how=\"left\")\n",
    "\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Temperature\", df_temp, dict_preds)\n",
    "\n",
    "    #! SPEI\n",
    "    df_spei = attach_or_load_predictor_dataset(\"spei_anom\")\n",
    "\n",
    "    # Rename columns from numbers to months\n",
    "    df_spei.columns = [\n",
    "        var.replace(\"-1_\", \"-jan_\")\n",
    "        .replace(\"-2_\", \"-feb_\")\n",
    "        .replace(\"-3_\", \"-mar_\")\n",
    "        .replace(\"-4_\", \"-apr_\")\n",
    "        .replace(\"-5_\", \"-may_\")\n",
    "        .replace(\"-6_\", \"-jun_\")\n",
    "        .replace(\"-7_\", \"-jul_\")\n",
    "        .replace(\"-8_\", \"-aug_\")\n",
    "        .replace(\"-9_\", \"-sep_\")\n",
    "        .replace(\"-10_\", \"-oct_\")\n",
    "        .replace(\"-11_\", \"-nov_\")\n",
    "        .replace(\"-12_\", \"-dec_\")\n",
    "        .replace(\"-13_\", \"-ann_\")\n",
    "        for var in df_spei.columns\n",
    "    ]\n",
    "\n",
    "    # Keep features describing seasonal anomalies\n",
    "    spei_durations = [f\"spei{i}-\" for i in [1, 3, 6, 9, 12, 15, 18, 21, 24]]\n",
    "    spei_months = [f\"*-{i}_*\" for i in [\"feb\", \"may\", \"aug\", \"nov\"]]\n",
    "    spei_subset = match_variables(df_spei, spei_durations)\n",
    "    spei_subset = match_variables(df_spei[spei_subset], spei_months)\n",
    "\n",
    "    df_spei = df_spei[[\"idp\"] + spei_subset]\n",
    "\n",
    "    df_preds = df_preds.merge(df_spei, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"SPEI\", df_spei, dict_preds)\n",
    "\n",
    "    #! Management\n",
    "    df_human = attach_or_load_predictor_dataset(\"management\")\n",
    "    df_preds = df_preds.merge(df_human, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Management\", df_human, dict_preds)\n",
    "\n",
    "    #! NDVI\n",
    "    df_ndvi = attach_or_load_predictor_dataset(\"ndvi\")\n",
    "    df_preds = df_preds.merge(df_ndvi, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"NDVI\", df_ndvi, dict_preds)\n",
    "\n",
    "    # ! Align direction of variables\n",
    "    # Increasing distance to road should mean more management\n",
    "    df_preds.dist_road = df_preds.dist_road.replace({0: 4, 1: 3, 3: 1, 4: 0})\n",
    "\n",
    "    # ! Update dictionary --------------------------------------------------------------------------------\n",
    "    dict_preds_org = dict_preds.copy()\n",
    "    dict_preds_org\n",
    "    dict_preds = dict_preds_org.copy()\n",
    "    dict_preds.pop(\"Tree\", None)\n",
    "    dict_preds.pop(\"Stand\", None)\n",
    "    dict_preds.pop(\"Soil\", None)\n",
    "    dict_preds.pop(\"Carrying Capacity\", None)\n",
    "\n",
    "    dict_preds[\"Tree Size\"] = [\n",
    "        \"htot_final\",\n",
    "        \"c13_1\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Light Competition\"] = [\n",
    "        \"c13_rel\",\n",
    "        \"social_status\",\n",
    "        \"competition_larger\",\n",
    "        \"competition_larger_rel\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Species Competition\"] = [\n",
    "        \"competition_same_species\",\n",
    "        \"competition_same_species_rel\",\n",
    "        \"competition_other_species\",\n",
    "        \"competition_other_species_rel\",\n",
    "        \"belongs_to_dom_spec\",\n",
    "        \"num_species\",\n",
    "        \"simpson_species\",\n",
    "        \"shannon_species\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Stand Structure\"] = [\n",
    "        \"num_trees\",\n",
    "        \"gini_ba_1\",\n",
    "        \"mean_dbh\",\n",
    "        \"carrying_capacity\",\n",
    "        \"competition_total\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Soil Fertility\"] = [\n",
    "        \"CN\",\n",
    "        \"pH\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Soil Water Conditions\"] = [\n",
    "        \"waterlogging_temp\",\n",
    "        \"waterlogging_perm\",\n",
    "        \"swhc\",\n",
    "    ]\n",
    "\n",
    "    # Save dictionary to file\n",
    "    with open(f\"{current_dir}/feature_category_dictionary.json\", \"w\") as f:\n",
    "        json.dump(dict_preds, f)\n",
    "\n",
    "    # ! DATA PREPARATION --------------------------------------------------------------------------------\n",
    "\n",
    "    ## ! One-Hot-Encoding\n",
    "    # Note: Technically not needed because no categorical features but keeping it for future use\n",
    "    df_ohe = df_preds.copy()\n",
    "\n",
    "    # Get all variables names before one-hot encoding\n",
    "    all_var_names_before_ohe = sorted(df_ohe.columns.to_list())\n",
    "\n",
    "    # Set variables to not ohe:\n",
    "    my_vars_not_to_ohe = [\"test_train_strata\", \"target\", \"idp\", \"tree_id\"]\n",
    "\n",
    "    # Do the OHE\n",
    "    df_ohe = do_ohe(df_ohe, my_vars_not_to_ohe, verbose=False)\n",
    "\n",
    "    # Get all variables names after one-hot encoding\n",
    "    all_var_names_after_ohe = sorted(df_ohe.columns.to_list())\n",
    "\n",
    "    # Get variable dictionary\n",
    "    var_ohe_dict = {}\n",
    "    for var in all_var_names_before_ohe:\n",
    "        sub_vars = []\n",
    "\n",
    "        if var in all_var_names_after_ohe:\n",
    "            # If the variable was not ohe, it stays the same\n",
    "            var_ohe_dict[var] = [var]\n",
    "            continue\n",
    "        else:\n",
    "            # If the variable was ohe, search for pattern and add it\n",
    "            pattern = r\"^\" + var + r\"_.*\"\n",
    "            for sub_var in all_var_names_after_ohe:\n",
    "                # print(pattern, sub_var, re.match(pattern, sub_var))\n",
    "                if re.match(pattern, sub_var):\n",
    "                    sub_vars.append(sub_var)\n",
    "        var_ohe_dict[var] = sub_vars\n",
    "\n",
    "    ## ! Final Dataset\n",
    "    df_predictors_final = df_ohe.copy()\n",
    "\n",
    "    # Raise error if target and predictor df have not same number of rows\n",
    "    if df_target.shape[0] != df_predictors_final.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Target and predictor datasets have different number of rows: {df_target.shape[0]} vs {df_predictors_final.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    # Merge to get correct order\n",
    "    df_target_pred_final = pd.merge(\n",
    "        df_target, df_predictors_final, on=[\"idp\", \"tree_id\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_target_pred_final = df_target_pred_final.drop(\n",
    "        columns=[\"idp\", \"tree_id\", \"first_year\"], errors=\"ignore\"\n",
    "    )\n",
    "    # df_target_pred_final.to_csv(\"df_final_target_predictors.csv\", index=False)\n",
    "\n",
    "    ## ! Test/Train Split\n",
    "    # Get df\n",
    "    df_for_splitting = df_target_pred_final.copy()\n",
    "    print(f\" - Shape of df before splitting: \\t {df_for_splitting.shape}\")\n",
    "\n",
    "    X = df_for_splitting.drop(\"target\", axis=1)\n",
    "    y = df_for_splitting[\"target\"]\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=user_input[\"test_split\"],\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    Xy_train = pd.concat([y_train, X_train], axis=1).reset_index(drop=True)\n",
    "    Xy_test = pd.concat([y_test, X_test], axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Impute missing data using the training data mean values\n",
    "    for col in Xy_train.columns:\n",
    "        if Xy_train[col].dtype == \"float64\":\n",
    "            # Use training mean to avoid data leakage\n",
    "            # Generally less than 1% of data is missing\n",
    "            train_mean = Xy_train[col].mean()\n",
    "            Xy_train[col] = Xy_train[col].fillna(train_mean)\n",
    "            Xy_test[col] = Xy_test[col].fillna(train_mean)\n",
    "\n",
    "    # Check for any missing values\n",
    "    if Xy_train.isnull().sum().sum() > 0:\n",
    "        raise ValueError(\"Missing values in train dataset!\")\n",
    "\n",
    "    if Xy_test.isnull().sum().sum() > 0:\n",
    "        raise ValueError(\"Missing values in test dataset!\")\n",
    "\n",
    "    print(f\" - Shape of Xy_train:\\t\\t\\t {Xy_train.shape}\")\n",
    "    print(f\" - Shape of Xy_test:\\t\\t\\t {Xy_test.shape}\")\n",
    "\n",
    "    # Keep original dfs for saving tree ID further below\n",
    "    df_target_for_treeid = df_target.copy()\n",
    "    df_predictors_final_for_treeid = df_predictors_final.copy()\n",
    "\n",
    "    # ! RFE ------------------------------------------------------------------------------\n",
    "    # Ensure SMOTE is only applied to training data\n",
    "    user_input[\"do_smote_test_validation\"] = False\n",
    "    user_input[\"do_smote_test_final\"] = False\n",
    "\n",
    "    display(\" --- FEATURE ELIMINATION ---\")\n",
    "    rfecv_params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 8,\n",
    "        \"max_features\": 0.01,\n",
    "        \"bootstrap\": True,\n",
    "        \"criterion\": \"gini\",\n",
    "    }\n",
    "\n",
    "    df_cvmetrics_per_nfeatures = run_rfecv_treemort(\n",
    "        dict_categories=dict_preds.copy(),\n",
    "        var_ohe_dict=var_ohe_dict.copy(),\n",
    "        Xy_train_for_rfe=Xy_train.copy(),\n",
    "        user_input=user_input,\n",
    "        rfecv_params=rfecv_params,\n",
    "        debug_stop=False,\n",
    "        debug_stop_after_n_iterations=10,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    #! Report best variables ----------------------------------------------------------------\n",
    "    display(\" --- BEST FEATURES ---\")\n",
    "    # If rfe based on best oob, set best_model_metric to it too\n",
    "    if user_input[\"method_validation\"] == \"oob\":\n",
    "        user_input[\"best_model_metric\"] = \"oob\"\n",
    "\n",
    "    # Select best-performing model based on user input\n",
    "    # If best_metric, select the model with the highest score\n",
    "    # If best_per_category, select the model with the single best feature per feature category\n",
    "    if user_input[\"best_model_decision\"] == \"best_metric\":\n",
    "\n",
    "        ohed_variables_in_final_model = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[\"ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        non_ohed_variables_in_final_model = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[\"non_ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        best_score = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[user_input[\"best_model_metric\"]]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "    elif user_input[\"best_model_decision\"] == \"best_per_category\":\n",
    "        dict_len = len(dict_preds)\n",
    "\n",
    "        ohed_variables_in_final_model = df_cvmetrics_per_nfeatures.query(\n",
    "            \"n_features == @dict_len\"\n",
    "        )[\"ohe_vars_in_model\"].values[0]\n",
    "\n",
    "        non_ohed_variables_in_final_model = df_cvmetrics_per_nfeatures.query(\n",
    "            \"n_features == @dict_len\"\n",
    "        )[\"non_ohe_vars_in_model\"].values[0]\n",
    "\n",
    "        best_score = df_cvmetrics_per_nfeatures.query(\"n_features == @dict_len\")[\n",
    "            user_input[\"best_model_metric\"]\n",
    "        ].values[0]\n",
    "\n",
    "    elif user_input[\"best_model_decision\"] == \"best_metric_max1\":\n",
    "        dict_len = len(dict_preds)\n",
    "\n",
    "        max1cat = df_cvmetrics_per_nfeatures.query(\"n_features <= @dict_len\")\n",
    "\n",
    "        non_ohed_variables_in_final_model = (\n",
    "            max1cat.sort_values(by=user_input[\"best_model_metric\"], ascending=False)\n",
    "            .head(1)[\"non_ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        best_score = (\n",
    "            max1cat.sort_values(by=user_input[\"best_model_metric\"], ascending=False)\n",
    "            .head(1)[user_input[\"best_model_metric\"]]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid selection for final model decision!: {user_input['best_model_decision']}\"\n",
    "        )\n",
    "\n",
    "    txt_best_var = f\"\"\"\n",
    "    - Best score: {user_input['best_model_metric']} = {round(best_score,3)} based on model selecting by '{user_input['best_model_decision']}\n",
    "    \n",
    "    - Variables in best model (ohe):\\t{ohed_variables_in_final_model}\n",
    "    \n",
    "    - Variables in best model (non-ohe):\\t{sorted(non_ohed_variables_in_final_model)}\n",
    "        \"\"\"\n",
    "\n",
    "    # print(txt_best_var)\n",
    "    with open(f\"{current_dir}/final_model_variables.txt\", \"w\") as f:\n",
    "        f.write(txt_best_var)\n",
    "\n",
    "    # ! Select variables of best model\n",
    "    Xy_train_best_preds = Xy_train.copy()[[\"target\"] + ohed_variables_in_final_model]\n",
    "\n",
    "    # ! Correlation Removal ----------------------------------------------------------------\n",
    "    # First get feature importance of the best model\n",
    "    if user_input[\"method_validation\"] == \"cv\":\n",
    "        rf, sco, rf_vi = SMOTE_cv(\n",
    "            Xy_all=Xy_train_best_preds,\n",
    "            var_ohe_dict=var_ohe_dict,\n",
    "            rf_params=rfecv_params,\n",
    "            method_importance=user_input[\"method_importance\"],\n",
    "            smote_on_test=user_input[\"do_smote_test_validation\"],\n",
    "            rnd_seed=user_input[\"seed_nr\"],\n",
    "            verbose=False,\n",
    "            save_directory=None,\n",
    "        )\n",
    "    elif user_input[\"method_validation\"] == \"oob\":\n",
    "        rf, sco, rf_vi = SMOTE_oob(\n",
    "            Xy_all=Xy_train_best_preds,\n",
    "            var_ohe_dict=var_ohe_dict,\n",
    "            rf_params=rfecv_params,\n",
    "            method_importance=user_input[\"method_importance\"],\n",
    "            smote_on_test=user_input[\"do_smote_test_validation\"],\n",
    "            rnd_seed=user_input[\"seed_nr\"],\n",
    "            verbose=False,\n",
    "            save_directory=None,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Failed during RFE - Invalid method_validation! Got: {user_input['method_validation']}\"\n",
    "        )\n",
    "\n",
    "    # Get order of features (note that they are NOT ohe'd, so I have to first decode the dataframe, before selection. As done below.)\n",
    "    order_of_features = rf_vi.Feature.to_list()\n",
    "    final_vars = remove_correlation_based_on_vi(\n",
    "        Xy_train_best_preds,\n",
    "        var_ohe_dict,\n",
    "        rf_vi,\n",
    "        threshold=user_input[\"correlation_threshold\"],\n",
    "        make_heatmaps=False,\n",
    "        return_only_top_n=15,\n",
    "        save_directory=current_dir,\n",
    "    )\n",
    "\n",
    "    # ! SET FINAL FEATURES ----------------------------------------------------------------\n",
    "    Xy_train_final = Xy_train_best_preds.copy()[[\"target\"] + final_vars]\n",
    "    Xy_test_final = Xy_test.copy()[[\"target\"] + final_vars]\n",
    "\n",
    "    # ! TUNING -----------------------------------------------------------------------\n",
    "    # ! Prescribed Gridsearch\n",
    "    display(\" --- GRID SEARCH ---\")\n",
    "    # Get dataframe\n",
    "    Xy_train_for_tuning = Xy_train_final.copy()\n",
    "\n",
    "    # Split into response and predictors\n",
    "    Xy = Xy_train_for_tuning.copy()\n",
    "    X = Xy.drop(\n",
    "        columns=[\"target\", \"test_train_strata\", \"tree_id\", \"idp\"], errors=\"ignore\"\n",
    "    )\n",
    "    y = Xy[\"target\"]\n",
    "\n",
    "    # Build model\n",
    "    oversample = SMOTE(random_state=user_input[\"seed_nr\"])\n",
    "    model = RandomForestClassifier(random_state=user_input[\"seed_nr\"], n_jobs=-1)\n",
    "\n",
    "    # Apply oversampling to train set\n",
    "    X_train_over, y_train_over = oversample.fit_resample(X, y)\n",
    "\n",
    "    # Create Stratified K-fold cross validation\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=3, n_repeats=1, random_state=user_input[\"seed_nr\"]\n",
    "    )\n",
    "\n",
    "    # Get parameter grid\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [100, 300],  # Higher than 300 has minor influence\n",
    "        \"max_depth\": [1, 3, 12, 18],  # Higher than 18 has minor influence\n",
    "        \"max_features\": [0.01, 0.1, \"sqrt\"],  # Minor influence\n",
    "    }\n",
    "\n",
    "    # Set the grid search model\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        return_train_score=True,\n",
    "        scoring=user_input[\"gsc_metric\"],\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(\n",
    "        X,\n",
    "        y,\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    display(\"\")\n",
    "    print(\"--- FINAL RESULTS ---\")\n",
    "    print(\"Parameter grid:\")\n",
    "    for key, value in param_grid.items():\n",
    "        print(f\" - {key}: {value}\")\n",
    "\n",
    "    print(\"\\nBest parameters:\")\n",
    "    for key, value in grid_search.best_params_.items():\n",
    "        print(f\" - {key}: {value}\")\n",
    "    print(\n",
    "        f\"\\nBest {user_input['best_model_metric']}: {round(grid_search.best_score_, 2)}\"\n",
    "    )\n",
    "\n",
    "    # Get best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    # Visualize tuning\n",
    "    # plot_grid_search_results(\n",
    "    #     grid_search, \"prescribed\", save_directory=current_dir, show=False\n",
    "    # )\n",
    "\n",
    "    # ! Final Model ------------------------------------------------------------------------\n",
    "    display(\" --- FINAL MODEL RUN ---\")\n",
    "\n",
    "    # Setup model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        n_jobs=-1,\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    # Split response and predictors\n",
    "    X_train_final = Xy_train_final.drop(columns=[\"target\"], errors=\"ignore\")\n",
    "    y_train_final = Xy_train_final[\"target\"]\n",
    "\n",
    "    X_test_final = Xy_test_final.drop(columns=[\"target\"], errors=\"ignore\")\n",
    "    y_test_final = Xy_test_final[\"target\"]\n",
    "\n",
    "    # Apply SMOTE to train data\n",
    "    sm = SMOTE(random_state=user_input[\"seed_nr\"])\n",
    "    X_train_final, y_train_final = sm.fit_resample(X_train_final, y_train_final)\n",
    "\n",
    "    # Fit model\n",
    "    rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Feature importance\n",
    "    rf_vi = assessing_top_predictors(\n",
    "        vi_method=\"impurity\",\n",
    "        rf_in=rf_model,\n",
    "        X_train_in=X_train_final,\n",
    "        X_test_in=X_test_final,\n",
    "        y_test_in=y_test_final,\n",
    "        dict_ohe_in=var_ohe_dict,\n",
    "        with_aggregation=True,\n",
    "        n_predictors=20,\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        verbose=False,\n",
    "        save_directory=None,\n",
    "        # save_directory=user_input[\"current_dir\"],\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    model_evaluation_classification(\n",
    "        rf_model=rf_model,\n",
    "        X_train=X_train_final,\n",
    "        y_train=y_train_final,\n",
    "        X_test=X_test_final,\n",
    "        y_test=y_test_final,\n",
    "        prob_threshold=0.4,  # Irrelevant when calculating full AUC\n",
    "        save_directory=user_input[\"current_dir\"],\n",
    "        metric=\"f1-score\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # ! Save tree_id information separately, needed for merging SHAP and features during analysis\n",
    "    final_predictors = (\n",
    "        pd.read_csv(f\"{current_dir}/final_model/X_test.csv\")\n",
    "        .drop(columns=[\"Unnamed: 0\"])\n",
    "        .columns.to_list()\n",
    "    )\n",
    "\n",
    "    final_predictors = []\n",
    "\n",
    "    df_targted_treeid = pd.merge(\n",
    "        df_target_for_treeid,\n",
    "        df_predictors_final_for_treeid,\n",
    "        on=[\"idp\", \"tree_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    print(f\" - Shape of df_targted_treeid: {df_targted_treeid.shape}\")\n",
    "    print(f\" - Shape of df_targted_treeid target: {df_targted_treeid['target'].shape}\")\n",
    "\n",
    "    # Repeat same splitting as done before model fitting\n",
    "    X_train_treeid, X_test_treeid, y_train_treeid, y_test_treeid = train_test_split(\n",
    "        df_targted_treeid,\n",
    "        df_targted_treeid[\"target\"],\n",
    "        test_size=user_input[\"test_split\"],\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        stratify=df_targted_treeid[\"target\"],\n",
    "    )\n",
    "\n",
    "    dir_treeid = f\"{current_dir}/treeid\"\n",
    "    os.makedirs(dir_treeid, exist_ok=True)\n",
    "\n",
    "    X_train_treeid[[\"tree_id\"] + final_predictors].to_csv(\n",
    "        f\"{dir_treeid}/X_train_treeid.csv\", index=True\n",
    "    )\n",
    "    X_test_treeid[[\"tree_id\"] + final_predictors].to_csv(\n",
    "        f\"{dir_treeid}/X_test_treeid.csv\", index=True\n",
    "    )\n",
    "\n",
    "    y_train_treeid.to_csv(f\"{dir_treeid}/y_train_treeid.csv\", index=True)\n",
    "    y_test_treeid.to_csv(f\"{dir_treeid}/y_test_treeid.csv\", index=True)\n",
    "\n",
    "    # ! Save data --------------------------------------------------------\n",
    "    # General information\n",
    "    df_save = pd.DataFrame(\n",
    "        {\n",
    "            \"subset\": [user_input[\"subset\"][0]],\n",
    "            \"subset_group\": [user_input[\"subset_group\"][0]],\n",
    "            \"created\": [datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"best_model_decision\": [user_input[\"best_model_decision\"]],\n",
    "            \"N_died\": [df_target.target.sum()],\n",
    "            \"N_surv\": [df_target.shape[0] - df_target.target.sum()],\n",
    "            \"dir\": [user_input[\"current_dir\"]],\n",
    "            \"oversampled_cv\": [user_input[\"do_smote_test_validation\"]],\n",
    "            \"oversampled_test\": [user_input[\"do_smote_test_final\"]],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Final model metrics\n",
    "    df_save = pd.concat(\n",
    "        [df_save, pd.read_csv(f\"{current_dir}/classification_metrics.csv\")], axis=1\n",
    "    )\n",
    "\n",
    "    df_save.to_csv(f\"{current_dir}/final_model_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {}\n",
    "\n",
    "# ! General -----------------------------------------------------------------------\n",
    "user_input[\"dir_suffix\"] = None  # None or string\n",
    "user_input[\"description_file\"] = None\n",
    "user_input[\"subset\"] = [\"species_lat2\"]\n",
    "# ! TRAINING -----------------------------------------------------------------------\n",
    "# Data splitting\n",
    "user_input[\"test_split\"] = 0.2\n",
    "# Feature Elimination\n",
    "user_input[\"do_ref\"] = True\n",
    "user_input[\"method_validation\"] = \"oob\"  # none | cv | oob\n",
    "user_input[\"method_importance\"] = \"impurity\"  # permutation | impurity\n",
    "user_input[\"cv_folds\"] = 5  # Number of folds for if cv is selected for validation\n",
    "user_input[\"do_tuning\"] = False  # Tune during rfe validation?\n",
    "user_input[\"correlation_threshold\"] = 0.8  # Threshold for pearson-r correlation removal\n",
    "# Tuning\n",
    "user_input[\"do_prescribed_search\"] = True\n",
    "user_input[\"do_random_search\"] = False\n",
    "user_input[\"gsc_metric\"] = \"roc_auc\"  # grid search metric\n",
    "# ! Final model ---------------------------------------------------------------------------------\n",
    "# best_per_category | best_metric | best_metric_max1\n",
    "user_input[\"best_model_decision\"] = \"best_per_category\"\n",
    "user_input[\"best_model_metric\"] = \"roc_auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all seeds\n",
    "all_seeds = pd.read_csv(\"all_seeds.csv\").seed.tolist()\n",
    "all_species = all_species_norm.index.tolist()\n",
    "all_runs = pd.DataFrame(\n",
    "    list(itertools.product(all_seeds, all_species)),\n",
    "    columns=[\"seed\", \"species\"],\n",
    ")\n",
    "all_runs[\"dir\"] = \"\"\n",
    "all_runs[\"done\"] = False\n",
    "\n",
    "# Loop over all runs and check if that run has been completed\n",
    "for i, row in all_runs.iterrows():\n",
    "    # Get folder matching the seed\n",
    "    seed = row.seed\n",
    "    base_dir = glob.glob(f\"./model_runs/all_runs/run_{seed}\")\n",
    "\n",
    "    if len(base_dir) == 0:\n",
    "        # print(f\" - No folder found for seed: {seed}\")\n",
    "        continue\n",
    "    else:\n",
    "        base_dir = base_dir[0]\n",
    "        all_runs.loc[i, \"dir\"] = base_dir\n",
    "\n",
    "    if os.path.isfile(f\"./{base_dir}/{row.species}/final_model_performance.csv\"):\n",
    "        all_runs.loc[i, \"done\"] = True\n",
    "    elif os.path.isfile(f\"{base_dir}/{row.species}/⚠️ too few dead trees.txt\"):\n",
    "        all_runs.loc[i, \"done\"] = True\n",
    "    else:\n",
    "        all_runs.loc[i, \"done\"] = False\n",
    "\n",
    "# Get missing runs\n",
    "runs_to_run = (\n",
    "    all_runs.query(\"done == False\")\n",
    "    # .sort_values([\"species\", \"seed\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ! Option to only run top9 species\n",
    "# final_species = get_species_with_models(\"list\")\n",
    "# top9 = all_species_norm.index.tolist()\n",
    "# runs_to_run = runs_to_run.query(\"species in @top9\")\n",
    "\n",
    "# ! Option for running on multiple notebooks\n",
    "# Create multiple notebooks with name 01_model_fitting 1.ipynb, 01_model_fitting 2.ipynb, etc.\n",
    "# import IPython\n",
    "\n",
    "# # Number of notebooks\n",
    "# n_splits = 10\n",
    "# # Current notebook index\n",
    "# nb_name = IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"]\n",
    "# nb_id = int(nb_name.split(\"01_model_fitting \")[-1].split(\".\")[0])\n",
    "# print(f\"Running notebook {nb_id} with {n_splits} splits\")\n",
    "# # Select according nested list\n",
    "# l_runs_to_run = split_df_into_list_of_group_or_ns(runs_to_run, n_splits, \"seed\")\n",
    "# # l_runs_to_run = split_df_into_list_of_group_or_ns(runs_to_run, n_splits)\n",
    "# runs_to_run = l_runs_to_run[nb_id].reset_index(drop=True)\n",
    "\n",
    "# # Sort runs by increasing number of trees\n",
    "# sort_order = runs_to_run.species.value_counts().index.tolist()\n",
    "\n",
    "# # Display runs to run\n",
    "# runs_to_run\n",
    "\n",
    "# # Get sort order by increasing number of trees\n",
    "# species_order = all_species_norm.sort_values(ascending=True).index.tolist()\n",
    "\n",
    "# # Sort all_runs by species_order\n",
    "# runs_to_run[\"species\"] = runs_to_run[\"species\"].astype(\"category\")\n",
    "# runs_to_run[\"species\"] = runs_to_run[\"species\"].cat.set_categories(species_order)\n",
    "# runs_to_run = runs_to_run.sort_values([\"species\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ! Loop over all runs\n",
    "for i, row in runs_to_run.iterrows():\n",
    "\n",
    "    iseed = row.seed\n",
    "    ispecies = row.species\n",
    "    idir = row.dir\n",
    "\n",
    "    if idir == \"\":\n",
    "        # Create folder for run\n",
    "        idir = f\"model_runs/all_runs/run_{iseed}\"\n",
    "        os.makedirs(idir, exist_ok=True)\n",
    "        all_runs.loc[i, \"dir\"] = idir\n",
    "\n",
    "    # Start run\n",
    "    user_input[\"seed_nr\"] = iseed\n",
    "    display(\"\")\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        --------------------------------------------------------------------------------\n",
    "        Run {i}/{runs_to_run.shape[0]}\n",
    "        Seed: {iseed}\n",
    "        Species: {ispecies}\n",
    "        Dir: {idir}\n",
    "        Started: {datetime.datetime.now().strftime('%Y-%m-%d @ %H:%M:%S')}\n",
    "        --------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "    )\n",
    "    ist = start_time(False)\n",
    "    run_all(ispecies, user_input, base_dir=idir)\n",
    "    clear_output(wait=True)\n",
    "    end_time(ist, None, ring=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! osascript -e 'tell app \"System Events\" to shut down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Fitting Calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>species</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./model_runs/all_runs/run_569/Populus nigra/fi...</td>\n",
       "      <td>Populus nigra</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./model_runs/all_runs/run_569/Populus tremula/...</td>\n",
       "      <td>Populus tremula</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./model_runs/all_runs/run_569/Quercus petraea/...</td>\n",
       "      <td>Quercus petraea</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./model_runs/all_runs/run_569/Prunus avium/fin...</td>\n",
       "      <td>Prunus avium</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./model_runs/all_runs/run_569/Corylus avellana...</td>\n",
       "      <td>Corylus avellana</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>./model_runs/all_runs/run_1229/Acer campestre/...</td>\n",
       "      <td>Acer campestre</td>\n",
       "      <td>run_1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>./model_runs/all_runs/run_1229/Pinus pinaster/...</td>\n",
       "      <td>Pinus pinaster</td>\n",
       "      <td>run_1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>./model_runs/all_runs/run_1229/Tilia platyphyl...</td>\n",
       "      <td>Tilia platyphyllos</td>\n",
       "      <td>run_1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>./model_runs/all_runs/run_1229/Fagus sylvatica...</td>\n",
       "      <td>Fagus sylvatica</td>\n",
       "      <td>run_1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>./model_runs/all_runs/run_1229/Salix cinerea/f...</td>\n",
       "      <td>Salix cinerea</td>\n",
       "      <td>run_1229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file             species  \\\n",
       "0     ./model_runs/all_runs/run_569/Populus nigra/fi...       Populus nigra   \n",
       "1     ./model_runs/all_runs/run_569/Populus tremula/...     Populus tremula   \n",
       "2     ./model_runs/all_runs/run_569/Quercus petraea/...     Quercus petraea   \n",
       "3     ./model_runs/all_runs/run_569/Prunus avium/fin...        Prunus avium   \n",
       "4     ./model_runs/all_runs/run_569/Corylus avellana...    Corylus avellana   \n",
       "...                                                 ...                 ...   \n",
       "2595  ./model_runs/all_runs/run_1229/Acer campestre/...      Acer campestre   \n",
       "2596  ./model_runs/all_runs/run_1229/Pinus pinaster/...      Pinus pinaster   \n",
       "2597  ./model_runs/all_runs/run_1229/Tilia platyphyl...  Tilia platyphyllos   \n",
       "2598  ./model_runs/all_runs/run_1229/Fagus sylvatica...     Fagus sylvatica   \n",
       "2599  ./model_runs/all_runs/run_1229/Salix cinerea/f...       Salix cinerea   \n",
       "\n",
       "         model  \n",
       "0      run_569  \n",
       "1      run_569  \n",
       "2      run_569  \n",
       "3      run_569  \n",
       "4      run_569  \n",
       "...        ...  \n",
       "2595  run_1229  \n",
       "2596  run_1229  \n",
       "2597  run_1229  \n",
       "2598  run_1229  \n",
       "2599  run_1229  \n",
       "\n",
       "[2600 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_available = glob.glob(\"./model_runs/all_runs/run_*/*/final_model_performance.csv\")\n",
    "df_available = pd.DataFrame(df_available, columns=[\"file\"])\n",
    "df_available[\"species\"] = df_available[\"file\"].str.split(\"/\").str[-2]\n",
    "df_available[\"model\"] = df_available[\"file\"].str.split(\"/\").str[-3]\n",
    "df_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Splitting df into 10 groups, grouped by model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random_forest_utils import calculate_rf_performance\n",
    "\n",
    "run_mp(\n",
    "    calculate_rf_performance,\n",
    "    split_df_into_list_of_group_or_ns(df_available, 10, \"model\"),\n",
    "    base_dir=\"./model_runs/all_runs/\",\n",
    "    progress_bar=True,\n",
    "    num_cores=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SHAP Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:43<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "shap_run_new_loop_mp(\n",
    "    df_available,\n",
    "    run_interaction=False,\n",
    "    approximate=True,\n",
    "    test_or_train=\"test\",\n",
    "    force_run=False,\n",
    "    verbose=False,\n",
    "    num_cores=9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! osascript -e 'tell app \"System Events\" to shut down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP - Variable Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>species</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./model_runs/all_runs/run_569/Populus nigra/fi...</td>\n",
       "      <td>Populus nigra</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./model_runs/all_runs/run_569/Populus tremula/...</td>\n",
       "      <td>Populus tremula</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./model_runs/all_runs/run_569/Quercus petraea/...</td>\n",
       "      <td>Quercus petraea</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./model_runs/all_runs/run_569/Prunus avium/fin...</td>\n",
       "      <td>Prunus avium</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./model_runs/all_runs/run_569/Corylus avellana...</td>\n",
       "      <td>Corylus avellana</td>\n",
       "      <td>run_569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file           species  \\\n",
       "0  ./model_runs/all_runs/run_569/Populus nigra/fi...     Populus nigra   \n",
       "1  ./model_runs/all_runs/run_569/Populus tremula/...   Populus tremula   \n",
       "2  ./model_runs/all_runs/run_569/Quercus petraea/...   Quercus petraea   \n",
       "3  ./model_runs/all_runs/run_569/Prunus avium/fin...      Prunus avium   \n",
       "4  ./model_runs/all_runs/run_569/Corylus avellana...  Corylus avellana   \n",
       "\n",
       "     model  \n",
       "0  run_569  \n",
       "1  run_569  \n",
       "2  run_569  \n",
       "3  run_569  \n",
       "4  run_569  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Grab df_available from above\n",
    "df_in = df_available.copy()\n",
    "df_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model_runs/all_runs top9/run_569/Populus nigra/final_model/X_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df_in\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39mdf_in\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# !for i, row in tqdm(df_in.iterrows(), total=df_in.shape[0]):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Get predictor data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     ipreds \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model_runs/all_runs top9/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mspecies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_model/X_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m     ipreds \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mipreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Get SHAP data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     ishap \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model_runs/all_runs top9/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mspecies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/shap/approximated/shap_values_test.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/my_work/projects active/ifn_analysis/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_work/projects active/ifn_analysis/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/my_work/projects active/ifn_analysis/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_work/projects active/ifn_analysis/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/my_work/projects active/ifn_analysis/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model_runs/all_runs top9/run_569/Populus nigra/final_model/X_test.csv'"
     ]
    }
   ],
   "source": [
    "# Loop over runs and species and calculate mean absolute SHAP values\n",
    "for i, row in tqdm(df_in.head().iterrows(), total=df_in.head().shape[0]):\n",
    "    # !for i, row in tqdm(df_in.iterrows(), total=df_in.shape[0]):\n",
    "    # Get predictor data\n",
    "    ipreds = f\"./model_runs/all_runs/{row.model}/{row.species}/final_model/X_test.csv\"\n",
    "    ipreds = pd.read_csv(ipreds, index_col=[0])\n",
    "\n",
    "    # Get SHAP data\n",
    "    ishap = f\"./model_runs/all_runs/{row.model}/{row.species}/shap/approximated/shap_values_test.pkl\"\n",
    "    if not os.path.exists(ishap):\n",
    "        raise ValueError(\n",
    "            f\" 🚨 Skipping {row.model}/{row.species} because no SHAP values calculated yet!\"\n",
    "        )\n",
    "    ishap = load_shap(ishap)\n",
    "\n",
    "    # Extract SHAP values per prediction (saved in third dimension)\n",
    "    ishap = ishap.values[:, :, 1]\n",
    "\n",
    "    # Get the row of SHAP values to have a basis to add to\n",
    "    ishapAll = pd.DataFrame(ishap[0].tolist()).T\n",
    "\n",
    "    # Give the df the correct predictor names\n",
    "    ishapAll.columns = ipreds.columns\n",
    "\n",
    "    # Loop over all SHAP predictions and concatenate\n",
    "    for j in range(1, len(ishap)):\n",
    "        iii = pd.DataFrame(ishap[j].tolist()).T\n",
    "        iii.columns = ipreds.columns\n",
    "        ishapAll = pd.concat([ishapAll, iii], axis=0, ignore_index=True)\n",
    "\n",
    "    # Safety check: Shape of predictors should be the same as for SHAP values\n",
    "    if ipreds.shape != ishapAll.shape:\n",
    "        print(\n",
    "            f\" - Issue: The shape of the predictor data should equal the shape of the concatenated SHAP values!\"\n",
    "        )\n",
    "\n",
    "    # Take mean of SHAP values across all variables\n",
    "    ishapMean_org = ishapAll.abs().mean().sort_values(ascending=False)\n",
    "    ishapMean = ishapMean_org / ishapMean_org.sum()\n",
    "    ishapMean = pd.DataFrame(ishapMean)\n",
    "    ishapMean.columns = [\"Importance\"]\n",
    "    ishapMean.Importance = ishapMean.Importance * 100\n",
    "    ishapMean[\"Feature\"] = ishapMean.index\n",
    "    ishapMean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Link feature variable to predictor dataset in new column\n",
    "    # Load predictor dictionary\n",
    "    dict_preds = json.load(open(f\"./model_runs/feature_category_dictionary.json\"))\n",
    "    for f in ishapMean.Feature:\n",
    "        for key, value in dict_preds.items():\n",
    "            if f in value:\n",
    "                ishapMean.loc[ishapMean.Feature == f, \"dataset\"] = key\n",
    "\n",
    "    # Sum up the VI for each dataset\n",
    "    ishapMean_of_dataset = (\n",
    "        ishapMean[[\"Importance\", \"dataset\"]]\n",
    "        .groupby(\"dataset\")\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename({\"Importance\": \"dataset_imp\"}, axis=1)\n",
    "    )\n",
    "\n",
    "    ishapMean_of_dataset.dataset_imp = (\n",
    "        ishapMean_of_dataset.dataset_imp / ishapMean_of_dataset.dataset_imp.sum() * 100\n",
    "    )\n",
    "\n",
    "    ishapMean[\"mean_abs_shap_org\"] = ishapMean_org.values\n",
    "\n",
    "    # Attach dataset label with percentages\n",
    "    for j, jrow in ishapMean_of_dataset.iterrows():\n",
    "        ishapMean_of_dataset.loc[j, \"dataset_label\"] = (\n",
    "            str(round(ishapMean_of_dataset.loc[j, \"dataset_imp\"]))\n",
    "            + \"%: \"\n",
    "            + ishapMean_of_dataset.loc[j, \"dataset\"]\n",
    "        )\n",
    "\n",
    "    ishapMean = ishapMean.merge(ishapMean_of_dataset, on=\"dataset\", how=\"left\")\n",
    "\n",
    "    # Save SHAP data\n",
    "    ishapMean.to_csv(\n",
    "        f\"./model_runs/all_runs/{row.model}/{row.species}/shap_variable_importance.csv\"\n",
    "    )\n",
    "\n",
    "    # Load final model performance\n",
    "    ifinalOrg = f\"./model_runs/all_runs/{row.model}/{row.species}/final_model_performance_org.csv\"\n",
    "    ifinalNew = (\n",
    "        f\"./model_runs/all_runs/{row.model}/{row.species}/final_model_performance.csv\"\n",
    "    )\n",
    "\n",
    "    # If the original file has not yet been backuped, save it!\n",
    "    if not os.path.exists(ifinalOrg):\n",
    "        shutil.copy2(ifinalNew, ifinalOrg)\n",
    "\n",
    "    # Load model performance file, attach SHAP information and save it again\n",
    "    ifinalNewDf = pd.read_csv(ifinalNew)\n",
    "\n",
    "    for dataset in ishapMean.dataset.unique():\n",
    "        ifinalNewDf[f\"{dataset} - Importance\"] = ishapMean.loc[\n",
    "            ishapMean.dataset == dataset, \"dataset_imp\"\n",
    "        ].values[0]\n",
    "        ifinalNewDf[f\"{dataset} - Metrics\"] = [\n",
    "            ishapMean.loc[ishapMean.dataset == dataset, \"Feature\"].values\n",
    "        ]\n",
    "        ifinalNewDf[f\"{dataset} - Values\"] = [\n",
    "            ishapMean.loc[ishapMean.dataset == dataset, \"Importance\"].values\n",
    "        ]\n",
    "\n",
    "    ifinalNewDf.to_csv(ifinalNew, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
