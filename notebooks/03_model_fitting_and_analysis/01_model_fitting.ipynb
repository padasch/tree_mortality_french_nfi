{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Mortality Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()\n",
    "\n",
    "from IPython.display import clear_output  # For clearing the output of a cell\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "\n",
    "# List available data\n",
    "tmp = list_predictor_datasets(return_list=False)\n",
    "display(\"--------\")\n",
    "print(\"\\nList of available species and their percentages\")\n",
    "tmp = get_final_nfi_data_for_analysis(verbose=False).query(\n",
    "    \"tree_state_change in ['alive_alive', 'alive_dead']\"\n",
    ")\n",
    "# Get normalized and non normalized counts\n",
    "all_species = tmp[\"species_lat2\"].value_counts()\n",
    "all_species_norm = tmp[\"species_lat2\"].value_counts(normalize=True)\n",
    "top9_species = all_species.head(9).index.tolist()\n",
    "\n",
    "for i in all_species.index:\n",
    "    print(f\"{i:25} {all_species[i]:<30} {all_species_norm[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(species, user_input, base_dir=None):\n",
    "\n",
    "    # ! Set and save user input settings ---------------------------------\n",
    "    # Set species\n",
    "    user_input[\"subset_group\"] = [species]\n",
    "\n",
    "    # ! Get current directory (create if non-existent)\n",
    "    if base_dir is None:\n",
    "        user_input[\"current_dir\"] = create_new_run_folder_treemort(\n",
    "            user_input[\"subset_group\"][0]\n",
    "        )\n",
    "    else:\n",
    "        if base_dir[-1] != \"/\":\n",
    "            base_dir += \"/\"\n",
    "        user_input[\"current_dir\"] = base_dir + species + \"/\"\n",
    "        os.makedirs(user_input[\"current_dir\"], exist_ok=True)\n",
    "\n",
    "    current_dir = user_input[\"current_dir\"]\n",
    "\n",
    "    # ! Skip run if already done\n",
    "    ffile = f\"{current_dir}/final_model_performance.csv\"\n",
    "    if os.path.exists(ffile):\n",
    "        print(f\"Skipping {species} as it already exists\")\n",
    "        return None\n",
    "\n",
    "    # ! Write settings to file\n",
    "    file_path = f\"{current_dir}/__user_input.txt\"\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for key, value in user_input.items():\n",
    "            if isinstance(value, list):\n",
    "                file.write(f\"{key}:\")\n",
    "                for v in value:\n",
    "                    file.write(f\"\\n - {v}\")\n",
    "                file.write(\"\\n\\n\")\n",
    "            else:\n",
    "                file.write(f\"{key}:\\n - {value}\\n\\n\")\n",
    "\n",
    "    # ! Get Target Data -------------------------------------------------------\n",
    "    # ! Load NFI Dataset\n",
    "    df_raw = pd.read_feather(here(\"data/final/nfi/nfi_ready_for_analysis.feather\"))\n",
    "\n",
    "    # ! Filter Target Data\n",
    "    df_subset = df_raw.copy()\n",
    "\n",
    "    # Check if species column present\n",
    "    for subset in user_input[\"subset\"]:\n",
    "        if subset not in df_subset.columns:\n",
    "            raise KeyError(f\"{subset} not in columns\")\n",
    "\n",
    "    # Filter out trees that do not belong to the desired species\n",
    "    for subset in user_input[\"subset\"]:\n",
    "        df_subset = df_subset[df_subset[subset].isin(user_input[\"subset_group\"])].copy()\n",
    "\n",
    "    # Keep only trees that survived or died\n",
    "    df_subset = df_subset.query(\n",
    "        \"tree_state_change == 'alive_alive' or tree_state_change == 'alive_dead'\"\n",
    "    ).copy()\n",
    "\n",
    "    # Encode target (1 = dead, 0 = alive)\n",
    "    df_subset[\"target\"] = (\n",
    "        df_subset[\"tree_state_change\"]\n",
    "        .copy()\n",
    "        .apply(lambda x: 1 if x == \"alive_dead\" else 0)\n",
    "    )\n",
    "\n",
    "    # Clean df\n",
    "    df_subset = move_vars_to_front(df_subset, [\"idp\", \"tree_id\", \"target\"])\n",
    "\n",
    "    # Keep target dataset separately\n",
    "    df_target = df_subset[[\"idp\", \"tree_id\", \"target\"]].copy()\n",
    "\n",
    "    # Break function if only alive trees\n",
    "    # Counts has only shape 1 if only alive trees\n",
    "    if df_target.target.value_counts().shape[0] == 1:\n",
    "        display(df_target.target.value_counts())\n",
    "        print(f\" - Skipping because too few dead trees\")\n",
    "        write_txt(f\"{current_dir}/⚠️ too few dead trees.txt\")\n",
    "        return None\n",
    "\n",
    "    # Break function if too little dead trees\n",
    "    # Algorithm usually broke when there were less than 35 dead trees\n",
    "    if df_target.target.value_counts()[1] < 35:\n",
    "        # Value counts [1] is the count of dead trees\n",
    "        display(df_target.target.value_counts())\n",
    "        print(f\" - Skipping because too few dead trees\")\n",
    "        write_txt(f\"{current_dir}/⚠️ too few dead trees.txt\")\n",
    "        return None\n",
    "\n",
    "    # Break function if less trees than smote k requires\n",
    "    if df_target.target.value_counts()[1] <= user_input[\"smote_k\"] * 1.25:\n",
    "        display(df_target.target.value_counts())\n",
    "        print(f\" - Skipping because too few dead trees\")\n",
    "        write_txt(f\"{current_dir}/⚠️ too few dead trees.txt\")\n",
    "        write_txt(f\"{current_dir}/⚠️ N dead less than smote k requires.txt\")\n",
    "        return None\n",
    "\n",
    "    # ! Attach Feature Data -------------------------------------------------------\n",
    "\n",
    "    # Select predictor data\n",
    "    user_input[\"predictor_datasets\"] = [\"\"]  # Not needed anymore\n",
    "\n",
    "    # Initiate dictionary and df\n",
    "    dict_preds = {}\n",
    "    df_preds = df_subset.copy()[[\"idp\", \"tree_id\"]]\n",
    "\n",
    "    #! Tree Properties\n",
    "    # Using df_subset from above to pick variables\n",
    "    voi = [\"htot_final\", \"c13_rel\", \"c13_1\"]\n",
    "    df_tree = df_subset[[\"idp\", \"tree_id\"] + voi]\n",
    "    df_preds = df_preds.merge(df_tree, on=[\"idp\", \"tree_id\"], how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Tree\", df_tree, dict_preds)\n",
    "\n",
    "    #! Stand Properties\n",
    "    # Using df_subset from above to pick variables\n",
    "    df_stand = df_subset[[\"idp\", \"tree_id\", \"social_status\"]]\n",
    "\n",
    "    # Using separately calculated metrics\n",
    "    df_stand = (\n",
    "        df_stand.merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_competition\"),\n",
    "            on=[\"idp\", \"tree_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_biodiversity\"),\n",
    "            on=[\"idp\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_gini\"),\n",
    "            on=[\"idp\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    df_preds = df_preds.merge(df_stand, on=[\"idp\", \"tree_id\"], how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Stand\", df_stand, dict_preds)\n",
    "\n",
    "    #! Carrying Capacity\n",
    "    df_cc = attach_or_load_predictor_dataset(\"forest_carrying_capacity\")\n",
    "    df_preds = df_preds.merge(df_cc, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Carrying Capacity\", df_cc, dict_preds)\n",
    "\n",
    "    #! Topography\n",
    "    df_topo = attach_or_load_predictor_dataset(\"topography\")\n",
    "    # Keep only variables at 1000m resolution (we will use this as the main resolution)\n",
    "    df_topo = df_topo[[\"idp\"] + [var for var in df_topo.columns if \"1000\" in var]]\n",
    "    # Remove dem1000_ and _mean from variable names\n",
    "    df_topo.columns = [\"idp\"] + [\n",
    "        var.replace(\"dem1000_\", \"\").replace(\"_mean\", \"\") for var in df_topo.columns[1:]\n",
    "    ]\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_topo, on=\"idp\", how=\"left\")\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Topography\", df_topo, dict_preds)\n",
    "\n",
    "    #! Soil Conditions\n",
    "    df_soil = attach_or_load_predictor_dataset(\"soil\")\n",
    "    # Clean variable names\n",
    "    df_soil.columns = [var.replace(\"soil_\", \"\") for var in df_soil.columns]\n",
    "    df_soil = df_soil.drop(columns=[\"first_year\"])\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_soil, on=\"idp\", how=\"left\")\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Soil\", df_soil, dict_preds)\n",
    "\n",
    "    #! Temperature\n",
    "    drop_cols = [\"idp\", \"first_year\", \"yrs_before_second_visit\"]\n",
    "    df_temp = pd.concat(\n",
    "        [\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmoy\"),\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmin\").drop(columns=drop_cols),\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmax\").drop(columns=drop_cols),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_temp, on=\"idp\", how=\"left\")\n",
    "\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Temperature\", df_temp, dict_preds)\n",
    "\n",
    "    #! SPEI\n",
    "    df_spei = attach_or_load_predictor_dataset(\"spei_anom\")\n",
    "\n",
    "    # Rename columns from numbers to months\n",
    "    df_spei.columns = [\n",
    "        var.replace(\"-1_\", \"-jan_\")\n",
    "        .replace(\"-2_\", \"-feb_\")\n",
    "        .replace(\"-3_\", \"-mar_\")\n",
    "        .replace(\"-4_\", \"-apr_\")\n",
    "        .replace(\"-5_\", \"-may_\")\n",
    "        .replace(\"-6_\", \"-jun_\")\n",
    "        .replace(\"-7_\", \"-jul_\")\n",
    "        .replace(\"-8_\", \"-aug_\")\n",
    "        .replace(\"-9_\", \"-sep_\")\n",
    "        .replace(\"-10_\", \"-oct_\")\n",
    "        .replace(\"-11_\", \"-nov_\")\n",
    "        .replace(\"-12_\", \"-dec_\")\n",
    "        .replace(\"-13_\", \"-ann_\")\n",
    "        for var in df_spei.columns\n",
    "    ]\n",
    "\n",
    "    # Keep features describing seasonal anomalies\n",
    "    spei_durations = [f\"spei{i}-\" for i in [1, 3, 6, 9, 12, 15, 18, 21, 24]]\n",
    "    spei_months = [f\"*-{i}_*\" for i in [\"feb\", \"may\", \"aug\", \"nov\"]]\n",
    "    spei_subset = match_variables(df_spei, spei_durations)\n",
    "    spei_subset = match_variables(df_spei[spei_subset], spei_months)\n",
    "\n",
    "    df_spei = df_spei[[\"idp\"] + spei_subset]\n",
    "\n",
    "    df_preds = df_preds.merge(df_spei, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"SPEI\", df_spei, dict_preds)\n",
    "\n",
    "    #! Management\n",
    "    df_human = attach_or_load_predictor_dataset(\"management\")\n",
    "    df_preds = df_preds.merge(df_human, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Management\", df_human, dict_preds)\n",
    "\n",
    "    #! NDVI\n",
    "    df_ndvi = attach_or_load_predictor_dataset(\"ndvi\")\n",
    "    df_preds = df_preds.merge(df_ndvi, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"NDVI\", df_ndvi, dict_preds)\n",
    "\n",
    "    # ! Align direction of variables\n",
    "    # Increasing distance to road should mean more management\n",
    "    df_preds.dist_road = df_preds.dist_road.replace({0: 4, 1: 3, 3: 1, 4: 0})\n",
    "\n",
    "    # ! Update dictionary --------------------------------------------------------------------------------\n",
    "    dict_preds_org = dict_preds.copy()\n",
    "    dict_preds_org\n",
    "    dict_preds = dict_preds_org.copy()\n",
    "    dict_preds.pop(\"Tree\", None)\n",
    "    dict_preds.pop(\"Stand\", None)\n",
    "    dict_preds.pop(\"Soil\", None)\n",
    "    dict_preds.pop(\"Carrying Capacity\", None)\n",
    "\n",
    "    dict_preds[\"Tree Size\"] = [\n",
    "        \"htot_final\",\n",
    "        \"c13_1\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Light Competition\"] = [\n",
    "        \"c13_rel\",\n",
    "        \"social_status\",\n",
    "        \"competition_larger\",\n",
    "        \"competition_larger_rel\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Species Competition\"] = [\n",
    "        \"competition_same_species\",\n",
    "        \"competition_same_species_rel\",\n",
    "        \"competition_other_species\",\n",
    "        \"competition_other_species_rel\",\n",
    "        \"belongs_to_dom_spec\",\n",
    "        \"num_species\",\n",
    "        \"simpson_species\",\n",
    "        \"shannon_species\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Stand Structure\"] = [\n",
    "        \"num_trees\",\n",
    "        \"gini_ba_1\",\n",
    "        \"mean_dbh\",\n",
    "        \"carrying_capacity\",\n",
    "        \"competition_total\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Soil Fertility\"] = [\n",
    "        \"CN\",\n",
    "        \"pH\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Soil Water Conditions\"] = [\n",
    "        \"waterlogging_temp\",\n",
    "        \"waterlogging_perm\",\n",
    "        \"swhc\",\n",
    "    ]\n",
    "\n",
    "    # Save dictionary to file\n",
    "    with open(f\"{current_dir}/feature_category_dictionary.json\", \"w\") as f:\n",
    "        json.dump(dict_preds, f)\n",
    "\n",
    "    # ! DATA PREPARATION --------------------------------------------------------------------------------\n",
    "\n",
    "    ## ! One-Hot-Encoding\n",
    "    # Note: Technically not needed because no categorical features but keeping it for future use\n",
    "    df_ohe = df_preds.copy()\n",
    "\n",
    "    # Get all variables names before one-hot encoding\n",
    "    all_var_names_before_ohe = sorted(df_ohe.columns.to_list())\n",
    "\n",
    "    # Set variables to not ohe:\n",
    "    my_vars_not_to_ohe = [\"test_train_strata\", \"target\", \"idp\", \"tree_id\"]\n",
    "\n",
    "    # Do the OHE\n",
    "    df_ohe = do_ohe(df_ohe, my_vars_not_to_ohe, verbose=False)\n",
    "\n",
    "    # Get all variables names after one-hot encoding\n",
    "    all_var_names_after_ohe = sorted(df_ohe.columns.to_list())\n",
    "\n",
    "    # Get variable dictionary\n",
    "    var_ohe_dict = {}\n",
    "    for var in all_var_names_before_ohe:\n",
    "        sub_vars = []\n",
    "\n",
    "        if var in all_var_names_after_ohe:\n",
    "            # If the variable was not ohe, it stays the same\n",
    "            var_ohe_dict[var] = [var]\n",
    "            continue\n",
    "        else:\n",
    "            # If the variable was ohe, search for pattern and add it\n",
    "            pattern = r\"^\" + var + r\"_.*\"\n",
    "            for sub_var in all_var_names_after_ohe:\n",
    "                # print(pattern, sub_var, re.match(pattern, sub_var))\n",
    "                if re.match(pattern, sub_var):\n",
    "                    sub_vars.append(sub_var)\n",
    "        var_ohe_dict[var] = sub_vars\n",
    "\n",
    "    ## ! Final Dataset\n",
    "    df_predictors_final = df_ohe.copy()\n",
    "\n",
    "    # Raise error if target and predictor df have not same number of rows\n",
    "    if df_target.shape[0] != df_predictors_final.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Target and predictor datasets have different number of rows: {df_target.shape[0]} vs {df_predictors_final.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    # Merge to get correct order\n",
    "    df_target_pred_final = pd.merge(\n",
    "        df_target, df_predictors_final, on=[\"idp\", \"tree_id\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_target_pred_final = df_target_pred_final.drop(\n",
    "        columns=[\"idp\", \"tree_id\", \"first_year\"], errors=\"ignore\"\n",
    "    )\n",
    "    # df_target_pred_final.to_csv(\"df_final_target_predictors.csv\", index=False)\n",
    "\n",
    "    ## ! Test/Train Split\n",
    "    # Get df\n",
    "    df_for_splitting = df_target_pred_final.copy()\n",
    "    print(f\" - Shape of df before splitting: \\t {df_for_splitting.shape}\")\n",
    "\n",
    "    X = df_for_splitting.drop(\"target\", axis=1)\n",
    "    y = df_for_splitting[\"target\"]\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=user_input[\"test_split\"],\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    Xy_train = pd.concat([y_train, X_train], axis=1).reset_index(drop=True)\n",
    "    Xy_test = pd.concat([y_test, X_test], axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Impute missing data using the training data mean values\n",
    "    for col in Xy_train.columns:\n",
    "        if Xy_train[col].dtype == \"float64\":\n",
    "            # Use training mean to avoid data leakage\n",
    "            # Generally less than 1% of data is missing\n",
    "            train_mean = Xy_train[col].mean()\n",
    "            Xy_train[col] = Xy_train[col].fillna(train_mean)\n",
    "            Xy_test[col] = Xy_test[col].fillna(train_mean)\n",
    "\n",
    "    # Check for any missing values\n",
    "    if Xy_train.isnull().sum().sum() > 0:\n",
    "        raise ValueError(\"Missing values in train dataset!\")\n",
    "\n",
    "    if Xy_test.isnull().sum().sum() > 0:\n",
    "        raise ValueError(\"Missing values in test dataset!\")\n",
    "\n",
    "    print(f\" - Shape of Xy_train:\\t\\t\\t {Xy_train.shape}\")\n",
    "    print(f\" - Shape of Xy_test:\\t\\t\\t {Xy_test.shape}\")\n",
    "\n",
    "    # Keep original dfs for saving tree ID further below\n",
    "    df_target_for_treeid = df_target.copy()\n",
    "    df_predictors_final_for_treeid = df_predictors_final.copy()\n",
    "\n",
    "    # ! RFE ------------------------------------------------------------------------------\n",
    "    # Ensure SMOTE is only applied to training data\n",
    "    user_input[\"do_smote_test_validation\"] = False\n",
    "    user_input[\"do_smote_test_final\"] = False\n",
    "\n",
    "    display(\" --- FEATURE ELIMINATION ---\")\n",
    "    rfecv_params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 8,\n",
    "        \"max_features\": 0.01,\n",
    "        \"bootstrap\": True,\n",
    "        \"criterion\": \"gini\",\n",
    "    }\n",
    "\n",
    "    df_cvmetrics_per_nfeatures = run_rfecv_treemort(\n",
    "        dict_categories=dict_preds.copy(),\n",
    "        var_ohe_dict=var_ohe_dict.copy(),\n",
    "        Xy_train_for_rfe=Xy_train.copy(),\n",
    "        user_input=user_input,\n",
    "        rfecv_params=rfecv_params,\n",
    "        debug_stop=False,\n",
    "        debug_stop_after_n_iterations=10,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    #! Report best variables ----------------------------------------------------------------\n",
    "    display(\" --- BEST FEATURES ---\")\n",
    "    # If rfe based on best oob, set best_model_metric to it too\n",
    "    if user_input[\"method_validation\"] == \"oob\":\n",
    "        user_input[\"best_model_metric\"] = \"oob\"\n",
    "\n",
    "    # Select best-performing model based on user input\n",
    "    # If best_metric, select the model with the highest score\n",
    "    # If best_per_category, select the model with the single best feature per feature category\n",
    "    if user_input[\"best_model_decision\"] == \"best_metric\":\n",
    "\n",
    "        ohed_variables_in_final_model = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[\"ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        non_ohed_variables_in_final_model = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[\"non_ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        best_score = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[user_input[\"best_model_metric\"]]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "    elif user_input[\"best_model_decision\"] == \"best_per_category\":\n",
    "        dict_len = len(dict_preds)\n",
    "\n",
    "        ohed_variables_in_final_model = df_cvmetrics_per_nfeatures.query(\n",
    "            \"n_features == @dict_len\"\n",
    "        )[\"ohe_vars_in_model\"].values[0]\n",
    "\n",
    "        non_ohed_variables_in_final_model = df_cvmetrics_per_nfeatures.query(\n",
    "            \"n_features == @dict_len\"\n",
    "        )[\"non_ohe_vars_in_model\"].values[0]\n",
    "\n",
    "        best_score = df_cvmetrics_per_nfeatures.query(\"n_features == @dict_len\")[\n",
    "            user_input[\"best_model_metric\"]\n",
    "        ].values[0]\n",
    "\n",
    "    elif user_input[\"best_model_decision\"] == \"best_metric_max1\":\n",
    "        dict_len = len(dict_preds)\n",
    "\n",
    "        max1cat = df_cvmetrics_per_nfeatures.query(\"n_features <= @dict_len\")\n",
    "\n",
    "        non_ohed_variables_in_final_model = (\n",
    "            max1cat.sort_values(by=user_input[\"best_model_metric\"], ascending=False)\n",
    "            .head(1)[\"non_ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        best_score = (\n",
    "            max1cat.sort_values(by=user_input[\"best_model_metric\"], ascending=False)\n",
    "            .head(1)[user_input[\"best_model_metric\"]]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid selection for final model decision!: {user_input['best_model_decision']}\"\n",
    "        )\n",
    "\n",
    "    txt_best_var = f\"\"\"\n",
    "    - Best score: {user_input['best_model_metric']} = {round(best_score,3)} based on model selecting by '{user_input['best_model_decision']}\n",
    "    \n",
    "    - Variables in best model (ohe):\\t{ohed_variables_in_final_model}\n",
    "    \n",
    "    - Variables in best model (non-ohe):\\t{sorted(non_ohed_variables_in_final_model)}\n",
    "        \"\"\"\n",
    "\n",
    "    # print(txt_best_var)\n",
    "    with open(f\"{current_dir}/final_model_variables.txt\", \"w\") as f:\n",
    "        f.write(txt_best_var)\n",
    "\n",
    "    # ! Select variables of best model\n",
    "    Xy_train_best_preds = Xy_train.copy()[[\"target\"] + ohed_variables_in_final_model]\n",
    "\n",
    "    # ! Correlation Removal ----------------------------------------------------------------\n",
    "    # First get feature importance of the best model\n",
    "    if user_input[\"method_validation\"] == \"cv\":\n",
    "        rf, sco, rf_vi = SMOTE_cv(\n",
    "            Xy_all=Xy_train_best_preds,\n",
    "            var_ohe_dict=var_ohe_dict,\n",
    "            rf_params=rfecv_params,\n",
    "            method_importance=user_input[\"method_importance\"],\n",
    "            smote_on_test=user_input[\"do_smote_test_validation\"],\n",
    "            rnd_seed=user_input[\"seed_nr\"],\n",
    "            verbose=False,\n",
    "            save_directory=None,\n",
    "        )\n",
    "    elif user_input[\"method_validation\"] == \"oob\":\n",
    "        rf, sco, rf_vi = SMOTE_oob(\n",
    "            Xy_all=Xy_train_best_preds,\n",
    "            var_ohe_dict=var_ohe_dict,\n",
    "            rf_params=rfecv_params,\n",
    "            method_importance=user_input[\"method_importance\"],\n",
    "            smote_on_test=user_input[\"do_smote_test_validation\"],\n",
    "            rnd_seed=user_input[\"seed_nr\"],\n",
    "            verbose=False,\n",
    "            save_directory=None,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Failed during RFE - Invalid method_validation! Got: {user_input['method_validation']}\"\n",
    "        )\n",
    "\n",
    "    # Get order of features (note that they are NOT ohe'd, so I have to first decode the dataframe, before selection. As done below.)\n",
    "    order_of_features = rf_vi.Feature.to_list()\n",
    "    final_vars = remove_correlation_based_on_vi(\n",
    "        Xy_train_best_preds,\n",
    "        var_ohe_dict,\n",
    "        rf_vi,\n",
    "        threshold=user_input[\"correlation_threshold\"],\n",
    "        make_heatmaps=False,\n",
    "        return_only_top_n=15,\n",
    "        save_directory=current_dir,\n",
    "    )\n",
    "\n",
    "    # ! SET FINAL FEATURES ----------------------------------------------------------------\n",
    "    Xy_train_final = Xy_train_best_preds.copy()[[\"target\"] + final_vars]\n",
    "    Xy_test_final = Xy_test.copy()[[\"target\"] + final_vars]\n",
    "\n",
    "    # ! TUNING -----------------------------------------------------------------------\n",
    "    # ! Prescribed Gridsearch\n",
    "    display(\" --- GRID SEARCH ---\")\n",
    "    # Get dataframe\n",
    "    Xy_train_for_tuning = Xy_train_final.copy()\n",
    "\n",
    "    # Split into response and predictors\n",
    "    Xy = Xy_train_for_tuning.copy()\n",
    "    X = Xy.drop(\n",
    "        columns=[\"target\", \"test_train_strata\", \"tree_id\", \"idp\"], errors=\"ignore\"\n",
    "    )\n",
    "    y = Xy[\"target\"]\n",
    "\n",
    "    # Build model\n",
    "    model = RandomForestClassifier(random_state=user_input[\"seed_nr\"], n_jobs=-1)\n",
    "\n",
    "    # Apply oversampling to train set\n",
    "    X_train_over, y_train_over = apply_smote(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        seed=user_input[\"seed_nr\"],\n",
    "        k=user_input[\"smote_k\"],\n",
    "    )\n",
    "\n",
    "    # Create Stratified K-fold cross validation\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=3, n_repeats=1, random_state=user_input[\"seed_nr\"]\n",
    "    )\n",
    "\n",
    "    # Get parameter grid\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [100, 300],  # Higher than 300 has minor influence\n",
    "        \"max_depth\": [1, 3, 12, 18],  # Higher than 18 has minor influence\n",
    "        \"max_features\": [0.01, 0.1, \"sqrt\"],  # Minor influence\n",
    "    }\n",
    "\n",
    "    # Set the grid search model\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        return_train_score=True,\n",
    "        scoring=user_input[\"gsc_metric\"],\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(\n",
    "        X,\n",
    "        y,\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    display(\"\")\n",
    "    print(\"--- FINAL RESULTS ---\")\n",
    "    print(\"Parameter grid:\")\n",
    "    for key, value in param_grid.items():\n",
    "        print(f\" - {key}: {value}\")\n",
    "\n",
    "    print(\"\\nBest parameters:\")\n",
    "    for key, value in grid_search.best_params_.items():\n",
    "        print(f\" - {key}: {value}\")\n",
    "    print(\n",
    "        f\"\\nBest {user_input['best_model_metric']}: {round(grid_search.best_score_, 2)}\"\n",
    "    )\n",
    "\n",
    "    # Get best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    # Visualize tuning\n",
    "    # plot_grid_search_results(\n",
    "    #     grid_search, \"prescribed\", save_directory=current_dir, show=False\n",
    "    # )\n",
    "\n",
    "    # ! Final Model ------------------------------------------------------------------------\n",
    "    display(\" --- FINAL MODEL RUN ---\")\n",
    "\n",
    "    # Setup model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        n_jobs=-1,\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    # Split response and predictors\n",
    "    X_train_final = Xy_train_final.drop(columns=[\"target\"], errors=\"ignore\")\n",
    "    y_train_final = Xy_train_final[\"target\"]\n",
    "\n",
    "    X_test_final = Xy_test_final.drop(columns=[\"target\"], errors=\"ignore\")\n",
    "    y_test_final = Xy_test_final[\"target\"]\n",
    "\n",
    "    # Apply SMOTE to train data\n",
    "    X_train_final, y_train_final = apply_smote(\n",
    "        X=X_train_final,\n",
    "        y=y_train_final,\n",
    "        seed=user_input[\"seed_nr\"],\n",
    "        k=user_input[\"smote_k\"],\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Feature importance\n",
    "    rf_vi = assessing_top_predictors(\n",
    "        vi_method=\"impurity\",\n",
    "        rf_in=rf_model,\n",
    "        X_train_in=X_train_final,\n",
    "        X_test_in=X_test_final,\n",
    "        y_test_in=y_test_final,\n",
    "        dict_ohe_in=var_ohe_dict,\n",
    "        with_aggregation=True,\n",
    "        n_predictors=20,\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        verbose=False,\n",
    "        save_directory=None,\n",
    "        # save_directory=user_input[\"current_dir\"],\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    model_evaluation_classification(\n",
    "        rf_model=rf_model,\n",
    "        X_train=X_train_final,\n",
    "        y_train=y_train_final,\n",
    "        X_test=X_test_final,\n",
    "        y_test=y_test_final,\n",
    "        prob_threshold=0.4,  # Irrelevant when calculating full AUC\n",
    "        save_directory=user_input[\"current_dir\"],\n",
    "        metric=\"f1-score\",\n",
    "        verbose=False,\n",
    "        save_only_predictions=False,\n",
    "    )\n",
    "\n",
    "    # ! Save tree_id information separately, needed for merging SHAP and features during analysis\n",
    "    final_predictors = (\n",
    "        pd.read_csv(f\"{current_dir}/final_model/X_test.csv\")\n",
    "        .drop(columns=[\"Unnamed: 0\"])\n",
    "        .columns.to_list()\n",
    "    )\n",
    "\n",
    "    final_predictors = []\n",
    "\n",
    "    df_targted_treeid = pd.merge(\n",
    "        df_target_for_treeid,\n",
    "        df_predictors_final_for_treeid,\n",
    "        on=[\"idp\", \"tree_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    print(f\" - Shape of df_targted_treeid: {df_targted_treeid.shape}\")\n",
    "    print(f\" - Shape of df_targted_treeid target: {df_targted_treeid['target'].shape}\")\n",
    "\n",
    "    # Repeat same splitting as done before model fitting\n",
    "    X_train_treeid, X_test_treeid, y_train_treeid, y_test_treeid = train_test_split(\n",
    "        df_targted_treeid,\n",
    "        df_targted_treeid[\"target\"],\n",
    "        test_size=user_input[\"test_split\"],\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        stratify=df_targted_treeid[\"target\"],\n",
    "    )\n",
    "\n",
    "    dir_treeid = f\"{current_dir}/treeid\"\n",
    "    os.makedirs(dir_treeid, exist_ok=True)\n",
    "\n",
    "    X_train_treeid[[\"tree_id\"] + final_predictors].to_csv(\n",
    "        f\"{dir_treeid}/X_train_treeid.csv\", index=True\n",
    "    )\n",
    "    X_test_treeid[[\"tree_id\"] + final_predictors].to_csv(\n",
    "        f\"{dir_treeid}/X_test_treeid.csv\", index=True\n",
    "    )\n",
    "\n",
    "    y_train_treeid.to_csv(f\"{dir_treeid}/y_train_treeid.csv\", index=True)\n",
    "    y_test_treeid.to_csv(f\"{dir_treeid}/y_test_treeid.csv\", index=True)\n",
    "\n",
    "    # ! Save data --------------------------------------------------------\n",
    "    # General information\n",
    "    df_save = pd.DataFrame(\n",
    "        {\n",
    "            \"subset\": [user_input[\"subset\"][0]],\n",
    "            \"subset_group\": [user_input[\"subset_group\"][0]],\n",
    "            \"created\": [datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"best_model_decision\": [user_input[\"best_model_decision\"]],\n",
    "            \"N_died\": [df_target.target.sum()],\n",
    "            \"N_surv\": [df_target.shape[0] - df_target.target.sum()],\n",
    "            \"dir\": [user_input[\"current_dir\"]],\n",
    "            \"oversampled_cv\": [user_input[\"do_smote_test_validation\"]],\n",
    "            \"oversampled_test\": [user_input[\"do_smote_test_final\"]],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Final model metrics\n",
    "    df_save = pd.concat(\n",
    "        [df_save, pd.read_csv(f\"{current_dir}/classification_metrics.csv\")], axis=1\n",
    "    )\n",
    "\n",
    "    df_save.to_csv(f\"{current_dir}/final_model_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = {}\n",
    "\n",
    "# # ! General -----------------------------------------------------------------------\n",
    "# runs_dir = \"./model_runs/all_runs\"\n",
    "# user_input[\"dir_suffix\"] = None  # None or string\n",
    "# user_input[\"description_file\"] = None\n",
    "# user_input[\"subset\"] = [\"species_lat2\"]\n",
    "# # ! TRAINING -----------------------------------------------------------------------\n",
    "# # Data splitting\n",
    "# user_input[\"test_split\"] = 0.2\n",
    "# # Feature Elimination\n",
    "# user_input[\"do_ref\"] = True\n",
    "# user_input[\"method_validation\"] = \"oob\"  # none | cv | oob\n",
    "# user_input[\"method_importance\"] = \"impurity\"  # permutation | impurity\n",
    "# user_input[\"cv_folds\"] = 5  # Number of folds for if cv is selected for validation\n",
    "# user_input[\"do_tuning\"] = False  # Tune during rfe validation?\n",
    "# user_input[\"correlation_threshold\"] = 0.8  # Threshold for r correlation removal\n",
    "# # Tuning\n",
    "# user_input[\"do_prescribed_search\"] = True\n",
    "# user_input[\"do_random_search\"] = False\n",
    "# user_input[\"gsc_metric\"] = \"roc_auc\"  # grid search metric\n",
    "# # ! Final model ---------------------------------------------------------------------\n",
    "# # best_per_category | best_metric | best_metric_max1\n",
    "# user_input[\"best_model_decision\"] = \"best_per_category\"\n",
    "# user_input[\"best_model_metric\"] = \"roc_auc\"\n",
    "\n",
    "# # ! Extra analyses ------------------------------------------------\n",
    "# # Number of nearest neighbors for SMOTE (default setting is 5)\n",
    "# user_input[\"smote_k\"] = 5\n",
    "# # Minimum number of features per category to keep (default is 2)\n",
    "# user_input[\"min_features_per_category\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all seeds\n",
    "# all_seeds = pd.read_csv(\"all_seeds.csv\").seed.tolist()\n",
    "# all_species = all_species_norm.index.tolist()\n",
    "# all_runs = pd.DataFrame(\n",
    "#     list(itertools.product(all_seeds, all_species)),\n",
    "#     columns=[\"seed\", \"species\"],\n",
    "# )\n",
    "# all_runs[\"dir\"] = \"\"\n",
    "# all_runs[\"done\"] = False\n",
    "\n",
    "# # Loop over all runs and check if that run has been completed\n",
    "# for i, row in all_runs.iterrows():\n",
    "#     # Get folder matching the seed\n",
    "#     seed = row.seed\n",
    "#     base_dir = glob.glob(f\"{runs_dir}/run_{seed}\")\n",
    "\n",
    "#     if len(base_dir) == 0:\n",
    "#         # print(f\" - No folder found for seed: {seed}\")\n",
    "#         continue\n",
    "#     else:\n",
    "#         base_dir = base_dir[0]\n",
    "#         all_runs.loc[i, \"dir\"] = base_dir\n",
    "\n",
    "#     if os.path.isfile(f\"./{base_dir}/{row.species}/final_model_performance.csv\"):\n",
    "#         all_runs.loc[i, \"done\"] = True\n",
    "#     elif os.path.isfile(f\"{base_dir}/{row.species}/⚠️ too few dead trees.txt\"):\n",
    "#         all_runs.loc[i, \"done\"] = True\n",
    "#     else:\n",
    "#         all_runs.loc[i, \"done\"] = False\n",
    "\n",
    "# # Get missing runs\n",
    "# runs_to_run = (\n",
    "#     all_runs.query(\"done == False\")\n",
    "#     # .sort_values([\"species\", \"seed\"])\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# # ! Option to only run top9 species\n",
    "# # final_species = get_species_with_models(\"list\")\n",
    "# # top9 = all_species_norm.index.tolist()\n",
    "# # runs_to_run = runs_to_run.query(\"species in @top9\")\n",
    "\n",
    "# # Reset index to start from 0\n",
    "# runs_to_run = runs_to_run.reset_index(drop=True)\n",
    "\n",
    "# # ! Option for running on multiple notebooks\n",
    "# # Create multiple notebooks with name 01_model_fitting 1.ipynb, 01_model_fitting 2.ipynb, etc.\n",
    "# # import IPython\n",
    "\n",
    "# # # Number of notebooks\n",
    "# # n_splits = 10\n",
    "# # # Current notebook index\n",
    "# # nb_name = IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"]\n",
    "# # nb_id = int(nb_name.split(\"01_model_fitting \")[-1].split(\".\")[0])\n",
    "# # print(f\"Running notebook {nb_id} with {n_splits} splits\")\n",
    "# # # Select according nested list\n",
    "# # l_runs_to_run = split_df_into_list_of_group_or_ns(runs_to_run, n_splits, \"seed\")\n",
    "# # # l_runs_to_run = split_df_into_list_of_group_or_ns(runs_to_run, n_splits)\n",
    "# # runs_to_run = l_runs_to_run[nb_id].reset_index(drop=True)\n",
    "\n",
    "# # # Sort runs by increasing number of trees\n",
    "# # sort_order = runs_to_run.species.value_counts().index.tolist()\n",
    "\n",
    "# # # Display runs to run\n",
    "# # runs_to_run\n",
    "\n",
    "# # # Get sort order by increasing number of trees\n",
    "# # species_order = all_species_norm.sort_values(ascending=True).index.tolist()\n",
    "\n",
    "# # # Sort all_runs by species_order\n",
    "# # runs_to_run[\"species\"] = runs_to_run[\"species\"].astype(\"category\")\n",
    "# # runs_to_run[\"species\"] = runs_to_run[\"species\"].cat.set_categories(species_order)\n",
    "# # runs_to_run = runs_to_run.sort_values([\"species\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # ! Loop over all runs\n",
    "# for i, row in runs_to_run.iterrows():\n",
    "\n",
    "#     iseed = row.seed\n",
    "#     ispecies = row.species\n",
    "#     idir = row.dir\n",
    "\n",
    "#     if idir == \"\":\n",
    "#         # Create folder for run\n",
    "#         idir = f\"model_runs/all_runs/run_{iseed}\"\n",
    "#         os.makedirs(idir, exist_ok=True)\n",
    "#         all_runs.loc[i, \"dir\"] = idir\n",
    "\n",
    "#     # Start run\n",
    "#     user_input[\"seed_nr\"] = iseed\n",
    "#     display(\"\")\n",
    "#     print(\n",
    "#         f\"\"\"\n",
    "#         --------------------------------------------------------------------------------\n",
    "#         Run {i}/{runs_to_run.shape[0]}\n",
    "#         Seed: {iseed}\n",
    "#         Species: {ispecies}\n",
    "#         Dir: {idir}\n",
    "#         Started: {datetime.datetime.now().strftime('%Y-%m-%d @ %H:%M:%S')}\n",
    "#         --------------------------------------------------------------------------------\n",
    "#         \"\"\"\n",
    "#     )\n",
    "#     ist = start_time(False)\n",
    "#     run_all(ispecies, user_input, base_dir=idir)\n",
    "#     clear_output(wait=True)\n",
    "#     end_time(ist, None, ring=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! osascript -e 'tell app \"System Events\" to shut down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE and RFE (Top 9 Species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# # ! Select which extra analyses to run <<<<<\n",
    "extra_analyses = \"smotek\"  # smotek | rfe_nkeep\n",
    "# species_to_test = [\"Fagus sylvatica\"]  # Species to test for extra analyses\n",
    "species_to_test = top9_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {}\n",
    "\n",
    "# ! Settings for extra analyses ------------------------------------------------\n",
    "# todo: Adjust directory\n",
    "runs_dir = \"./extra_runs/\"\n",
    "# Number of nearest neighbors for SMOTE (default setting is 5)\n",
    "user_input[\"smote_k\"] = 5\n",
    "# Minimum number of features per category to keep (default is 2)\n",
    "user_input[\"min_features_per_category\"] = 2\n",
    "\n",
    "if extra_analyses not in [\"smotek\", \"rfe_nkeep\"]:\n",
    "    raise ValueError(f\"Invalid extra analysis: {extra_analyses}\")\n",
    "elif extra_analyses == \"smotek\":\n",
    "    all_extras = [0, 1, 5, 15]\n",
    "elif extra_analyses == \"rfe_nkeep\":\n",
    "    all_extras = [4, 6]\n",
    "\n",
    "# ! General -----------------------------------------------------------------------\n",
    "user_input[\"dir_suffix\"] = None  # None or string\n",
    "user_input[\"description_file\"] = None\n",
    "user_input[\"subset\"] = [\"species_lat2\"]\n",
    "# ! TRAINING -----------------------------------------------------------------------\n",
    "# Data splitting\n",
    "user_input[\"test_split\"] = 0.2\n",
    "# Feature Elimination\n",
    "user_input[\"do_ref\"] = True\n",
    "user_input[\"method_validation\"] = \"oob\"  # none | cv | oob\n",
    "user_input[\"method_importance\"] = \"impurity\"  # permutation | impurity\n",
    "user_input[\"cv_folds\"] = 5  # Number of folds for if cv is selected for validation\n",
    "user_input[\"do_tuning\"] = False  # Tune during rfe validation?\n",
    "user_input[\"correlation_threshold\"] = 0.8  # Threshold for r correlation removal\n",
    "user_input[\"seed_nr\"] = 51  # Set seed for randomization\n",
    "# Tuning\n",
    "user_input[\"do_prescribed_search\"] = True\n",
    "user_input[\"do_random_search\"] = False\n",
    "user_input[\"gsc_metric\"] = \"roc_auc\"  # grid search metric\n",
    "# ! Final model ---------------------------------------------------------------------\n",
    "# best_per_category | best_metric | best_metric_max1\n",
    "user_input[\"best_model_decision\"] = \"best_per_category\"\n",
    "user_input[\"best_model_metric\"] = \"roc_auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! ------------------------------------------------------------------------------------------------\n",
    "# # ! Note that this is for smote k runs and not for seeds but I am simply reusing the same code\n",
    "# # ! ------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Get all run configurations\n",
    "all_seeds = pd.read_csv(\"all_seeds.csv\").seed.tolist()\n",
    "\n",
    "# Create all combinations of seeds, extras, and species\n",
    "all_runs = pd.DataFrame(\n",
    "    list(itertools.product(all_seeds, all_extras, species_to_test)),\n",
    "    columns=[\"seed\", \"extra\", \"species\"],\n",
    ")\n",
    "all_runs[\"dir\"] = \"\"\n",
    "\n",
    "# ! Set runs\n",
    "runs_to_run = all_runs.copy()\n",
    "if runs_to_run.shape[0] == 0:\n",
    "    raise ValueError(\n",
    "        f\"No runs to run for subset: {subset}. Please check if the species are available in the dataset.\"\n",
    "    )\n",
    "\n",
    "# Expected number of runs\n",
    "expected_runs = len(all_seeds) * len(all_extras) * len(species_to_test)\n",
    "\n",
    "if runs_to_run.shape[0] != expected_runs:\n",
    "    raise ValueError(\n",
    "        f\"Expected {expected_runs} runs but got {runs_to_run.shape[0]} runs. Please check the configuration.\"\n",
    "    )\n",
    "\n",
    "# ! Filter for completed runs\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Filter out completed or failed runs before splitting\n",
    "def is_run_incomplete(row):\n",
    "    iseed, ispecies, iextra = row[\"seed\"], row[\"species\"], row[\"extra\"]\n",
    "\n",
    "    if extra_analyses == \"smotek\":\n",
    "        tmp_dir = f\"./extra_runs/smotek_{iextra}\"\n",
    "    elif extra_analyses == \"rfe_nkeep\":\n",
    "        tmp_dir = f\"./extra_runs/rfe_nkeep_{iextra}\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid 'extra_analyses'! Got: {extra_analyses}. Expected 'smotek' or 'rfe_nkeep'.\"\n",
    "        )\n",
    "\n",
    "    completed = f\"{tmp_dir}/run_{iseed}/{ispecies}/final_model_performance.csv\"\n",
    "    failed = f\"{tmp_dir}/run_{iseed}/{ispecies}/⚠️ too few dead trees.txt\"\n",
    "\n",
    "    # Return false if the run is completed or failed\n",
    "    if os.path.isfile(completed) or os.path.isfile(failed):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "runs_to_run[\"done\"] = runs_to_run.apply(is_run_incomplete, axis=1)\n",
    "runs_to_run = runs_to_run.query(\"done == False\").reset_index(drop=True)\n",
    "runs_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Option for running on multiple notebooks\n",
    "## Before running:\n",
    "## 1. Create 10 multiple notebooks with name pattern:\n",
    "##   -`01_model_fitting copy 0.ipynb`,\n",
    "##   -`01_model_fitting copy 1.ipynb`,\n",
    "##   -`01_model_fitting copy 2.ipynb`,\n",
    "##   ...\n",
    "##   -`01_model_fitting copy 9.ipynb`,\n",
    "## 2. Uncomment the code below to run the notebooks in parallel.\n",
    "## 3. Add `raise` stopper to the end of the code block to stop execution after this point.\n",
    "## 4. Run code below in each notebook.\n",
    "\n",
    "\n",
    "# Imports\n",
    "import IPython\n",
    "\n",
    "# ! For using multiple notebooks ---\n",
    "# Number of notebooks\n",
    "n_splits = 5\n",
    "# Current notebook index\n",
    "nb_name = IPython.extract_module_locals()[1][\"__vsc_ipynb_file__\"]\n",
    "nb_id = int(nb_name.split(\"01_model_fitting copy \")[-1].split(\".\")[0])\n",
    "# nb_id = 0  # todo: REMOVE THIS\n",
    "\n",
    "print(f\" - Running notebook {nb_id} with {n_splits} splits\")\n",
    "# Select according nested list\n",
    "l_runs_to_run = split_df_into_list_of_group_or_ns(runs_to_run, n_splits, \"seed\")\n",
    "# l_runs_to_run = split_df_into_list_of_group_or_ns(runs_to_run, n_splits)\n",
    "runs_to_run = l_runs_to_run[nb_id].reset_index(drop=True)\n",
    "\n",
    "# ! Skip to here for only one notebook ---\n",
    "\n",
    "# Get sort order by increasing number of trees\n",
    "species_order = all_species_norm.sort_values(ascending=True).index.tolist()\n",
    "# Sort all_runs by species_order\n",
    "runs_to_run[\"species\"] = runs_to_run[\"species\"].astype(\"category\")\n",
    "runs_to_run[\"species\"] = runs_to_run[\"species\"].cat.set_categories(species_order)\n",
    "runs_to_run = runs_to_run.sort_values([\"species\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\" - Seeds to run: \\t\\t{runs_to_run.seed.unique().tolist()}\")\n",
    "print(f\" - Extra {extra_analyses} to run: \\t{runs_to_run.extra.unique()}\")\n",
    "print(f\" - Species to run: \\t\\t{runs_to_run.species.unique().tolist()}\")\n",
    "print(f\" - Number of runs to run: \\t{runs_to_run.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Loop over all runs\n",
    "for i, row in runs_to_run.iterrows():\n",
    "\n",
    "    iseed = row.seed\n",
    "    ispecies = row.species\n",
    "    idir = row.dir\n",
    "    iextra = row.extra\n",
    "\n",
    "    # Set correct user input\n",
    "    user_input[\"seed_nr\"] = iseed\n",
    "    if extra_analyses == \"smotek\":\n",
    "        dir_prefix = f\"smotek_{iextra}\"\n",
    "        user_input[\"smote_k\"] = iextra\n",
    "        if user_input[\"min_features_per_category\"] != 2:\n",
    "            raise ValueError(\n",
    "                f\"Minimum number of features per category is set to {user_input['min_features_per_category']} but should be 2 for smotek runs!\"\n",
    "            )\n",
    "    elif extra_analyses == \"rfe_nkeep\":\n",
    "        dir_prefix = f\"rfe_nkeep_{iextra}\"\n",
    "        user_input[\"min_features_per_category\"] = iextra\n",
    "        if user_input[\"smote_k\"] != 5:\n",
    "            raise ValueError(\n",
    "                f\"SMOTE k is set to {user_input['smote_k']} but should be 5 for rfe_nkeep runs!\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid 'extra_analyses'! Got: {extra_analyses}. Expected 'smotek' or 'rfe_nkeep' in the path.\"\n",
    "        )\n",
    "\n",
    "    # Create folder for run\n",
    "    all_runs.loc[i, \"dir\"] = idir\n",
    "    idir = f\"{runs_dir}/{dir_prefix}/run_{iseed}\"\n",
    "    os.makedirs(idir, exist_ok=True)\n",
    "\n",
    "    # Check if run is already done\n",
    "    if os.path.isfile(f\"{idir}/{ispecies}/final_model_performance.csv\"):\n",
    "        continue\n",
    "    elif os.path.isfile(f\"{idir}/{ispecies}/⚠️ too few dead trees.txt\"):\n",
    "        continue\n",
    "\n",
    "    # Run it\n",
    "    display(\"\")\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        --------------------------------------------------------------------------------\n",
    "        Progress: {i}/{runs_to_run.shape[0]}\n",
    "        Seed: {user_input['seed_nr']}\n",
    "        k: {user_input['smote_k']}\n",
    "        rfe_nkeep: {user_input['min_features_per_category']}\n",
    "        Species: {ispecies}\n",
    "        Dir: {idir}\n",
    "        Started: {datetime.datetime.now().strftime('%Y-%m-%d @ %H:%M:%S')}\n",
    "        --------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "    )\n",
    "    ist = start_time(False)\n",
    "    run_all(ispecies, user_input, base_dir=idir)\n",
    "    clear_output(wait=True)\n",
    "    end_time(ist, None, ring=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Fitting Calculations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Final Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available runs directories\n",
    "available_dirs = [\"./model_runs\"] + sorted(glob.glob(\"./extra_runs/*\"))\n",
    "print(\"Available runs directories:\")\n",
    "for i, d in enumerate(available_dirs):\n",
    "    print(f\"{i + 1}: {d}\")\n",
    "\n",
    "# ! SET RUNS DIRECTORY\n",
    "# runs_dir = \"./model_runs/all_runs\"\n",
    "# runs_dir = \"./extra_runs/*\"\n",
    "# runs_dir = \"./extra_runs/smote*\"\n",
    "runs_dir = \"./extra_runs/rfe*\"\n",
    "\n",
    "# ! SET SPECIES TO ANALYZE\n",
    "# species_subset = get_species_with_models(\"list\")\n",
    "species_subset = top9_species\n",
    "# species_subset = [\"Abies alba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model directories\n",
    "df_available = glob.glob(f\"{runs_dir}/run_*/*/final_model_performance.csv\")\n",
    "df_available = pd.DataFrame(df_available, columns=[\"file\"])\n",
    "df_available[\"species\"] = df_available[\"file\"].str.split(\"/\").str[-2]\n",
    "df_available = df_available.query(\"species in @species_subset\").reset_index(drop=True)\n",
    "df_available[\"model\"] = df_available[\"file\"].str.split(\"/\").str[-3]\n",
    "df_available[\"base_dir\"] = df_available[\"file\"].str.split(\"/run\").str[0] + \"/\"\n",
    "\n",
    "# Check for missing runs\n",
    "print(\" --- The following species do not have their 50 seed runs yet: ---\")\n",
    "print(f\" - Species found: {df_available['species'].nunique()}\")\n",
    "print(f\" - Seeds found: {df_available['model'].nunique()}\")\n",
    "if \"extra\" in runs_dir:\n",
    "    df_available[\"extraid\"] = (\n",
    "        df_available[\"file\"].str.split(\"/run_\").str[0].str.split(\"_\").str[-1]\n",
    "    )\n",
    "    print(f\" - Extra IDs found: {df_available['extraid'].nunique()}\")\n",
    "    display(\n",
    "        df_available[[\"extraid\", \"species\"]]\n",
    "        .value_counts()\n",
    "        .sort_values()[\n",
    "            df_available[[\"extraid\", \"species\"]].value_counts().sort_values() < 50\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    display(\n",
    "        df_available[[\"species\"]]\n",
    "        .value_counts()\n",
    "        .sort_values()[df_available[[\"species\"]].value_counts().sort_values() < 50]\n",
    "    )\n",
    "\n",
    "# df_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to missing runs\n",
    "df_todo = []\n",
    "for i, row in df_available.iterrows():\n",
    "    # Check if file exists\n",
    "    ifile = row.file.replace(\n",
    "        \"final_model_performance.csv\",\n",
    "        \"rf_performance/classification_metrics_fixed_threshold.csv\",\n",
    "    )\n",
    "\n",
    "    if not os.path.isfile(ifile):\n",
    "        df_todo.append(row)\n",
    "\n",
    "df_todo = pd.DataFrame(df_todo)\n",
    "\n",
    "if df_todo.shape[0] == 0:\n",
    "    raise ValueError(\"✅ All model have been run!\")\n",
    "else:\n",
    "    display(df_todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model performance\n",
    "from random_forest_utils import calculate_rf_performance\n",
    "\n",
    "ncores = 5\n",
    "run_mp(\n",
    "    calculate_rf_performance,\n",
    "    split_df_into_list_of_group_or_ns(df_todo, ncores, \"model\"),\n",
    "    skip_if_csv_exists=False,\n",
    "    progress_bar=True,\n",
    "    num_cores=ncores,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SHAP Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to missing runs\n",
    "df_todo = []\n",
    "for i, row in df_available.iterrows():\n",
    "    # Check if file exists\n",
    "    ifile = row.file.replace(\n",
    "        \"final_model_performance.csv\",\n",
    "        \"shap/approximated/shap_values_test.pkl\",\n",
    "    )\n",
    "    if not os.path.isfile(ifile):\n",
    "        df_todo.append(row)\n",
    "df_todo = pd.DataFrame(df_todo)\n",
    "if df_todo.shape[0] == 0:\n",
    "    raise ValueError(\"✅ All model have been run!\")\n",
    "else:\n",
    "    display(df_todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "\n",
    "shap_run_new_loop_mp(\n",
    "    df_todo,\n",
    "    run_interaction=False,\n",
    "    approximate=True,\n",
    "    test_or_train=\"test\",\n",
    "    force_run=False,\n",
    "    verbose=False,\n",
    "    num_cores=9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SHAP Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to missing runs\n",
    "df_todo = []\n",
    "for i, row in df_available.iterrows():\n",
    "    # Check if file exists\n",
    "    ifile = row.file.replace(\n",
    "        \"final_model_performance.csv\",\n",
    "        \"final_model_performance_org.csv\",\n",
    "    )\n",
    "    if not os.path.isfile(ifile):\n",
    "        df_todo.append(row)\n",
    "df_todo = pd.DataFrame(df_todo)\n",
    "if df_todo.shape[0] == 0:\n",
    "    raise ValueError(\"✅ All model have been run!\")\n",
    "else:\n",
    "    display(df_todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP Variable Importance\n",
    "\n",
    "# Loop over runs and species and calculate mean absolute SHAP values\n",
    "# for i, row in tqdm(df_todo.head().iterrows(), total=df_todo.head().shape[0]): # ! Only for subset!\n",
    "for i, row in tqdm(df_todo.iterrows(), total=df_todo.shape[0]):  # ! For all runs!\n",
    "    # Get predictor data\n",
    "    runs_dir = row.base_dir\n",
    "    ipreds = f\"{runs_dir}/{row.model}/{row.species}/final_model/X_test.csv\"\n",
    "    ipreds = pd.read_csv(ipreds, index_col=[0])\n",
    "\n",
    "    # Get SHAP data\n",
    "    ishap = (\n",
    "        f\"{runs_dir}/{row.model}/{row.species}/shap/approximated/shap_values_test.pkl\"\n",
    "    )\n",
    "    if not os.path.exists(ishap):\n",
    "        raise ValueError(\n",
    "            f\" 🚨 Skipping {row.model}/{row.species} because no SHAP values calculated yet!\"\n",
    "        )\n",
    "    ishap = load_shap(ishap)\n",
    "\n",
    "    # Extract SHAP values per prediction (saved in third dimension)\n",
    "    ishap = ishap.values[:, :, 1]\n",
    "\n",
    "    # Get the row of SHAP values to have a basis to add to\n",
    "    ishapAll = pd.DataFrame(ishap[0].tolist()).T\n",
    "\n",
    "    # Give the df the correct predictor names\n",
    "    ishapAll.columns = ipreds.columns\n",
    "\n",
    "    # Loop over all SHAP predictions and concatenate\n",
    "    for j in range(1, len(ishap)):\n",
    "        iii = pd.DataFrame(ishap[j].tolist()).T\n",
    "        iii.columns = ipreds.columns\n",
    "        ishapAll = pd.concat([ishapAll, iii], axis=0, ignore_index=True)\n",
    "\n",
    "    # Safety check: Shape of predictors should be the same as for SHAP values\n",
    "    if ipreds.shape != ishapAll.shape:\n",
    "        print(\n",
    "            f\" - Issue: The shape of the predictor data should equal the shape of the concatenated SHAP values!\"\n",
    "        )\n",
    "\n",
    "    # Take mean of SHAP values across all variables\n",
    "    ishapMean_org = ishapAll.abs().mean().sort_values(ascending=False)\n",
    "    ishapMean = ishapMean_org / ishapMean_org.sum()\n",
    "    ishapMean = pd.DataFrame(ishapMean)\n",
    "    ishapMean.columns = [\"Importance\"]\n",
    "    ishapMean.Importance = ishapMean.Importance * 100\n",
    "    ishapMean[\"Feature\"] = ishapMean.index\n",
    "    ishapMean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Link feature variable to predictor dataset in new column\n",
    "    # Load predictor dictionary\n",
    "    dict_preds = json.load(open(f\"./model_runs/feature_category_dictionary.json\"))\n",
    "    for f in ishapMean.Feature:\n",
    "        for key, value in dict_preds.items():\n",
    "            if f in value:\n",
    "                ishapMean.loc[ishapMean.Feature == f, \"dataset\"] = key\n",
    "\n",
    "    # Sum up the VI for each dataset\n",
    "    ishapMean_of_dataset = (\n",
    "        ishapMean[[\"Importance\", \"dataset\"]]\n",
    "        .groupby(\"dataset\")\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename({\"Importance\": \"dataset_imp\"}, axis=1)\n",
    "    )\n",
    "\n",
    "    ishapMean_of_dataset.dataset_imp = (\n",
    "        ishapMean_of_dataset.dataset_imp / ishapMean_of_dataset.dataset_imp.sum() * 100\n",
    "    )\n",
    "\n",
    "    ishapMean[\"mean_abs_shap_org\"] = ishapMean_org.values\n",
    "\n",
    "    # Attach dataset label with percentages\n",
    "    for j, jrow in ishapMean_of_dataset.iterrows():\n",
    "        ishapMean_of_dataset.loc[j, \"dataset_label\"] = (\n",
    "            str(round(ishapMean_of_dataset.loc[j, \"dataset_imp\"]))\n",
    "            + \"%: \"\n",
    "            + ishapMean_of_dataset.loc[j, \"dataset\"]\n",
    "        )\n",
    "\n",
    "    ishapMean = ishapMean.merge(ishapMean_of_dataset, on=\"dataset\", how=\"left\")\n",
    "\n",
    "    # Save SHAP data\n",
    "    ishapMean.to_csv(\n",
    "        f\"{runs_dir}/{row.model}/{row.species}/shap_variable_importance.csv\"\n",
    "    )\n",
    "\n",
    "    # Load final model performance\n",
    "    ifinalOrg = f\"{runs_dir}/{row.model}/{row.species}/final_model_performance_org.csv\"\n",
    "    ifinalNew = f\"{runs_dir}/{row.model}/{row.species}/final_model_performance.csv\"\n",
    "\n",
    "    # If the original file has not yet been backuped, save it!\n",
    "    if not os.path.exists(ifinalOrg):\n",
    "        shutil.copy2(ifinalNew, ifinalOrg)\n",
    "\n",
    "    # Load model performance file, attach SHAP information and save it again\n",
    "    ifinalNewDf = pd.read_csv(ifinalNew)\n",
    "\n",
    "    for dataset in ishapMean.dataset.unique():\n",
    "        ifinalNewDf[f\"{dataset} - Importance\"] = ishapMean.loc[\n",
    "            ishapMean.dataset == dataset, \"dataset_imp\"\n",
    "        ].values[0]\n",
    "        ifinalNewDf[f\"{dataset} - Metrics\"] = [\n",
    "            ishapMean.loc[ishapMean.dataset == dataset, \"Feature\"].values\n",
    "        ]\n",
    "        ifinalNewDf[f\"{dataset} - Values\"] = [\n",
    "            ishapMean.loc[ishapMean.dataset == dataset, \"Importance\"].values\n",
    "        ]\n",
    "\n",
    "    ifinalNewDf.to_csv(ifinalNew, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! osascript -e 'tell app \"System Events\" to shut down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
