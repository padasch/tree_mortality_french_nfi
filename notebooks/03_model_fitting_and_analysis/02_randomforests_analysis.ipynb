{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Random Forest Models\n",
    "\n",
    "**Important 🚨:** If only the demo data for the most common nine species is used, the results produced for all 52 species will naturally be different from the display items in the publication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()\n",
    "\n",
    "# Imports\n",
    "import matplotlib\n",
    "from matplotlib import cm, ticker\n",
    "from curlyBrace import curlyBrace\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 50)\n",
    "# To avoid these copy warnings\n",
    "pd.options.mode.copy_on_write = True\n",
    "# Set global font family to Helvetica\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Arial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df for top_species\n",
    "nfi_raw = get_final_nfi_data_for_analysis(verbose=False).query(\n",
    "    \"tree_state_change in ['alive_alive', 'alive_dead']\"\n",
    ")\n",
    "\n",
    "# Get normalized and non normalized counts of trees per species\n",
    "df_top_species = nfi_raw.copy()\n",
    "df_top_species[\"species\"] = df_top_species[\"species_lat2\"]\n",
    "tmp_abs = df_top_species[\"species\"].value_counts()\n",
    "tmp_norm = df_top_species[\"species\"].value_counts(normalize=True)\n",
    "top_species_all = pd.concat(\n",
    "    [tmp_abs, tmp_norm], axis=1, keys=[\"count\", \"percent\"]\n",
    ").reset_index()\n",
    "top9 = top_species_all.head(9).species.tolist()\n",
    "print(f\"Top 9 species: {top9}\")\n",
    "\n",
    "# Get species-family dictionary\n",
    "species_family = (\n",
    "    nfi_raw[[\"species_lat2\", \"family_lat\"]].drop_duplicates().reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Get number of trees per species\n",
    "df_ntrees = (\n",
    "    df_top_species.query(\"tree_state_change in ['alive_alive', 'alive_dead']\")\n",
    "    .groupby([\"species\", \"tree_state_change\"], observed=False)\n",
    "    .agg(n_trees=(\"tree_id\", \"count\"))\n",
    "    .reset_index()\n",
    "    .query(\"n_trees > 0\")\n",
    "    .pivot(index=\"species\", columns=\"tree_state_change\", values=\"n_trees\")\n",
    "    .replace(np.nan, 0)\n",
    "    .assign(n_trees_total=lambda x: x[\"alive_alive\"] + x[\"alive_dead\"])\n",
    "    .astype(int)\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "\n",
    "df_species_genus = (\n",
    "    nfi_raw[[\"species_lat2\", \"genus_lat\"]].drop_duplicates().reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Show\n",
    "display(df_ntrees.sort_values(\"alive_alive\", ascending=False).head(5))\n",
    "display(df_species_genus.head(5))\n",
    "\n",
    "# Load species for which final models were created\n",
    "species_in_final_anlysis = get_species_with_models(\"list\")\n",
    "species_in_final_anlysis = list(sorted(species_in_final_anlysis))\n",
    "\n",
    "# Get species ordered by number of trees\n",
    "df_ntrees = df_ntrees.sort_values(\"n_trees_total\", ascending=False).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "df_ntrees = df_ntrees.query(\"species in @species_in_final_anlysis\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "n_species = df_top_species.query(\n",
    "    \"tree_state_change in ['alive_alive', 'alive_dead'] and species_lat2 in @species_in_final_anlysis\"\n",
    ").species_lat2.nunique()\n",
    "\n",
    "n_sites = df_top_species.query(\n",
    "    \"tree_state_change in ['alive_alive', 'alive_dead'] and species_lat2 in @species_in_final_anlysis\"\n",
    ").idp.nunique()\n",
    "\n",
    "display(\"--------\")\n",
    "print(f\"Number of species: {n_species}\")\n",
    "print(f\"Number of sites: {n_sites}\")\n",
    "print(f\"{'Total trees (all)':25} {df_ntrees['n_trees_total'].sum():<30} 100.00%\")\n",
    "print(\n",
    "    f\"{'Total trees (surviving)':25} {df_ntrees['alive_alive'].sum():<30} {df_ntrees['alive_alive'].sum() / df_ntrees['n_trees_total'].sum() * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Total trees (dead)':25} {df_ntrees['alive_dead'].sum():<30} {df_ntrees['alive_dead'].sum() / df_ntrees['n_trees_total'].sum() * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dfs for single responses\n",
    "def summarize_runs_1pattern(df_responses, spei_or_temp, all_or_top9):\n",
    "\n",
    "    if all_or_top9 == \"top9\":\n",
    "        top9 = [\n",
    "            \"Fagus sylvatica\",\n",
    "            \"Quercus robur\",\n",
    "            \"Quercus petraea\",\n",
    "            \"Carpinus betulus\",\n",
    "            \"Castanea sativa\",\n",
    "            \"Quercus pubescens\",\n",
    "            \"Pinus sylvestris\",\n",
    "            \"Abies alba\",\n",
    "            \"Picea abies\",\n",
    "        ]\n",
    "        df_responses = df_responses.query(\"species in @top9\")\n",
    "\n",
    "    sry = (\n",
    "        df_responses.copy()\n",
    "        .groupby([f\"response_{spei_or_temp}\"])\n",
    "        .agg(group_size=(\"group_size\", \"sum\"), n_groups=(\"species\", \"count\"))\n",
    "        .sort_values(\"group_size\", ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Attach relative group size\n",
    "    sry[\"group_size_rel\"] = (\n",
    "        (sry[\"group_size\"] / sry[\"group_size\"].sum() * 100).round().astype(int)\n",
    "    )\n",
    "\n",
    "    # Attach percentages\n",
    "    sry[f\"response_{spei_or_temp}\"] = (\n",
    "        sry[f\"response_{spei_or_temp}\"]\n",
    "        + \" (\"\n",
    "        + sry[\"group_size_rel\"].astype(str)\n",
    "        + \"%)\"\n",
    "    )\n",
    "\n",
    "    return sry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bars_dataset_pattern(\n",
    "    patterns_merged,\n",
    "    all_dfs,\n",
    "    all_or_top9=\"all\",\n",
    "    color_temp=\"#77422C\",\n",
    "    color_spei=\"#D1A289\",\n",
    "    color_rest=\"lightgrey\",\n",
    "    color_wd=\"#B2182B\",  # Original: \"#B2182B\"\n",
    "    color_ww=\"#2166AC\",  # Original: \"#2166AC\"\n",
    "    color_other=\"lightgrey\",\n",
    "    color_cd=\"lightgrey\",  # Original: \"#EF8A62\",\n",
    "    color_cw=\"lightgrey\",  # Original: \"#67A9CF\",\n",
    "    base_fontsize=12,\n",
    "    filepath=None,\n",
    "    ytick_labels=[\n",
    "        \"Warmer\".title(),\n",
    "        \"Cooler\".title(),\n",
    "        \"Other\".title(),\n",
    "        \"Drier\".title(),\n",
    "        \"Wetter\".title(),\n",
    "        \"Other\".title(),\n",
    "        \"Warmer + Drier\".title(),\n",
    "        \"Warmer + Wetter\".title(),\n",
    "        \"Other\".title(),\n",
    "        \"Cooler + Drier\".title(),\n",
    "        \"Cooler + Wetter\".title(),\n",
    "    ],\n",
    "):\n",
    "    # Plot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Barplot for importance\n",
    "    ax_dataset_boxplot(\n",
    "        axs[0],\n",
    "        all_dfs,\n",
    "        imps,\n",
    "        base_fontsize,\n",
    "        color_spei=color_spei,\n",
    "        color_temp=color_temp,\n",
    "        color_rest=color_rest,\n",
    "        all_or_top9=all_or_top9,\n",
    "    )\n",
    "\n",
    "    # Barplot for patterns\n",
    "    sns.barplot(\n",
    "        data=patterns_merged,\n",
    "        x=\"group_size_rel\",\n",
    "        y=\"change_simple\",\n",
    "        hue=\"change_simple\",\n",
    "        palette=[\n",
    "            color_temp,\n",
    "            color_temp,\n",
    "            color_temp,\n",
    "            color_spei,\n",
    "            color_spei,\n",
    "            color_spei,\n",
    "            color_wd,\n",
    "            color_ww,\n",
    "            color_other,\n",
    "            color_cd,\n",
    "            color_cw,\n",
    "        ],\n",
    "        orient=\"h\",\n",
    "        height=0.5,\n",
    "        dodge=False,\n",
    "        edgecolor=\"black\",\n",
    "        # hue=\"response_spei\",\n",
    "        # palette=[\n",
    "        #     \"#B2182B\",\n",
    "        #     \"#2166AC\",\n",
    "        #     \"grey\",\n",
    "        # ],\n",
    "        ax=axs[1],\n",
    "    )\n",
    "\n",
    "    # Add values to end of bars\n",
    "    for i in range(len(patterns_merged)):\n",
    "        axs[1].text(\n",
    "            patterns_merged.loc[i, \"group_size_rel\"] + 1,\n",
    "            i + -0.1,\n",
    "            f\"{patterns_merged.loc[i, 'group_size_rel']}%\",\n",
    "            va=\"center\",\n",
    "            fontsize=base_fontsize * 0.9,\n",
    "        )\n",
    "\n",
    "    # Add horizontal lines\n",
    "    axs[1].axhline(2.35, color=\"black\", linewidth=1)\n",
    "    axs[1].axhline(5.35, color=\"black\", linewidth=1)\n",
    "\n",
    "    # Add text\n",
    "    axs[1].text(\n",
    "        95,\n",
    "        2,\n",
    "        \"Temperature\\nanomaly\",\n",
    "        ha=\"right\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=base_fontsize * 1,\n",
    "    )\n",
    "    axs[1].text(\n",
    "        95,\n",
    "        5,\n",
    "        \"Climatic water\\nbalance anomaly\",\n",
    "        ha=\"right\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=base_fontsize * 1,\n",
    "    )\n",
    "    axs[1].text(\n",
    "        95,\n",
    "        10,\n",
    "        \"Combined\",\n",
    "        ha=\"right\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=base_fontsize * 1,\n",
    "    )\n",
    "\n",
    "    # Add labels\n",
    "    axs[1].set_xlabel(\n",
    "        \"Model percentage (%)\",\n",
    "        fontweight=\"bold\",\n",
    "        labelpad=10,\n",
    "        fontsize=base_fontsize * 1.2,\n",
    "    )\n",
    "    axs[1].set_ylabel(\n",
    "        # \"Climatic conditions before 2$^{\\\\text{nd}}$ visit\",\n",
    "        \"Short-term climatic condition\\npromoting mortality\",\n",
    "        labelpad=10,\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=base_fontsize * 1.2,\n",
    "    )\n",
    "\n",
    "    # Fix y-ticks\n",
    "    axs[1].set_yticks(range(len(patterns_merged[\"change_simple\"])))\n",
    "    axs[1].set_yticklabels(\n",
    "        patterns_merged[\"change_simple\"], va=\"bottom\", fontsize=base_fontsize * 1\n",
    "    )\n",
    "    axs[1].tick_params(axis=\"y\", which=\"both\", left=False)\n",
    "\n",
    "    # Rename y-ticks\n",
    "    axs[1].set_yticklabels(ytick_labels)\n",
    "\n",
    "    # Fix axis limits\n",
    "    axs[1].set_xlim(0, 100)\n",
    "    axs[1].set_ylim(10.4, -0.4)\n",
    "\n",
    "    # Remove top and right axis\n",
    "    axs[1].spines[\"top\"].set_visible(False)\n",
    "    axs[1].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # Add letters\n",
    "    letters = [\"A\", \"B\"]\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.text(\n",
    "            -0.5,\n",
    "            0.99,\n",
    "            letters[i],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=base_fontsize * 1.3,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # Fix layout\n",
    "    # Fix x-tick size\n",
    "    axs[0].tick_params(axis=\"x\", which=\"both\", labelsize=base_fontsize * 0.8)\n",
    "    axs[1].tick_params(axis=\"x\", which=\"both\", labelsize=base_fontsize * 0.8)\n",
    "    plt.tight_layout(w_pad=2, h_pad=1)\n",
    "\n",
    "    if filepath is not None:\n",
    "        plt.savefig(filepath, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pie_heatmap_data(\n",
    "    change,\n",
    "    temp_or_spei,\n",
    "    tempy=\"anom\",\n",
    "    verbose=False,\n",
    "    df_double_change=None,\n",
    "    df_single_change=None,\n",
    "    method_scaling_importance=\"scaled_valid\",\n",
    "):\n",
    "\n",
    "    if \"_\" in change:\n",
    "        df = df_double_change.copy()\n",
    "    else:\n",
    "        df = df_single_change.copy()\n",
    "\n",
    "    if temp_or_spei == \"spei\":\n",
    "        # Get a heatmap version that holds anomaly information\n",
    "        test = df.copy()\n",
    "        test[\"spei_dur_season\"] = (\n",
    "            test[\"spei_duration\"].astype(str) + \"_\" + test[\"spei_season\"]\n",
    "        )\n",
    "\n",
    "        test = get_spei_table(\n",
    "            df=test,\n",
    "            select_change=change,\n",
    "            x_var=\"spei_dur_season\",\n",
    "            x_var_order=test[\"spei_dur_season\"].unique(),\n",
    "            y_var=\"spei_anom\",\n",
    "            y_var_order=[\"max\", \"min\", \"mean\"],\n",
    "            round_decimals=False,\n",
    "            method_scaling_importance=method_scaling_importance,\n",
    "        ).T\n",
    "\n",
    "        # Normalize per row\n",
    "        test = test.div(test.sum(axis=1), axis=0)\n",
    "        if verbose:\n",
    "            display(test)\n",
    "\n",
    "        # Get heatmap to be plotted\n",
    "        df_heatmap = get_spei_table(\n",
    "            df=df,\n",
    "            select_change=change,\n",
    "            y_var=\"spei_duration\",\n",
    "            y_var_order=[1, 3, 6, 9, 12, 15, 18, 21, 24],\n",
    "            # y_var_order=[24, 21, 18, 15, 12, 9, 6, 3, 1],\n",
    "            x_var=\"spei_season\",\n",
    "            x_var_order=[\"win\", \"spr\", \"sum\", \"aut\"],\n",
    "            # x_var_order=[\"aut\", \"sum\", \"spr\", \"win\"],\n",
    "            method_scaling_importance=method_scaling_importance,\n",
    "        )\n",
    "        if verbose:\n",
    "            display(df_heatmap)\n",
    "\n",
    "        # Merge heatmap and anomaly information\n",
    "        df_heatmap_long = df_heatmap.reset_index().melt(\n",
    "            id_vars=\"spei_duration\", var_name=\"spei_season\", value_name=\"value\"\n",
    "        )\n",
    "        df_heatmap_long[\"spei_dur_season\"] = (\n",
    "            df_heatmap_long[\"spei_duration\"].astype(str)\n",
    "            + \"_\"\n",
    "            + df_heatmap_long[\"spei_season\"]\n",
    "        )\n",
    "        test = df_heatmap_long.merge(test, on=\"spei_dur_season\")\n",
    "\n",
    "        test[\"size\"] = test[\"value\"]\n",
    "\n",
    "        test = test[\n",
    "            [\"spei_duration\", \"spei_season\", \"value\", \"size\", \"max\", \"min\", \"mean\"]\n",
    "        ]\n",
    "        # test.to_clipboard(index=False)\n",
    "        if verbose:\n",
    "            display(test)\n",
    "    elif temp_or_spei == \"temp\":\n",
    "\n",
    "        if tempy == \"anom\":\n",
    "            tempy = \"temp_anom\"\n",
    "            tempz = \"temp_metric\"\n",
    "        elif tempy == \"metric\":\n",
    "            tempy = \"temp_metric\"\n",
    "            tempz = \"temp_anom\"\n",
    "        else:\n",
    "            raise ValueError(f\"🟥 tempy not recognized: {tempy}\")\n",
    "\n",
    "        # Get a heatmap version that holds anomaly information\n",
    "        test = df.copy()\n",
    "        test[f\"{tempy}_season\"] = (\n",
    "            test[f\"{tempy}\"].astype(str) + \"_\" + test[\"temp_season\"]\n",
    "        )\n",
    "\n",
    "        test = get_spei_table(\n",
    "            df=test,\n",
    "            select_change=change,\n",
    "            x_var=f\"{tempy}_season\",\n",
    "            x_var_order=test[f\"{tempy}_season\"].unique(),\n",
    "            y_var=f\"{tempz}\",\n",
    "            y_var_order=[\"max\", \"min\", \"mean\"],\n",
    "            round_decimals=False,\n",
    "            method_scaling_importance=method_scaling_importance,\n",
    "        ).T\n",
    "\n",
    "        # Normalize per row\n",
    "        test = test.div(test.sum(axis=1), axis=0)\n",
    "        if verbose:\n",
    "            display(test)\n",
    "\n",
    "        # Get heatmap to be plotted\n",
    "        df_heatmap = get_spei_table(\n",
    "            df=df,\n",
    "            select_change=change,\n",
    "            y_var=f\"{tempy}\",\n",
    "            y_var_order=[\"max\", \"min\", \"mean\"],\n",
    "            x_var=\"temp_season\",\n",
    "            x_var_order=[\"win\", \"spr\", \"sum\", \"aut\"],\n",
    "            round_decimals=False,\n",
    "            method_scaling_importance=method_scaling_importance,\n",
    "        )\n",
    "        if verbose:\n",
    "            display(df_heatmap)\n",
    "\n",
    "        # Merge heatmap and anomaly information\n",
    "        df_heatmap_long = df_heatmap.reset_index().melt(\n",
    "            id_vars=f\"{tempy}\", var_name=\"temp_season\", value_name=\"value\"\n",
    "        )\n",
    "        df_heatmap_long[f\"{tempy}_season\"] = (\n",
    "            df_heatmap_long[f\"{tempy}\"].astype(str)\n",
    "            + \"_\"\n",
    "            + df_heatmap_long[\"temp_season\"]\n",
    "        )\n",
    "        test = df_heatmap_long.merge(test, on=f\"{tempy}_season\")\n",
    "\n",
    "        test[\"size\"] = test[\"value\"]\n",
    "\n",
    "        test = test[[f\"{tempy}\", \"temp_season\", \"value\", \"size\", \"max\", \"min\", \"mean\"]]\n",
    "        # test.to_clipboard(index=False)\n",
    "        if verbose:\n",
    "            display(test)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"🟥 temp_or_spei not recognized\")\n",
    "\n",
    "    test.replace(\n",
    "        {\n",
    "            \"win\": \"Winter\",\n",
    "            \"spr\": \"Spring\",\n",
    "            \"sum\": \"Summer\",\n",
    "            \"aut\": \"Autumn\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! --------------------------------------------------------------------------------------------\n",
    "def make_pie_chart_spei_v4(\n",
    "    single_or_double_change=None,\n",
    "    dfchange1=None,\n",
    "    dfchange2=None,\n",
    "    only_top9=False,\n",
    "    size_scale=1,\n",
    "    base_text=1,\n",
    "    fig_scale=1,\n",
    "    scale_by_nmodels=False,\n",
    "    scale_by100perc=False,\n",
    "    min_pie_size=0,\n",
    "    add_boxes=False,\n",
    "    dir_pie=None,\n",
    "    add_perc_to_legend=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    # ! Locals\n",
    "    record_ring_sizes = []\n",
    "\n",
    "    # ! Parameters\n",
    "    x_var = \"spei_season\"\n",
    "    y_var = \"spei_duration\"\n",
    "    n_spaces = 1\n",
    "\n",
    "    legend_title_drier = (\n",
    "        \"Drier conditions promoting mortality\"  # \"Meaning of drier conditions\"\n",
    "    )\n",
    "    legend_title_wetter = (\n",
    "        # \"Wetter conditions promoting mortality\"  # \"Meaning of wetter conditions\"\n",
    "        \"\"\n",
    "    )\n",
    "\n",
    "    legend_labels = (\n",
    "        [\n",
    "            \"Drier seasonal peak\",  # \"Less intense wet events\",\n",
    "            \"Drier seasonal low\",  # \"More intense dry events\",\n",
    "            \"Drier seasonal average\",  # \"Increase in average dryness\",\n",
    "        ]\n",
    "        + np.repeat(np.array(\"\"), n_spaces).tolist()\n",
    "        + [\n",
    "            \"Wetter seasonal peak\",  # \"More intense wet events\",\n",
    "            \"Wetter seasonal low\",  # \"Less intense dry events\",\n",
    "            \"Wetter seasonal average\",  # \"Decrease in average dryness\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    top9 = [\n",
    "        \"Fagus sylvatica\",\n",
    "        \"Quercus robur\",\n",
    "        \"Quercus petraea\",\n",
    "        \"Carpinus betulus\",\n",
    "        \"Castanea sativa\",\n",
    "        \"Quercus pubescens\",\n",
    "        \"Pinus sylvestris\",\n",
    "        \"Abies alba\",\n",
    "        \"Picea abies\",\n",
    "    ]\n",
    "\n",
    "    labels_circles = [\"40% \\n\\n20%\", \"\"]\n",
    "    radii = [1800, 900]\n",
    "    circles_spacing = -1.45\n",
    "\n",
    "    # ! Get data\n",
    "    if dfchange1 is None or dfchange2 is None:\n",
    "        raise ValueError(\"Both dataframes must be provided.\")\n",
    "\n",
    "    if only_top9:\n",
    "        dfchange1 = dfchange1.query(\"species in @top9\")\n",
    "        dfchange2 = dfchange2.query(\"species in @top9\")\n",
    "        labels_circles = [\"60% \\n\\n30%\", \"\"]\n",
    "        radii = [2500, 1350]\n",
    "        circles_spacing = -1.4\n",
    "\n",
    "    if single_or_double_change == \"single\":\n",
    "        change1 = \"drier\"\n",
    "        change2 = \"wetter\"\n",
    "        group_prefix = \"\"\n",
    "\n",
    "        colors = (\n",
    "            sns.color_palette(\"Reds\", 3)\n",
    "            + np.repeat(np.array(\"white\"), n_spaces).tolist()\n",
    "            + sns.color_palette(\"Blues\", 3)\n",
    "        )\n",
    "\n",
    "    elif single_or_double_change == \"double\":\n",
    "        print(\n",
    "            \"Using default changes: warmer_drier and warmer_wetter. Implementation for cooler_xxx not done yet.\"\n",
    "        )\n",
    "        change1 = \"warmer_drier\"\n",
    "        change2 = \"warmer_wetter\"\n",
    "        group_prefix = \"W\"\n",
    "\n",
    "        colors = (\n",
    "            sns.color_palette(\"Reds\", 3)\n",
    "            + np.repeat(np.array(\"white\"), n_spaces).tolist()\n",
    "            + sns.color_palette(\"Blues\", 3)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for single_or_double_change.\")\n",
    "\n",
    "    df1 = get_pie_heatmap_data(\n",
    "        change=change1,\n",
    "        temp_or_spei=\"spei\",\n",
    "        verbose=False,\n",
    "        df_double_change=dfchange2,\n",
    "        df_single_change=dfchange1,\n",
    "    )\n",
    "\n",
    "    df2 = get_pie_heatmap_data(\n",
    "        change=change2,\n",
    "        temp_or_spei=\"spei\",\n",
    "        verbose=False,\n",
    "        df_double_change=dfchange2,\n",
    "        df_single_change=dfchange1,\n",
    "    )\n",
    "\n",
    "    df1 = df1.replace({\"Autumn\": \"Fall\"})\n",
    "    df2 = df2.replace({\"Autumn\": \"Fall\"})\n",
    "\n",
    "    # Define all possible combinations\n",
    "    spei_durations = [1, 3, 6, 9, 12, 15, 18, 21, 24]\n",
    "    spei_seasons = [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "    # Create a DataFrame with all combinations using cartesian product\n",
    "    combinations = pd.MultiIndex.from_product(\n",
    "        [spei_durations, spei_seasons], names=[\"spei_duration\", \"spei_season\"]\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    # Merge the combinations with your original DataFrame\n",
    "    df1 = pd.merge(combinations, df1, on=[\"spei_duration\", \"spei_season\"], how=\"left\")\n",
    "    df2 = pd.merge(combinations, df2, on=[\"spei_duration\", \"spei_season\"], how=\"left\")\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    df1 = df1.fillna(0)\n",
    "    df2 = df2.fillna(0)\n",
    "\n",
    "    # ! Get percentage models per anomaly\n",
    "    # !for warmer drier\n",
    "    # Count values per anomaly metric\n",
    "    wd_anom_perc_text = (\n",
    "        dfchange2.query(\"change == 'warmer_drier'\")[[\"spei_anom\"]].value_counts(\n",
    "            normalize=True\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    # Turn into integer percentages\n",
    "    for val in wd_anom_perc_text.index:\n",
    "        wd_anom_perc_text[val] = round_to_0_decimals(wd_anom_perc_text[val], 0)\n",
    "    wd_anom_perc_text = wd_anom_perc_text.astype(int)\n",
    "\n",
    "    for v in [\"min\", \"mean\", \"max\"]:\n",
    "        if v not in wd_anom_perc_text.index:\n",
    "            wd_anom_perc_text[v] = 0\n",
    "\n",
    "    # ! for warmer wetter\n",
    "    # Count values per anomaly metric\n",
    "    ww_anom_perc_text = (\n",
    "        dfchange2.query(\"change == 'warmer_wetter'\")[[\"spei_anom\"]].value_counts(\n",
    "            normalize=True\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    # Turn into integer percentages\n",
    "    for val in ww_anom_perc_text.index:\n",
    "        ww_anom_perc_text[val] = round_to_0_decimals(ww_anom_perc_text[val], 0)\n",
    "    ww_anom_perc_text = ww_anom_perc_text.astype(int)\n",
    "\n",
    "    for v in [\"min\", \"mean\", \"max\"]:\n",
    "        if v not in ww_anom_perc_text.index:\n",
    "            ww_anom_perc_text[v] = 0\n",
    "\n",
    "    # ! Get ratio for scaling pie sizes\n",
    "    if scale_by_nmodels:\n",
    "        if single_or_double_change == \"single\":\n",
    "            df_tmp = dfchange1\n",
    "        else:\n",
    "            df_tmp = dfchange2\n",
    "        change_share = df_tmp.groupby(\"change\").agg({\"group_size\": \"sum\"})\n",
    "\n",
    "        ratio1 = change_share.loc[change1, \"group_size\"] / (\n",
    "            change_share.loc[change1, \"group_size\"]\n",
    "            + change_share.loc[change2, \"group_size\"]\n",
    "        )\n",
    "\n",
    "        ratio2 = change_share.loc[change2, \"group_size\"] / (\n",
    "            change_share.loc[change1, \"group_size\"]\n",
    "            + change_share.loc[change2, \"group_size\"]\n",
    "        )\n",
    "\n",
    "        ratio1 = ratio1 / ratio1\n",
    "        ratio2 = ratio2 / ratio1\n",
    "\n",
    "    else:\n",
    "        ratio1 = 1\n",
    "        ratio2 = 1\n",
    "\n",
    "    # ! Plot\n",
    "\n",
    "    # Create the figure and GridSpec layout\n",
    "    rows = df1[x_var].unique()\n",
    "    cols = df1[y_var].unique()\n",
    "\n",
    "    n_rows = df1[x_var].nunique()\n",
    "    n_cols = df1[y_var].nunique()\n",
    "\n",
    "    # Adjusting for horizontal layout (switch rows and columns)\n",
    "    fig = plt.figure(figsize=(n_cols * fig_scale, n_rows * fig_scale))\n",
    "    gs = gridspec.GridSpec(\n",
    "        (n_rows * 2) + 1, n_cols, figure=fig, hspace=-0.675, wspace=-0.8\n",
    "    )\n",
    "\n",
    "    # First 4x8 grid of pie charts\n",
    "    for j in range(n_cols):\n",
    "        col = cols[j]\n",
    "\n",
    "        for i in range((n_rows * 2) + 1):\n",
    "\n",
    "            # Add subplot\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "\n",
    "            if i == n_rows:\n",
    "                # print(f\"skipping: {i}, {j}\")\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            elif i <= n_rows:\n",
    "                # print(f\"plotting: {i}, {j}\")\n",
    "                # Filter data for the current pie\n",
    "                row = rows[i]\n",
    "                data = df1[(df1[x_var] == row) & (df1[y_var] == col)]\n",
    "                data[\"change\"] = \"warmer_drier\"\n",
    "                record_ring_sizes.append(data)\n",
    "                max_size = df1[\"size\"].max()\n",
    "                cmap = \"Reds\"\n",
    "\n",
    "                # Draw pie chart\n",
    "                draw_pie(\n",
    "                    ax,\n",
    "                    data,\n",
    "                    cmap=cmap,\n",
    "                    scale_by100perc=scale_by100perc,\n",
    "                    max_size=max_size,\n",
    "                    min_size=min_pie_size,\n",
    "                    size_scale=size_scale * ratio1,\n",
    "                )\n",
    "\n",
    "                # Set labels for subplots (swap x and y label positioning)\n",
    "                if i == 0:\n",
    "                    ax.set_xlabel(col, fontsize=base_text * 12, labelpad=-90)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(\n",
    "                        row,\n",
    "                        fontsize=base_text * 12,\n",
    "                        labelpad=-20,\n",
    "                        rotation=0,\n",
    "                        ha=\"right\",\n",
    "                        va=\"center\",\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                # print(f\"plotting: {i}, {j}\")\n",
    "                # Filter data for the current pie\n",
    "                row = rows[i - 5]\n",
    "                data = df2[(df2[x_var] == row) & (df2[y_var] == col)]\n",
    "                data[\"change\"] = \"warmer_wetter\"\n",
    "                record_ring_sizes.append(data)\n",
    "                max_size = df2[\"size\"].max()\n",
    "                cmap = \"Blues\"\n",
    "\n",
    "                # Draw pie chart\n",
    "                draw_pie(\n",
    "                    ax,\n",
    "                    data,\n",
    "                    cmap=cmap,\n",
    "                    scale_by100perc=scale_by100perc,\n",
    "                    max_size=max_size,\n",
    "                    min_size=min_pie_size,\n",
    "                    size_scale=size_scale * ratio2,\n",
    "                )\n",
    "\n",
    "                # Set labels for subplots (swap x and y label positioning)\n",
    "                if i == 0:\n",
    "                    ax.set_xlabel(col, fontsize=base_text * 12, labelpad=-120)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(\n",
    "                        row,\n",
    "                        fontsize=base_text * 12,\n",
    "                        labelpad=-20,\n",
    "                        rotation=0,\n",
    "                        ha=\"right\",\n",
    "                        va=\"center\",\n",
    "                    )\n",
    "\n",
    "    # Add y-axis label\n",
    "    fig.text(\n",
    "        0.16,\n",
    "        0.5,\n",
    "        \"Season\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=base_text * 14,\n",
    "        fontweight=\"bold\",\n",
    "        rotation=90,\n",
    "    )\n",
    "\n",
    "    # Add x-axis label\n",
    "    fig.text(\n",
    "        0.5,\n",
    "        0.95,\n",
    "        # \"SPEI timescale (months)\",\n",
    "        # \"Cumulative event period (Backwards in Months)\",\n",
    "        \"Event duration (backwards in months)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=base_text * 14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # ! Add color legend -------------------------------------------------\n",
    "\n",
    "    # Colors and labels for the legend\n",
    "    reds = sns.color_palette(\"Reds\", 3)\n",
    "    blues = sns.color_palette(\"Blues\", 3)\n",
    "\n",
    "    reds_max = \"Drier seasonal peak\"  #    \"Less intense wet events\"\n",
    "    reds_min = \"Drier seasonal low\"  #     \"More intense dry events\"\n",
    "    reds_mean = \"Drier seasonal average\"  # \"Increase in average dryness\"\n",
    "\n",
    "    blues_max = \"Wetter seasonal peak\"  # \"More intense wet events\"\n",
    "    blues_min = \"Wetter seasonal low\"  # \"Less intense dry events\"\n",
    "    blues_mean = \"Wetter seasonal average\"  # \"Decrease in average dryness\"\n",
    "\n",
    "    if add_perc_to_legend:\n",
    "        reds_max = f\"{reds_max} ({wd_anom_perc_text['max']}%)\"\n",
    "        reds_min = f\"{reds_min} ({wd_anom_perc_text['min']}%)\"\n",
    "        reds_mean = f\"{reds_mean} ({wd_anom_perc_text['mean']}%)\"\n",
    "        blues_max = f\"{blues_max} ({ww_anom_perc_text['max']}%)\"\n",
    "        blues_min = f\"{blues_min} ({ww_anom_perc_text['min']}%)\"\n",
    "        blues_mean = f\"{blues_mean} ({ww_anom_perc_text['mean']}%)\"\n",
    "\n",
    "    reds_labels = [reds_max, reds_min, reds_mean]\n",
    "    blues_labels = [blues_max, blues_min, blues_mean]\n",
    "\n",
    "    reds_legend = [\n",
    "        mpatches.Patch(color=color, label=label)\n",
    "        for color, label in zip(reds, reds_labels)\n",
    "    ]\n",
    "    blues_legend = [\n",
    "        mpatches.Patch(color=color, label=label)\n",
    "        for color, label in zip(blues, blues_labels)\n",
    "    ]\n",
    "\n",
    "    # Add first legend (warmer/drier)\n",
    "    legend_x = -2  # -2.6  # -3.25\n",
    "    legend_y = 0\n",
    "    legend_textsize = 12.5\n",
    "\n",
    "    legend1 = ax.legend(\n",
    "        handles=reds_legend,\n",
    "        # loc=\"upper left\",\n",
    "        bbox_to_anchor=(legend_x, legend_y),\n",
    "        fontsize=legend_textsize,\n",
    "        frameon=False,\n",
    "        handlelength=0.7,\n",
    "    )\n",
    "\n",
    "    legend1.set_title(\n",
    "        # \"Mortality-promoting anomaly (model frequency)\",\n",
    "        \"Water availability anomaly (model frequency)\",\n",
    "        prop={\n",
    "            \"weight\": \"bold\",\n",
    "            \"size\": 13,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Position legend\n",
    "    # - \"Mortality-promoting anomaly (model frequency)\": (75, 10) & 2nd legend x offset: 1.5\n",
    "    # - \"Water availability anomaly (model frequency)\": (64, 10)  & 2nd legend x offset: 1.75\n",
    "    legend1.get_title().set_position((64, 10))\n",
    "\n",
    "    # Add second legend (warmer/wetter)\n",
    "    legend2 = ax.legend(\n",
    "        handles=blues_legend,\n",
    "        bbox_to_anchor=(\n",
    "            legend_x + 1.75,\n",
    "            legend_y,\n",
    "        ),  # bbox_to_anchor=(legend_x + 2.25, legend_y),\n",
    "        fontsize=legend_textsize,\n",
    "        frameon=False,\n",
    "        title=\" \",\n",
    "        handlelength=0.7,\n",
    "    )\n",
    "\n",
    "    # Add the legends to the plot\n",
    "    ax.add_artist(legend1)  # Ensure both legends appear\n",
    "\n",
    "    # ! Add circle legend -------------------------------------------------\n",
    "\n",
    "    fig.text(\n",
    "        0.7,\n",
    "        0.085,\n",
    "        \"Model frequency\",\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    def create_circle_legend(ax, radii, labels):\n",
    "        # Create circle artists\n",
    "        handles = []\n",
    "        for i, radius in enumerate(radii):\n",
    "            circle = Line2D(\n",
    "                [0],\n",
    "                [0],\n",
    "                marker=\"o\",\n",
    "                color=\"w\",\n",
    "                markeredgecolor=\"black\",\n",
    "                markersize=np.sqrt(radius),\n",
    "            )  # Adjust size to match scatter size\n",
    "            handles.append(circle)\n",
    "\n",
    "        # Add the legend\n",
    "        fig.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            bbox_to_anchor=(0.8, 0.06),\n",
    "            frameon=False,  # Remove the legend frame\n",
    "            labelspacing=circles_spacing,  # Add spacing between markers and labels\n",
    "            handletextpad=2.0,  # Add padding between marker and text\n",
    "            fontsize=12,\n",
    "        )\n",
    "\n",
    "    # Add the circle legend\n",
    "    create_circle_legend(ax, radii, labels_circles)\n",
    "\n",
    "    # ! Add curly brackets -------------------------------------------------\n",
    "    # Add another axis at the bottom of the chart that spans the full figure\n",
    "    ax_bottom = fig.add_subplot(gs[:, -1])\n",
    "    ax_bottom.set_frame_on(False)\n",
    "    ax_bottom.tick_params(\n",
    "        left=False,\n",
    "        labelleft=False,\n",
    "        right=False,\n",
    "        labelright=False,\n",
    "        bottom=False,\n",
    "        labelbottom=False,\n",
    "    )\n",
    "    ax_bottom.set_xlim(0, 1)\n",
    "    ax_bottom.set_ylim(0, 1)\n",
    "\n",
    "    # Warmer Drier\n",
    "    bracket_width = 0.35\n",
    "    y_wd = 0.55\n",
    "    y_ww = 0.125\n",
    "    x_bracket = 0.6\n",
    "\n",
    "    curlyBrace(\n",
    "        fig,\n",
    "        ax_bottom,\n",
    "        [x_bracket, y_wd + bracket_width],\n",
    "        [x_bracket, y_wd],\n",
    "        # k_r=0.1,\n",
    "        # bool_auto=True,\n",
    "        color=\"black\",\n",
    "        str_text=\"Warmer & drier models\",\n",
    "        int_line_num=2,\n",
    "        fontdict={\n",
    "            \"family\": \"Helvetica\",\n",
    "            \"color\": \"black\",\n",
    "            \"weight\": \"bold\",\n",
    "            \"style\": \"normal\",\n",
    "            \"size\": 12,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Warmer Wetter\n",
    "    curlyBrace(\n",
    "        fig,\n",
    "        ax_bottom,\n",
    "        [x_bracket, y_ww + bracket_width],\n",
    "        [x_bracket, y_ww],\n",
    "        # k_r=0.1,\n",
    "        # bool_auto=True,\n",
    "        color=\"black\",\n",
    "        str_text=\"Warmer & wetter models\",\n",
    "        int_line_num=2,\n",
    "        fontdict={\n",
    "            \"family\": \"Helvetica\",\n",
    "            \"color\": \"black\",\n",
    "            \"weight\": \"bold\",\n",
    "            \"style\": \"normal\",\n",
    "            \"size\": 12,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # ! Add boxes around groups -------------------------------------------------\n",
    "    if add_boxes:\n",
    "\n",
    "        # Group WD1\n",
    "        rect = mpatches.Rectangle(\n",
    "            (0.252, 0.59),  # Lower-left corner coordinates\n",
    "            0.1,  # Width\n",
    "            0.09,  # Height\n",
    "            linewidth=1,  # Border width\n",
    "            linestyle=\"dotted\",\n",
    "            edgecolor=\"black\",  # Border color\n",
    "            facecolor=\"none\",  # No background fill\n",
    "            transform=fig.transFigure,  # Coordinate system\n",
    "        )\n",
    "        fig.patches.append(rect)\n",
    "        fig.text(\n",
    "            0.252,  # x\n",
    "            0.59,  # y\n",
    "            f\"{group_prefix}D1\",\n",
    "            ha=\"left\",\n",
    "            # va=\"center\",\n",
    "            fontsize=base_text * 10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        # Group WD2\n",
    "        rect = mpatches.Rectangle(\n",
    "            (0.37, 0.52),  # Lower-left corner coordinates\n",
    "            0.162,  # Width\n",
    "            0.158,  # Height\n",
    "            linewidth=1,  # Border width\n",
    "            linestyle=\"dotted\",\n",
    "            edgecolor=\"black\",  # Border color\n",
    "            facecolor=\"none\",  # No background fill\n",
    "            transform=fig.transFigure,  # Coordinate system\n",
    "        )\n",
    "        fig.patches.append(rect)\n",
    "        fig.text(\n",
    "            0.37,  # x\n",
    "            0.52,  # y\n",
    "            f\"{group_prefix}D2\",\n",
    "            ha=\"left\",\n",
    "            # va=\"center\",\n",
    "            fontsize=base_text * 10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        # Group WW1\n",
    "        rect = mpatches.Rectangle(\n",
    "            (0.252, 0.31),  # Lower-left corner coordinates\n",
    "            0.162,  # Width\n",
    "            0.093,  # Height\n",
    "            linewidth=1,  # Border width\n",
    "            linestyle=\"dotted\",\n",
    "            edgecolor=\"black\",  # Border color\n",
    "            facecolor=\"none\",  # No background fill\n",
    "            transform=fig.transFigure,  # Coordinate system\n",
    "        )\n",
    "        fig.patches.append(rect)\n",
    "        fig.text(\n",
    "            0.252,  # x\n",
    "            0.31,  # y\n",
    "            f\"{group_prefix}W1\",\n",
    "            ha=\"left\",\n",
    "            # va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        tmp = pd.concat(record_ring_sizes).sort_values([\"change\", \"size\"])\n",
    "        tmp[\"value\"] = tmp[\"value\"].round(0)\n",
    "        display(tmp.value.sum())\n",
    "        display(tmp[tmp[\"change\"] == \"warmer_drier\"].nlargest(3, \"value\"))\n",
    "        display(tmp[tmp[\"change\"] == \"warmer_wetter\"].nlargest(3, \"value\"))\n",
    "\n",
    "    if dir_pie is None:\n",
    "        print(\"No directory provided. Not saving the figure!\")\n",
    "    else:\n",
    "        os.makedirs(dir_pie, exist_ok=True)\n",
    "        plt.savefig(\n",
    "            f\"{dir_pie}/pie-spei-{single_or_double_change}_change-top9_{only_top9}-scaled_by_100perc_{scale_by100perc}-scale_by_nmodels_{scale_by_nmodels}-add_boxes_{add_boxes}.png\",\n",
    "            dpi=600,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! --------------------------------------------------------------------------------------------\n",
    "def make_pie_chart_temp_v3(\n",
    "    single_or_double_change=None,\n",
    "    dfchange1=None,\n",
    "    dfchange2=None,\n",
    "    only_top9=False,\n",
    "    size_scale=1,\n",
    "    fig_scale=1,\n",
    "    scale_by_nmodels=False,\n",
    "    scale_by100perc=False,\n",
    "    min_pie_size=0,\n",
    "    add_boxes=False,\n",
    "    dir_pie=None,\n",
    "    merge_legend=False,\n",
    "    method_scaling_importance=\"scaled_valid\",\n",
    "    add_perc_to_legend=False,\n",
    "    verbose=False,\n",
    "):\n",
    "    # ! Locals\n",
    "    record_ring_sizes = []\n",
    "\n",
    "    # ! Parameters\n",
    "    x_var = \"temp_season\"\n",
    "    y_var = \"temp_metric\"\n",
    "    dict_temp = {\n",
    "        \"max\": \"Max.\",\n",
    "        \"min\": \"Min.\",\n",
    "        \"mean\": \"Mean\",\n",
    "    }\n",
    "    n_spaces = 1\n",
    "\n",
    "    legend_title_single = \"Warmer/cooler conditions promoting mortality\"  # \"Meaning of warmer/cooler conditions\"\n",
    "    legend_title_double = (\n",
    "        # \"Warmer conditions promoting mortality\"  # \"Meaning of warmer conditions\"\n",
    "        # \"Mortality-promoting anomaly (model frequency)\"\n",
    "        \"Temperature anomaly (model frequency)\"\n",
    "    )\n",
    "\n",
    "    legend_labels_warmer = [\n",
    "        \"Warmer seasonal peak\",  # \"More intense hot event\",\n",
    "        \"Milder seasonal low\",  # \"Less intense cold event\",\n",
    "        \"Warmer seasonal average\",  # \"Increase in average temperature\",\n",
    "    ]\n",
    "\n",
    "    legend_labels_cooler = [\n",
    "        \"Milder seasonal peak\",  # \"Less intense hot event\",\n",
    "        \"Cooler seasonal low\",  # \"More intense cold event\",\n",
    "        \"Cooler seasonal average\",  # \"Decrease in average temperature\",\n",
    "    ]\n",
    "\n",
    "    top9 = [\n",
    "        \"Fagus sylvatica\",\n",
    "        \"Quercus robur\",\n",
    "        \"Quercus petraea\",\n",
    "        \"Carpinus betulus\",\n",
    "        \"Castanea sativa\",\n",
    "        \"Quercus pubescens\",\n",
    "        \"Pinus sylvestris\",\n",
    "        \"Abies alba\",\n",
    "        \"Picea abies\",\n",
    "    ]\n",
    "\n",
    "    # Labels for circles in the legend (if not top9, else its overwritten below)\n",
    "    labels_circles = [\"20% \\n\\n10%\", \"\"]\n",
    "    radii = [1800, 900]\n",
    "\n",
    "    # ! Get data\n",
    "    if dfchange1 is None or dfchange2 is None:\n",
    "        raise ValueError(\"Both dataframes must be provided.\")\n",
    "\n",
    "    if only_top9:\n",
    "        dfchange1 = dfchange1.query(\"species in @top9\")\n",
    "        dfchange2 = dfchange2.query(\"species in @top9\")\n",
    "        labels_circles = [\"30% \\n\\n15%\", \"\"]\n",
    "        radii = [2500, 1350]\n",
    "\n",
    "    if single_or_double_change == \"single\":\n",
    "        change1 = \"warmer\"\n",
    "        change2 = \"cooler\"\n",
    "\n",
    "        merge_legend = False\n",
    "        legend_title = legend_title_single\n",
    "\n",
    "        colors = (\n",
    "            sns.color_palette(\"Reds\", 3)\n",
    "            + np.repeat(np.array(\"white\"), n_spaces).tolist()\n",
    "            + sns.color_palette(\"Blues\", 3)\n",
    "        )\n",
    "\n",
    "        legend_labels = (\n",
    "            legend_labels_warmer\n",
    "            + np.repeat(np.array(\"\"), n_spaces).tolist()\n",
    "            + legend_labels_cooler\n",
    "        )\n",
    "\n",
    "        legend_x = 0.78\n",
    "        legend_y = 0.69\n",
    "\n",
    "        group_prefix = \"\"\n",
    "\n",
    "    elif single_or_double_change == \"double\":\n",
    "        print(\"Using default changes: warmer_drier and warmer_wetter\")\n",
    "        change1 = \"warmer_drier\"\n",
    "        change2 = \"warmer_wetter\"\n",
    "        group_prefix = \"W\"\n",
    "\n",
    "        legend_labels_cooler = legend_labels_warmer\n",
    "        legend_title = legend_title_double\n",
    "\n",
    "        colors = (\n",
    "            [sns.color_palette(\"Reds\", 3)[-1]]\n",
    "            + [sns.color_palette(\"Blues\", 3)[-1]]\n",
    "            + np.repeat(np.array(\"white\"), n_spaces).tolist()\n",
    "            + sns.color_palette(\"Greys\", 3)\n",
    "        )\n",
    "\n",
    "        legend_labels = (\n",
    "            [\"Warmer & drier models\", \"Warmer & wetter models\"]\n",
    "            + np.repeat(np.array(\"\"), n_spaces).tolist()\n",
    "            + legend_labels_warmer\n",
    "        )\n",
    "\n",
    "        legend_x = 0.82  # Original: 0.79\n",
    "        legend_y = 0.66  # Seems to be overwritten below...\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for single_or_double_change.\")\n",
    "\n",
    "    df1 = get_pie_heatmap_data(\n",
    "        change=change1,\n",
    "        temp_or_spei=\"temp\",\n",
    "        tempy=\"metric\",\n",
    "        verbose=False,\n",
    "        df_double_change=dfchange2,\n",
    "        df_single_change=dfchange1,\n",
    "        method_scaling_importance=method_scaling_importance,\n",
    "    )\n",
    "\n",
    "    df2 = get_pie_heatmap_data(\n",
    "        change=change2,\n",
    "        temp_or_spei=\"temp\",\n",
    "        tempy=\"metric\",\n",
    "        verbose=False,\n",
    "        df_double_change=dfchange2,\n",
    "        df_single_change=dfchange1,\n",
    "        method_scaling_importance=method_scaling_importance,\n",
    "    )\n",
    "\n",
    "    df1 = df1.replace({\"Autumn\": \"Fall\"})\n",
    "    df2 = df2.replace({\"Autumn\": \"Fall\"})\n",
    "\n",
    "    # Define all possible combinations\n",
    "    temp_metric = [\"max\", \"min\", \"mean\"]\n",
    "    temp_season = [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "    # Create a DataFrame with all combinations using cartesian product\n",
    "    combinations = pd.MultiIndex.from_product(\n",
    "        [temp_metric, temp_season], names=[\"temp_metric\", \"temp_season\"]\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    # Merge the combinations with your original DataFrame\n",
    "    df1 = pd.merge(combinations, df1, on=[\"temp_metric\", \"temp_season\"], how=\"left\")\n",
    "    df2 = pd.merge(combinations, df2, on=[\"temp_metric\", \"temp_season\"], how=\"left\")\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    df1 = df1.fillna(0)\n",
    "    df2 = df2.fillna(0)\n",
    "\n",
    "    # ! Get percentage models per anomaly\n",
    "    # ! for warmer drier\n",
    "    # Count values per anomaly metric\n",
    "    wd_anom_perc_text = (\n",
    "        dfchange2.query(\"change == 'warmer_drier'\")[[\"temp_anom\"]].value_counts(\n",
    "            normalize=True\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    # Round percentages to 0 decimals\n",
    "    for val in wd_anom_perc_text.index:\n",
    "        wd_anom_perc_text[val] = round_to_0_decimals(wd_anom_perc_text[val], 0)\n",
    "\n",
    "    # If rounding failed and does not sum up to 100:\n",
    "    if wd_anom_perc_text.values.sum() > 100:\n",
    "        # reduce lowest percentage (as that is likely rounded up falsely)\n",
    "        min_val = wd_anom_perc_text.min()\n",
    "        wd_anom_perc_text[wd_anom_perc_text == min_val] -= 1\n",
    "    elif wd_anom_perc_text.values.sum() < 100:\n",
    "        # increase highest percentage (as that is likely rounded down falsely)\n",
    "        max_val = wd_anom_perc_text.max()\n",
    "        wd_anom_perc_text[wd_anom_perc_text == max_val] += 1\n",
    "\n",
    "    # Turn into integer percentages\n",
    "    wd_anom_perc_text = wd_anom_perc_text.astype(int)\n",
    "\n",
    "    for v in [\"min\", \"mean\", \"max\"]:\n",
    "        if v not in wd_anom_perc_text.index:\n",
    "            wd_anom_perc_text[v] = 0\n",
    "\n",
    "    # ! for warmer wetter\n",
    "    # Count values per anomaly metric\n",
    "    ww_anom_perc_text = (\n",
    "        dfchange2.query(\"change == 'warmer_wetter'\")[[\"temp_anom\"]].value_counts(\n",
    "            normalize=True\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    # Round percentages to 0 decimals\n",
    "    for val in ww_anom_perc_text.index:\n",
    "        ww_anom_perc_text[val] = round_to_0_decimals(ww_anom_perc_text[val], 0)\n",
    "\n",
    "    # If rounding failed and does not sum up to 100, reduce lowest percentage (as that is likely rounded up falsely)\n",
    "    if ww_anom_perc_text.values.sum() != 100:\n",
    "        min_val = ww_anom_perc_text.min()\n",
    "        ww_anom_perc_text[ww_anom_perc_text == min_val] -= 1\n",
    "\n",
    "    # Turn into integer percentages\n",
    "    ww_anom_perc_text = ww_anom_perc_text.astype(int)\n",
    "\n",
    "    for v in [\"min\", \"mean\", \"max\"]:\n",
    "        if v not in ww_anom_perc_text.index:\n",
    "            ww_anom_perc_text[v] = 0\n",
    "\n",
    "    # ! for ALL models\n",
    "    # Count values per anomaly metric\n",
    "    all_anom_perc_text = dfchange2[[\"temp_anom\"]].value_counts(normalize=True) * 100\n",
    "\n",
    "    # Turn into integer percentages\n",
    "    for val in all_anom_perc_text.index:\n",
    "        all_anom_perc_text[val] = round_to_0_decimals(all_anom_perc_text[val], 0)\n",
    "    all_anom_perc_text = all_anom_perc_text.astype(int)\n",
    "\n",
    "    for v in [\"min\", \"mean\", \"max\"]:\n",
    "        if v not in all_anom_perc_text.index:\n",
    "            all_anom_perc_text[v] = 0\n",
    "\n",
    "    # ! Get ratio for scaling pie sizes\n",
    "    if scale_by_nmodels:\n",
    "        if single_or_double_change == \"single\":\n",
    "            df_tmp = dfchange1\n",
    "        else:\n",
    "            df_tmp = dfchange2\n",
    "        change_share = df_tmp.groupby(\"change\").agg({\"group_size\": \"sum\"})\n",
    "\n",
    "        ratio1 = change_share.loc[change1, \"group_size\"] / (\n",
    "            change_share.loc[change1, \"group_size\"]\n",
    "            + change_share.loc[change2, \"group_size\"]\n",
    "        )\n",
    "\n",
    "        ratio2 = change_share.loc[change2, \"group_size\"] / (\n",
    "            change_share.loc[change1, \"group_size\"]\n",
    "            + change_share.loc[change2, \"group_size\"]\n",
    "        )\n",
    "\n",
    "        ratio1 = ratio1 / ratio1\n",
    "        ratio2 = ratio2 / ratio1\n",
    "\n",
    "    else:\n",
    "        ratio1 = 1\n",
    "        ratio2 = 1\n",
    "\n",
    "    # ! Plot\n",
    "    # Create the figure and GridSpec layout\n",
    "    rows = df1[x_var].unique()\n",
    "    cols = df1[y_var].unique()\n",
    "\n",
    "    n_rows = df1[x_var].nunique()\n",
    "    n_cols = df1[y_var].nunique()\n",
    "\n",
    "    # Adjusting for horizontal layout (switch rows and columns)\n",
    "    fig = plt.figure(figsize=(n_cols * fig_scale * 2.5, n_rows * fig_scale))\n",
    "    gs = gridspec.GridSpec(\n",
    "        n_rows, (n_cols * 2) + 1, figure=fig, hspace=-0.675, wspace=-0.8\n",
    "    )\n",
    "\n",
    "    # ! First 4x8 grid of pie charts for warmer_drier\n",
    "    for i in range(n_rows):\n",
    "        row = rows[i]\n",
    "        for j in range((n_cols * 2) + 1):\n",
    "\n",
    "            # Add subplot\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "\n",
    "            if j == n_cols:\n",
    "                # print(f\"skipping: {i}, {j}\")\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            elif j <= n_cols:\n",
    "                # print(f\"plotting: {i}, {j}\")\n",
    "                # Filter data for the current pie\n",
    "                col = cols[j]\n",
    "                data = df1[(df1[x_var] == row) & (df1[y_var] == col)]\n",
    "                data[\"change\"] = \"warmer_drier\"\n",
    "                record_ring_sizes.append(data)\n",
    "                max_size = df1[\"size\"].max()\n",
    "                cmap = \"Reds\"\n",
    "\n",
    "                # Draw pie chart\n",
    "                draw_pie(\n",
    "                    ax,\n",
    "                    data,\n",
    "                    cmap=cmap,\n",
    "                    scale_by100perc=scale_by100perc,\n",
    "                    max_size=max_size,\n",
    "                    min_size=min_pie_size,\n",
    "                    size_scale=size_scale * ratio1,\n",
    "                )\n",
    "\n",
    "                # Set labels for subplots (swap x and y label positioning)\n",
    "                if i == 0:\n",
    "                    ax.set_xlabel(dict_temp[col], fontsize=12, labelpad=-130)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(\n",
    "                        row,\n",
    "                        fontsize=12,\n",
    "                        labelpad=-50,\n",
    "                        rotation=0,\n",
    "                        ha=\"right\",\n",
    "                        va=\"center\",\n",
    "                    )\n",
    "\n",
    "            # ! Second 4x8 grid of pie charts for warmer_wetter\n",
    "            else:\n",
    "                # print(f\"plotting: {i}, {j}\")\n",
    "                # Filter data for the current pie\n",
    "                col = cols[j - 4]\n",
    "                data = df2[(df2[x_var] == row) & (df2[y_var] == col)]\n",
    "                data[\"change\"] = \"warmer_wetter\"\n",
    "                record_ring_sizes.append(data)\n",
    "                max_size = df2[\"size\"].max()\n",
    "                cmap = \"Blues\"\n",
    "\n",
    "                # Draw pie chart\n",
    "                draw_pie(\n",
    "                    ax,\n",
    "                    data,\n",
    "                    cmap=cmap,\n",
    "                    scale_by100perc=scale_by100perc,\n",
    "                    max_size=max_size,\n",
    "                    min_size=min_pie_size,\n",
    "                    size_scale=size_scale * ratio2,\n",
    "                )\n",
    "\n",
    "                # Set labels for subplots (swap x and y label positioning)\n",
    "                if i == 0:\n",
    "                    ax.set_xlabel(dict_temp[col], fontsize=12, labelpad=-130)\n",
    "\n",
    "    # Add y-axis label\n",
    "    fig.text(\n",
    "        0.16,\n",
    "        0.5,\n",
    "        \"Season\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        rotation=90,\n",
    "    )\n",
    "\n",
    "    # Add x-axis label\n",
    "    fig.text(\n",
    "        0.525,\n",
    "        0.85,\n",
    "        \"Temperature metric\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # Add legend\n",
    "    if merge_legend:\n",
    "\n",
    "        # Todo: should be defined on top\n",
    "        legend_handles_x = 2.25\n",
    "        legend_handles_y = 1.5\n",
    "\n",
    "        txt_max = legend_labels[-3]\n",
    "        txt_min = legend_labels[-2]\n",
    "        txt_mean = legend_labels[-1]\n",
    "\n",
    "        if add_perc_to_legend:\n",
    "\n",
    "            # > Version for ww and wd models separately:\n",
    "            legend_handles_x = 2.475  # 2.7\n",
    "            txt_max = (\n",
    "                f\"{txt_max} ({wd_anom_perc_text['max']}% / {ww_anom_perc_text['max']}%)\"\n",
    "            )\n",
    "            txt_min = (\n",
    "                f\"{txt_min} ({wd_anom_perc_text['min']}% / {ww_anom_perc_text['min']}%)\"\n",
    "            )\n",
    "            txt_mean = f\"{txt_mean} ({wd_anom_perc_text['mean']}% / {ww_anom_perc_text['mean']}%)\"\n",
    "\n",
    "            # > Version for all ww and wd models:\n",
    "            # legend_handles_x = 2.5\n",
    "            # txt_max = f\"{txt_max} ({all_anom_perc_text['max']}%)\"\n",
    "            # txt_min = f\"{txt_min} ({all_anom_perc_text['min']}%)\"\n",
    "            # txt_mean = f\"{txt_mean} ({all_anom_perc_text['mean']}%)\"\n",
    "\n",
    "        legend_patches = [\n",
    "            Patch(facecolor=sns.color_palette(\"Reds\", 3)[0], label=\"\"),\n",
    "            Patch(facecolor=sns.color_palette(\"Reds\", 3)[1], label=\"\"),\n",
    "            Patch(facecolor=sns.color_palette(\"Reds\", 3)[2], label=\"\"),\n",
    "            Patch(facecolor=sns.color_palette(\"Blues\", 3)[0], label=txt_max),\n",
    "            Patch(facecolor=sns.color_palette(\"Blues\", 3)[1], label=txt_min),\n",
    "            Patch(facecolor=sns.color_palette(\"Blues\", 3)[2], label=txt_mean),\n",
    "        ]\n",
    "\n",
    "        # ww_anom_perc_text\n",
    "        # display(legend_patches)\n",
    "        # raise\n",
    "\n",
    "        plt.legend(\n",
    "            handles=legend_patches,\n",
    "            bbox_to_anchor=(\n",
    "                legend_handles_x,\n",
    "                legend_handles_y,\n",
    "            ),  # Original with loc = \"center\": (1.5, 1),\n",
    "            # loc=\"center\",  # Align in the center of the left side (y-axis)\n",
    "            fontsize=12,\n",
    "            frameon=False,\n",
    "            # title=\"Meaning of warmer conditions\",\n",
    "            title=\"\",\n",
    "            ncol=2,  # Ensure single column layout\n",
    "            columnspacing=-0.8,\n",
    "            handlelength=1,\n",
    "        )\n",
    "        legend_y = 0.7  # Original 0.6\n",
    "    else:\n",
    "        legend_patches = [\n",
    "            mpatches.Patch(color=color, label=label)\n",
    "            for color, label in zip(colors, legend_labels)\n",
    "        ]\n",
    "\n",
    "        # Add the combined legend to the plot\n",
    "        plt.legend(\n",
    "            handles=legend_patches,\n",
    "            bbox_to_anchor=(1.5, 1),\n",
    "            loc=\"center\",  # Align in the center of the left side (y-axis)\n",
    "            fontsize=12,\n",
    "            frameon=False,\n",
    "            # title=\"Meaning of warmer conditions\",\n",
    "            title=\"\",\n",
    "            ncol=1,  # Ensure single column layout\n",
    "        )\n",
    "        # plt.setp(plt.gca().get_legend().get_title(), fontweight=\"bold\", fontsize=12, ha=\"left\")\n",
    "\n",
    "    # Add text to describe the legend\n",
    "    fig.text(\n",
    "        legend_x,\n",
    "        legend_y,\n",
    "        legend_title,\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # ! Add circle legend -------------------------------------------------\n",
    "    fig.text(\n",
    "        legend_x,\n",
    "        legend_y - 0.22,\n",
    "        \"Model frequency\",\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    def create_circle_legend(ax, radii, labels):\n",
    "        # Create circle artists\n",
    "        handles = []\n",
    "        for i, radius in enumerate(radii):\n",
    "            circle = Line2D(\n",
    "                [0],\n",
    "                [0],\n",
    "                marker=\"o\",\n",
    "                color=\"w\",\n",
    "                markeredgecolor=\"black\",\n",
    "                markersize=np.sqrt(radius),\n",
    "            )  # Adjust size to match scatter size\n",
    "            handles.append(circle)\n",
    "\n",
    "        # Add the legend\n",
    "        fig.legend(\n",
    "            handles,\n",
    "            labels,\n",
    "            bbox_to_anchor=(0.935, 0.45),  # Move the legend below the plot\n",
    "            # loc=\"upper center\",  # Position above the plot\n",
    "            frameon=False,  # Remove the legend frame\n",
    "            # ncol=len(labels),  # Arrange legend items in a single row\n",
    "            labelspacing=-1.45,  # Add spacing between markers and labels\n",
    "            handletextpad=2.0,  # Add padding between marker and text\n",
    "            # columnspacing=2.5,  # Increase spacing between columns\n",
    "        )\n",
    "\n",
    "    # Add the circle legend\n",
    "    create_circle_legend(ax, radii, labels_circles)\n",
    "\n",
    "    # ! Add curly brackets -------------------------------------------------\n",
    "    # Add another axis at the bottom of the chart that spans the full figure\n",
    "    ax_bottom = fig.add_subplot(gs[-1, :])\n",
    "    ax_bottom.set_frame_on(False)\n",
    "    ax_bottom.tick_params(\n",
    "        left=False,\n",
    "        labelleft=False,\n",
    "        right=False,\n",
    "        labelright=False,\n",
    "        bottom=False,\n",
    "        labelbottom=False,\n",
    "    )\n",
    "    ax_bottom.set_xlim(0, 1)\n",
    "    ax_bottom.set_ylim(0, 1)\n",
    "\n",
    "    # Warmer Drier\n",
    "    bracket_width = 0.25\n",
    "    x_wd = 0.2\n",
    "    x_ww = 0.57\n",
    "\n",
    "    curlyBrace(\n",
    "        fig,\n",
    "        ax_bottom,\n",
    "        [x_wd + bracket_width, 0.4],\n",
    "        [x_wd, 0.4],\n",
    "        # k_r=0.1,\n",
    "        # bool_auto=True,\n",
    "        color=\"black\",\n",
    "        str_text=\"Warmer & drier models\",\n",
    "        int_line_num=2,\n",
    "        fontdict={\n",
    "            \"family\": \"Helvetica\",\n",
    "            \"color\": \"black\",\n",
    "            \"weight\": \"bold\",\n",
    "            \"style\": \"normal\",\n",
    "            \"size\": 12,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Warmer Wetter\n",
    "    curlyBrace(\n",
    "        fig,\n",
    "        ax_bottom,\n",
    "        [x_ww + bracket_width, 0.4],\n",
    "        [x_ww, 0.4],\n",
    "        # k_r=0.1,\n",
    "        # bool_auto=True,\n",
    "        color=\"black\",\n",
    "        str_text=\"Warmer & wetter models\",\n",
    "        int_line_num=2,\n",
    "        fontdict={\n",
    "            \"family\": \"Helvetica\",\n",
    "            \"color\": \"black\",\n",
    "            \"weight\": \"bold\",\n",
    "            \"style\": \"normal\",\n",
    "            \"size\": 12,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # ! Add boxes around groups\n",
    "    if add_boxes:\n",
    "\n",
    "        # Group W1\n",
    "        rect = mpatches.Rectangle(\n",
    "            (0.27, 0.625),  # Lower-left corner coordinates\n",
    "            0.21,  # Width\n",
    "            0.119,  # Height\n",
    "            linewidth=1,  # Border width\n",
    "            linestyle=\"dotted\",\n",
    "            edgecolor=\"black\",  # Border color\n",
    "            facecolor=\"none\",  # No background fill\n",
    "            transform=fig.transFigure,  # Coordinate system\n",
    "        )\n",
    "        fig.patches.append(rect)\n",
    "        fig.text(\n",
    "            0.27,  # x\n",
    "            0.625,  # y\n",
    "            f\"WD1\",\n",
    "            ha=\"left\",\n",
    "            # va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        # Group W2\n",
    "        rect = mpatches.Rectangle(\n",
    "            (0.27, 0.5),  # Lower-left corner coordinates\n",
    "            0.21,  # Width\n",
    "            0.119,  # Height\n",
    "            linewidth=1,  # Border width\n",
    "            linestyle=\"dotted\",\n",
    "            edgecolor=\"black\",  # Border color\n",
    "            facecolor=\"none\",  # No background fill\n",
    "            transform=fig.transFigure,  # Coordinate system\n",
    "        )\n",
    "        fig.patches.append(rect)\n",
    "        fig.text(\n",
    "            0.27,  # x\n",
    "            0.5,  # y\n",
    "            f\"WD2\",\n",
    "            ha=\"left\",\n",
    "            # va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "        # Group W3\n",
    "        rect = mpatches.Rectangle(\n",
    "            (0.555, 0.625),  # Lower-left corner coordinates\n",
    "            0.21,  # Width\n",
    "            0.119,  # Height\n",
    "            linewidth=1,  # Border width\n",
    "            linestyle=\"dotted\",\n",
    "            edgecolor=\"black\",  # Border color\n",
    "            facecolor=\"none\",  # No background fill\n",
    "            transform=fig.transFigure,  # Coordinate system\n",
    "        )\n",
    "        fig.patches.append(rect)\n",
    "        fig.text(\n",
    "            0.555,  # x\n",
    "            0.625,  # y\n",
    "            f\"WW1\",\n",
    "            ha=\"left\",\n",
    "            # va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        # Group W4\n",
    "        rect = mpatches.Rectangle(\n",
    "            (0.555, 0.375),  # Lower-left corner coordinates\n",
    "            0.21,  # Width\n",
    "            0.119,  # Height\n",
    "            linewidth=1,  # Border width\n",
    "            linestyle=\"dotted\",\n",
    "            edgecolor=\"black\",  # Border color\n",
    "            facecolor=\"none\",  # No background fill\n",
    "            transform=fig.transFigure,  # Coordinate system\n",
    "        )\n",
    "        fig.patches.append(rect)\n",
    "        fig.text(\n",
    "            0.555,  # x\n",
    "            0.375,  # y\n",
    "            f\"WW2\",\n",
    "            ha=\"left\",\n",
    "            # va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        display(\"--- Ring Sizes\")\n",
    "        tmp = pd.concat(record_ring_sizes).sort_values(\n",
    "            [\"change\", \"temp_season\", \"value\"]\n",
    "        )[[\"change\", \"temp_season\", \"temp_metric\", \"value\", \"max\", \"min\", \"mean\"]]\n",
    "        tmp[\"value\"] = tmp[\"value\"].round(0)\n",
    "        display(tmp.value.sum())\n",
    "        display(tmp)\n",
    "        # display(tmp[tmp[\"change\"] == \"warmer_drier\"].nlargest(3, \"value\"))\n",
    "        # display(tmp[tmp[\"change\"] == \"warmer_wetter\"].nlargest(3, \"value\"))\n",
    "\n",
    "    if dir_pie is None:\n",
    "        print(\"No directory provided. Not saving the figure!\")\n",
    "    else:\n",
    "        # pass\n",
    "        os.makedirs(dir_pie, exist_ok=True)\n",
    "        plt.savefig(\n",
    "            f\"{dir_pie}/pie-temp-{single_or_double_change}_change-top9_{only_top9}-scaled_by_100perc_{scale_by100perc}-scale_by_nmodels_{scale_by_nmodels}-add_boxes_{add_boxes}.png\",\n",
    "            dpi=600,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! --- Analysis Settings ---\n",
    "# Define which runs to analyze\n",
    "which_analysis = \"default\"  # default, smotek_n, rfe_nkeep_n\n",
    "\n",
    "# For extra analyses, define species to analyze:\n",
    "# species_extra_run = [\"Abies alba\"]\n",
    "species_extra_run = top9  # defined above in setup cells\n",
    "# species_extra_run = species_in_final_anlysis # defined above in setup cells\n",
    "\n",
    "# Directories\n",
    "available_dirs = [\"./model_runs\"] + sorted(glob.glob(\"./extra_runs/*\"))\n",
    "print(\"Available runs directories:\")\n",
    "for i, d in enumerate(available_dirs):\n",
    "    print(f\"{i + 1}: {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Additional Settings (fixed)\n",
    "impurity_or_permutation = \"impurity\"\n",
    "sort_runs_by = \"roc\"  # roc, tss, specificity\n",
    "roc_threshold = 0.6\n",
    "\n",
    "# Extract performance and plot per species and run\n",
    "force_aggregating_runs = True\n",
    "do_tss_calculation = False\n",
    "do_plot_feature_importance = False\n",
    "do_plot_model_performances = False\n",
    "\n",
    "# Group run by features\n",
    "scale_response = True\n",
    "scale_shap = True\n",
    "make_plot = False\n",
    "\n",
    "do_plot_interactions = False\n",
    "sort_plots_by = \"run\"  # performance - run\n",
    "single_response = True\n",
    "interaction_1d = False\n",
    "interaction_2d = False\n",
    "\n",
    "# ! Check that extra runs are either for one, top9 or all species\n",
    "if which_analysis != \"default\":\n",
    "    if len(species_extra_run) not in [1, 9, 52]:\n",
    "        raise ValueError(\n",
    "            \"For extra runs, species_extra_run must be either a single species, the top 9 species, or all species.\"\n",
    "        )\n",
    "    if len(species_extra_run) == 1:\n",
    "        extra_nspecies = species_extra_run[0]\n",
    "    elif len(species_extra_run) == 9:\n",
    "        extra_nspecies = \"_top9_species\"\n",
    "    elif len(species_extra_run) == 52:\n",
    "        extra_nspecies = \"_all_species\"\n",
    "\n",
    "    print(f\"Doing extra analysis for {extra_nspecies} species.\")\n",
    "\n",
    "# ! Select directory for runs\n",
    "if which_analysis == \"default\":\n",
    "    dir_runs = \"./model_runs/all_runs\"\n",
    "    dir_save = f\"./model_analysis/{datetime.datetime.now().strftime('%Y-%m-%d')}/\"\n",
    "else:\n",
    "    dir_runs = f\"./extra_runs/{which_analysis}\"\n",
    "    dir_save = f\"./extra_runs/{which_analysis}/_analysis/{extra_nspecies}/{datetime.datetime.now().strftime('%Y-%m-%d')}/\"\n",
    "\n",
    "if scale_response and scale_shap:\n",
    "    dir_save += \"scale_response_and_shape/\"\n",
    "elif scale_response:\n",
    "    dir_save += \"scale_response/\"\n",
    "elif scale_shap:\n",
    "    dir_save += \"scaled_SHAP/\"\n",
    "else:\n",
    "    dir_save += \"nothing_scaled/\"\n",
    "\n",
    "# Check if the directory exists, if not error message\n",
    "if not os.path.exists(dir_runs + \"/Fagus sylvatica\"):\n",
    "    if which_analysis == \"default\":\n",
    "        # For default analysis, check if SD directory exists\n",
    "        dir_runs = \"/Volumes/SAMSUNG 1TB/all_runs/\"\n",
    "        if not os.path.exists(dir_runs):\n",
    "            raise ValueError(\n",
    "                f\"Directory {dir_runs} does not exist. Please check the path.\"\n",
    "            )\n",
    "    else:\n",
    "        dir_runs = \"/Volumes/SAMSUNG 1TB/extra_runs/\" + which_analysis\n",
    "        if not os.path.exists(dir_runs):\n",
    "            raise ValueError(\n",
    "                f\"Directory {dir_runs} does not exist. Please check the path.\"\n",
    "            )\n",
    "\n",
    "# ! Define directory for saving results\n",
    "dir_save = dir_save + f\"/per_species/roc_{roc_threshold}/\"\n",
    "print(f\"Loading runs from: {dir_runs}\")\n",
    "print(f\"Saving directory: {dir_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all runs per species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs\n",
    "all_runs = os.listdir(dir_runs)\n",
    "# Remove some folders\n",
    "all_runs = [run for run in all_runs if not run.startswith(\".\")]\n",
    "all_runs = [run for run in all_runs if not run.endswith(\".txt\")]\n",
    "all_runs = [run for run in all_runs if run != \"_archive\"]\n",
    "all_runs = sorted(all_runs)\n",
    "# all_runs\n",
    "\n",
    "# Get all species\n",
    "all_species = nfi_raw[\"species_lat2\"].unique().tolist()\n",
    "all_species = sorted(all_species)\n",
    "\n",
    "if which_analysis != \"default\":\n",
    "    all_species = species_extra_run\n",
    "\n",
    "all_species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract performance and plot per single run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! This cell aggregates all runs for each species and plots all runs into a single figure\n",
    "\n",
    "df_all = []\n",
    "counter = 0\n",
    "\n",
    "for i_species in all_species:\n",
    "\n",
    "    # Progress\n",
    "    counter += 1\n",
    "    print(f\"{counter}/{len(all_species)}\\t{i_species}\")\n",
    "\n",
    "    # ! Aggregate all runs -----------------------------------------------------\n",
    "    metric_names = [\n",
    "        \"test_boot_mean\",\n",
    "        \"test_boot_sd\",\n",
    "        \"train_boot_mean\",\n",
    "        \"train_boot_sd\",\n",
    "    ]\n",
    "\n",
    "    # Check if all_runs.csv already exists\n",
    "    i_file = f\"{dir_save}/{i_species}/all_runs.csv\"\n",
    "    if not os.path.exists(i_file) or force_aggregating_runs:\n",
    "        # if True:\n",
    "        runs_subset = []\n",
    "        # Loop through all runs to get all runs according to the user input\n",
    "        for i_run in all_runs:\n",
    "            # Get directory\n",
    "            i_dir = os.path.join(dir_runs, i_run, i_species)\n",
    "\n",
    "            # Check if user input file exists\n",
    "            txt_ui = f\"{i_dir}/__user_input.txt\"\n",
    "            first_file = glob.glob(txt_ui)\n",
    "\n",
    "            # If no user input file, then skip\n",
    "            if len(first_file) == 0:\n",
    "                # raise Exception(f\"❌ No user input file in {i_dir}\")\n",
    "                continue\n",
    "\n",
    "            # Check if run was impurity or permutation\n",
    "            first_file = first_file[0]\n",
    "            with open(first_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    if \" - impurity\" in line:\n",
    "                        model_run = \"impurity\"\n",
    "                    elif \" - permutation\" in line:\n",
    "                        model_run = \"permutation\"\n",
    "\n",
    "            # Skip if not the same as user input\n",
    "            if model_run != impurity_or_permutation:\n",
    "                continue\n",
    "\n",
    "            # Check if final model performance exists\n",
    "            txt_ui = f\"{i_dir}/final_model_performance.csv\"\n",
    "            first_file = glob.glob(txt_ui)\n",
    "            # If no user input file, then skip\n",
    "            if len(first_file) == 0:\n",
    "                # print(f\"❌ No final model performance for {i_dir}\")\n",
    "                continue\n",
    "\n",
    "            # Load final model performance\n",
    "            final_model = pd.read_csv(f\"{i_dir}/final_model_performance.csv\")\n",
    "            final_model = final_model.drop(columns=metric_names)\n",
    "\n",
    "            new_scores_file = f\"{i_dir}/rf_performance/roc_auc.csv\"\n",
    "            # Check if new scores file exists\n",
    "            if not os.path.exists(new_scores_file):\n",
    "                # raise ValueError(\n",
    "                print(\n",
    "                    f\"❌ No new scores file for {i_dir} -> probably forgot to run analysis of final models in `01_model_fitting.ipynb`!\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            new_scores = pd.read_csv(f\"{i_dir}/rf_performance/roc_auc.csv\")\n",
    "            new_scores.columns = metric_names\n",
    "\n",
    "            f_pred_prob = f\"{i_dir}/final_model/y_test_proba.csv\"\n",
    "            f_true = f\"{i_dir}/final_model/y_test.csv\"\n",
    "\n",
    "            df_pred_prob = pd.read_csv(f_pred_prob).iloc[:, 2].reset_index(drop=True)\n",
    "            df_true = pd.read_csv(f_true).iloc[:, 1].reset_index(drop=True)\n",
    "\n",
    "            # Check if both have the same length\n",
    "            if df_pred_prob.shape[0] != df_true.shape[0]:\n",
    "                raise ValueError(\"🟥 Length of df_pred_prob and df_true do not match\")\n",
    "                chime.error()\n",
    "\n",
    "            # > Run TSS\n",
    "            if do_tss_calculation:\n",
    "                print(\" | Running TSS...\")\n",
    "                tss_mean, tss_std, rest = bootstrap_optimal_tss(\n",
    "                    df_true,\n",
    "                    df_pred_prob,\n",
    "                    n_bootstraps=100,\n",
    "                    random_state=42,\n",
    "                    thresholds=np.linspace(0.0, 1.0, num=101),\n",
    "                    show=False,\n",
    "                )\n",
    "\n",
    "                # Overwrite with TSS scores\n",
    "                new_scores[\"tss_mean\"] = tss_mean\n",
    "                new_scores[\"tss_std\"] = tss_std\n",
    "\n",
    "                new_scores[\"specificity_mean\"] = rest[\"optimal_specificity_mean\"]\n",
    "                new_scores[\"specificity_std\"] = rest[\"optimal_specificity_std\"]\n",
    "\n",
    "            final_model = pd.concat([final_model, new_scores], axis=1)\n",
    "            final_model[\"species\"] = i_species\n",
    "            runs_subset.append(final_model)\n",
    "\n",
    "        # Skip if no runs\n",
    "        if len(runs_subset) == 0:\n",
    "            print(f\"\\t❌ No runs for {i_species}\")\n",
    "            continue\n",
    "\n",
    "        # Get save directory\n",
    "        dir_analysis = f\"{dir_save}/{i_species}\"\n",
    "        os.makedirs(dir_analysis, exist_ok=True)\n",
    "\n",
    "        # Concatenate all runs\n",
    "        runs_subset = pd.concat(runs_subset)\n",
    "\n",
    "        # Overwrite subset_group variable\n",
    "        runs_subset[\"subset_group\"] = (\n",
    "            runs_subset[\"dir\"].str.split(\"/\").str[-3].str.split(\"_\").str[-1]\n",
    "        )\n",
    "\n",
    "        df_filtered = runs_subset.copy()\n",
    "        df_filtered.to_csv(f\"{dir_analysis}/all_runs.csv\", index=False)\n",
    "    else:\n",
    "        dir_analysis = f\"{dir_save}/{i_species}\"\n",
    "        df_filtered = pd.read_csv(i_file)\n",
    "\n",
    "    # Update save dir based on grouping\n",
    "    dir_analysis = f\"{dir_analysis}/by_{sort_runs_by}\"\n",
    "    os.makedirs(dir_analysis, exist_ok=True)\n",
    "\n",
    "    # Overwrite metric variable to match remainder of the code\n",
    "    if sort_runs_by == \"tss\":\n",
    "        df_filtered[\"test_boot_mean\"] = df_filtered[\"tss_mean\"]\n",
    "        df_filtered[\"test_boot_sd\"] = df_filtered[\"tss_std\"]\n",
    "    elif sort_runs_by == \"specificity\":\n",
    "        df_filtered[\"test_boot_mean\"] = df_filtered[\"specificity_mean\"]\n",
    "        df_filtered[\"test_boot_sd\"] = df_filtered[\"specificity_std\"]\n",
    "\n",
    "    # Add metric to group variable\n",
    "    df_filtered[\"run\"] = df_filtered[\"subset_group\"].str.replace(\"run_\", \"\")\n",
    "    df_filtered[\"subset_group\"] = (\n",
    "        df_filtered[\"subset_group\"]\n",
    "        + \" (\"\n",
    "        + df_filtered[\"test_boot_mean\"].round(3).astype(str)\n",
    "        + \")\"\n",
    "    )\n",
    "\n",
    "    # Adjust sorting\n",
    "    if sort_plots_by == \"performance\":\n",
    "        df_filtered = df_filtered.sort_values(\n",
    "            \"test_boot_mean\", ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "    elif sort_plots_by == \"run\":\n",
    "        df_filtered = df_filtered.sort_values(\n",
    "            \"subset_group\", ascending=True\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        chime.error()\n",
    "        raise ValueError(\"🟥 sort_plots_by not recognized\")\n",
    "\n",
    "    # Add df to all\n",
    "    df_all.append(df_filtered)\n",
    "\n",
    "    # ! Performance per run -----------------------------------------------------\n",
    "    if do_plot_model_performances:\n",
    "        # Mean and std of final model\n",
    "        mean_perf = df_filtered[\"test_boot_mean\"].mean()\n",
    "        std_perf = df_filtered[\"test_boot_mean\"].std()\n",
    "        txt = f\"Mean performance on test set: {round(mean_perf, 3)} +/- {round(std_perf, 3)}\"\n",
    "        with open(f\"{dir_analysis}/mean_roc_auc_{round(mean_perf, 3)}.txt\", \"w\") as f:\n",
    "            f.write(txt)\n",
    "\n",
    "        # Make plots\n",
    "        def plot_fct(df, order, ascending, show=False):\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            sc = plt.scatter(\n",
    "                data=df.sort_values(order, ascending=ascending),\n",
    "                y=\"subset_group\",\n",
    "                x=\"test_boot_mean\",\n",
    "                edgecolors=\"black\",\n",
    "                s=60,\n",
    "                zorder=10,\n",
    "            )\n",
    "            plt.errorbar(\n",
    "                data=df.sort_values(order, ascending=ascending),\n",
    "                y=\"subset_group\",\n",
    "                x=\"test_boot_mean\",\n",
    "                xerr=\"test_boot_sd\",\n",
    "                color=\"black\",\n",
    "                fmt=\"none\",\n",
    "                zorder=2,\n",
    "            )\n",
    "\n",
    "            plt.grid(axis=\"y\", zorder=3)\n",
    "            plt.xlabel(\"Bootstrapped ROC-AUC on Test Set\", fontsize=14)\n",
    "            plt.xlim(0, 1)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{dir_analysis}/model_performance_sorted_by_{order}.png\")\n",
    "            plt.show() if show else plt.close()\n",
    "\n",
    "        plot_fct(df_filtered, \"subset_group\", False)\n",
    "        plot_fct(df_filtered, \"test_boot_mean\", True)\n",
    "\n",
    "    # ! Feature importance by dataset -----------------------------------------------------\n",
    "    if do_plot_feature_importance:\n",
    "        # Select all dataset importances\n",
    "        df_datasets = df_filtered.set_index(\"subset_group\").filter(\n",
    "            regex=\"Importance\", axis=1\n",
    "        )\n",
    "\n",
    "        # Clean variable names\n",
    "        df_datasets.columns = df_datasets.columns.str.replace(\" - Importance\", \"\")\n",
    "        # Replace NA with 0\n",
    "        df_datasets = df_datasets.replace(np.nan, 0)\n",
    "\n",
    "        # Scale all to 0-1\n",
    "        # df_datasets = df_datasets / df_datasets.max().max() * 100\n",
    "        # Order by median\n",
    "        order = df_datasets.median().sort_values(ascending=False).index\n",
    "        df_datasets = df_datasets[order]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.boxplot(\n",
    "            data=df_datasets,\n",
    "            ax=ax,\n",
    "            # order=df_datasets.mean().sort_values(ascending=False).index,\n",
    "            # inner=\"point\",\n",
    "            orient=\"h\",\n",
    "            palette=\"Blues_r\",\n",
    "        )\n",
    "        # Add jittered points\n",
    "        # sns.swarmplot(data=df_datasets, orient=\"h\", color=\"black\", alpha=0.25)\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"Relative Importance of Dataset across all Runs (N = {df_filtered.subset_group.nunique()})\",\n",
    "            weight=\"bold\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Relative importance of dataset within a species model (%)\")\n",
    "        # plt.xticks(rotation=45)\n",
    "        plt.xlim(-0.5, 35)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{dir_analysis}/feature_importance-shap-by_dataset.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # ! Feature importance by feature -----------------------------------------------------\n",
    "        # Get all datasets\n",
    "        all_datasets = (\n",
    "            df_filtered.set_index(\"subset_group\")\n",
    "            .filter(regex=\"Importance\", axis=1)\n",
    "            .columns.str.replace(\" - Importance\", \"\")\n",
    "            .values.tolist()\n",
    "        )\n",
    "\n",
    "        # Initiate lists\n",
    "        dic_values = {}\n",
    "        dic_counts = {}\n",
    "        dic_preds = {}\n",
    "\n",
    "        # Loop through all datasets\n",
    "        for d in all_datasets:\n",
    "            # Get dataset subset and remove NA values\n",
    "            df_i = (\n",
    "                df_filtered.set_index(\"subset_group\").filter(regex=d, axis=1).dropna()\n",
    "            )\n",
    "            dataset_features = []\n",
    "\n",
    "            # Loop over all species\n",
    "            for i, species in enumerate(df_i.index):\n",
    "\n",
    "                # Set species\n",
    "                df_i[\"Species\"] = species\n",
    "\n",
    "                # Get metrics and values of dataset\n",
    "                imetrics = df_i.loc[species][f\"{d} - Metrics\"]\n",
    "                ivalues = df_i.loc[species][f\"{d} - Values\"]\n",
    "\n",
    "                # Turn string into literal\n",
    "                imetrics = ast.literal_eval(imetrics)\n",
    "                ivalues = ast.literal_eval(ivalues)\n",
    "\n",
    "                # Loop over spei metrics\n",
    "                for j, jmetric in enumerate(imetrics):\n",
    "                    # Check if metric is already in dictionary\n",
    "                    if jmetric not in dic_values:\n",
    "                        dic_values[jmetric] = []\n",
    "                        dic_counts[jmetric] = 0\n",
    "\n",
    "                    # Attach value to dictionary\n",
    "                    dic_values[jmetric].append(ivalues[j])\n",
    "                    # Add count to dictionary\n",
    "                    dic_counts[jmetric] += 1\n",
    "                    # Attach metric to dictionary\n",
    "                    dataset_features.append(jmetric)\n",
    "\n",
    "            # Remove metric duplicates\n",
    "            dataset_features = list(set(dataset_features))\n",
    "            # Attach to dictionary\n",
    "            dic_preds[d] = dataset_features\n",
    "\n",
    "        # Get df\n",
    "        df_values = turn_dictionary_into_df(dic_values)\n",
    "        df_counts = df_values.count()\n",
    "        max_value = df_values.max().max()\n",
    "\n",
    "        # Scale df\n",
    "        # df_values = df_values / df_values.max().max() * 100\n",
    "\n",
    "        # Replace NA with 0\n",
    "        df_values = df_values.replace(np.nan, 0)\n",
    "\n",
    "        # Sort keys by dataset order\n",
    "        dic_preds = {key: dic_preds[key] for key in order if key in dic_preds}\n",
    "\n",
    "        # Create a 2x6 grid of subplots\n",
    "        fig, axs = plt.subplots(\n",
    "            3,\n",
    "            4,\n",
    "            figsize=(22, 10),\n",
    "            # sharex=True,\n",
    "            # sharey=True,\n",
    "        )\n",
    "\n",
    "        # Flatten the axs array\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        # Combine all columns for color scaling\n",
    "        max_val = df_values.notna().sum().max()\n",
    "        cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "\n",
    "        # Loop over each axis\n",
    "        for i, ax in enumerate(axs):\n",
    "            # Check if i is smaller than the number of datasets\n",
    "            if i < len(dic_preds.keys()):\n",
    "                # Get variables of that dataset\n",
    "                df_i = df_values[dic_preds[list(dic_preds.keys())[i]]]\n",
    "\n",
    "                # Order by median\n",
    "                iorder = df_i.median().sort_values(ascending=False).index\n",
    "                df_i = df_i[iorder]\n",
    "\n",
    "                # Get counts and normalize for color scaling\n",
    "                max_val = df_i.notna().sum().max()\n",
    "                norm_counts = df_i.notna().sum() / max_val\n",
    "                cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "                colors = norm_counts\n",
    "\n",
    "                # Create violin plot\n",
    "                # sns.violinplot(\n",
    "                sns.boxplot(\n",
    "                    data=df_i,\n",
    "                    ax=ax,\n",
    "                    orient=\"h\",\n",
    "                    palette=sns.color_palette(\"Blues\", as_cmap=True)(colors).tolist(),\n",
    "                    linecolor=\"black\",\n",
    "                )\n",
    "\n",
    "                # Set title\n",
    "                ax.set_title(list(dic_preds.keys())[i], weight=\"bold\")\n",
    "\n",
    "                # Set x-axis range\n",
    "                # ax.set_xlim(0, 100)\n",
    "                ax.set_xlim(-1, max_value * 1.05)\n",
    "\n",
    "                if i > 6:\n",
    "                    # Set x-axis label\n",
    "                    ax.set_xlabel(\"Relative Feature Importance\")\n",
    "            else:\n",
    "                # Remove axis\n",
    "                ax.remove()\n",
    "\n",
    "        # Add colorbar in the bottom right corner\n",
    "        cbar_ax = fig.add_axes(\n",
    "            [0.875, 0.05, 0.05, 0.25]\n",
    "        )  # [left, bottom, width, height]\n",
    "        cbar = plt.colorbar(\n",
    "            plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(0, 1)),\n",
    "            cax=cbar_ax,\n",
    "            orientation=\"vertical\",\n",
    "        )\n",
    "        cbar.set_label(\n",
    "            f\"Share of occurrences across\\nall models (100% = {df_filtered.shape[0]} models)\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "        # Set the ticks of the colorbar to represent percentages from 0 to 100\n",
    "        cbar.set_ticks(np.linspace(0, 1, 11))  # Set 11 ticks from 0 to 1\n",
    "        cbar.set_ticklabels(\n",
    "            [f\"{int(tick*100)}%\" for tick in np.linspace(0, 1, 11)]\n",
    "        )  # Convert ticks to percentages\n",
    "\n",
    "        # Make sure the subplots are tight\n",
    "        # plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"{dir_analysis}/feature_importance-shap-by_feature-colored_relative_occurences.png\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    # ! Interaction plots -----------------------------------------------------\n",
    "    if do_plot_interactions:\n",
    "        datasets = [\n",
    "            \"SPEI\",\n",
    "            \"Temperature\",\n",
    "            # \"Tree Size\",\n",
    "            # \"Light Competition\",\n",
    "            # \"Species Competition\",\n",
    "            # \"Topography\",\n",
    "            # \"NDVI\",\n",
    "            # \"Stand Structure\",\n",
    "            # \"Soil Fertility\",\n",
    "            # \"Management\",\n",
    "            # \"Soil Water Conditions\",\n",
    "        ]\n",
    "\n",
    "        for ds1 in datasets:\n",
    "            if single_response:\n",
    "                plot_shap_response(\n",
    "                    df_filtered,\n",
    "                    ds1,\n",
    "                    scale_shap=scale_shap,\n",
    "                    scale_response=scale_response,\n",
    "                    remove_labels=False,\n",
    "                    show=False,\n",
    "                    dir_analysis=dir_analysis,\n",
    "                    filesuffix=f\"-sorted_by_{sort_plots_by}\",\n",
    "                )\n",
    "\n",
    "            for ds2 in datasets:\n",
    "                if ds1 == ds2:\n",
    "                    continue\n",
    "\n",
    "                if interaction_1d:\n",
    "                    plot_shap_response_interaction(\n",
    "                        df_filtered,\n",
    "                        ds1,\n",
    "                        ds2,\n",
    "                        remove_labels=False,\n",
    "                        show=False,\n",
    "                        dir_analysis=dir_analysis,\n",
    "                        filesuffix=f\"-sorted_by_{sort_plots_by}\",\n",
    "                    )\n",
    "                if interaction_2d:\n",
    "                    plot_shap_interaction2(\n",
    "                        df_filtered,\n",
    "                        ds1,\n",
    "                        ds2,\n",
    "                        remove_labels=False,\n",
    "                        show=False,\n",
    "                        dir_analysis=dir_analysis,\n",
    "                        filesuffix=f\"-sorted_by_{sort_plots_by}\",\n",
    "                    )\n",
    "\n",
    "if len(df_all) == 0:\n",
    "    raise ValueError(\n",
    "        \"🟥 No runs found for any species! -> maybe wrong species selected above for extra analysis?\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"Found {len(df_all)} species with runs.\")\n",
    "    df_all_concat = pd.concat(df_all).reset_index(drop=False)\n",
    "df_all_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate checks\n",
    "\n",
    "# Checkf for data on external drives\n",
    "if \"/Volumes/SAMSUNG 1TB\" in dir_runs:\n",
    "    if which_analysis == \"default\":\n",
    "        # Replace path to all_runs\n",
    "        df_all_concat[\"dir\"] = df_all_concat[\"dir\"].str.replace(\n",
    "            \"model_runs/all_runs\", \"/Volumes/SAMSUNG 1TB/all_runs\"\n",
    "        )\n",
    "        df_all_concat[\"dir\"] = df_all_concat[\"dir\"].str.replace(\n",
    "            \".//Volumes\", \"/Volumes\"\n",
    "        )\n",
    "    else:\n",
    "        # Replace path to extra runs\n",
    "        df_all_concat[\"dir\"] = df_all_concat[\"dir\"].str.replace(\n",
    "            \"./extra_runs//\" + which_analysis,\n",
    "            \"/Volumes/SAMSUNG 1TB/extra_runs/\" + which_analysis,\n",
    "        )\n",
    "        df_all_concat[\"dir\"] = df_all_concat[\"dir\"].str.replace(\n",
    "            \".//Volumes\", \"/Volumes\"\n",
    "        )\n",
    "\n",
    "# Checks for additional analyses\n",
    "if which_analysis != \"default\" and df_all_concat.species.nunique() not in [1, 9, 52]:\n",
    "    raise ValueError(\n",
    "        \"🚨 For non-default runs, not more than one species is expected to be analysed at once!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "tmp_vars = [\"species\", \"subset_group\"]\n",
    "\n",
    "for c in df_all_concat.columns:\n",
    "    if \"Metrics\" in c:  # or \"Values\" in c:\n",
    "        tmp_vars.append(c)\n",
    "\n",
    "# Drop rows that have na\n",
    "df_tmp = df_all_concat[tmp_vars]\n",
    "\n",
    "df_tmp_nona = df_tmp.dropna()\n",
    "nrow_all = df_tmp.shape[0]\n",
    "nrow_all_nona = df_tmp_nona.shape[0]\n",
    "print(\n",
    "    f\"Found {nrow_all} rows, of which {nrow_all_nona} ({round(nrow_all_nona / nrow_all * 100, 1)}%) have no NA values in the metrics.\"\n",
    ")\n",
    "\n",
    "df_tmp_9 = df_tmp.query(\"species in @top9\")\n",
    "nrow_top9 = df_tmp_9.shape[0]\n",
    "nrow_top9_nona = df_tmp_9.dropna().shape[0]\n",
    "print(\n",
    "    f\"Found {nrow_top9} rows for top 9 species, of which {nrow_top9_nona} ({round(nrow_top9_nona / nrow_top9 * 100, 1)}%) have no NA values in the metrics.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess runs with same predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut to avoid plotting\n",
    "make_plot = False\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Chosen settings for response classification\n",
    "     - Scale SHAP: {scale_shap}\n",
    "     - Scale Response: {scale_response}\n",
    "     - ROC Threshold: {roc_threshold}\n",
    "     - Make Plots: {make_plot}\n",
    "     - Saving Directory: {dir_save}\n",
    "     - Runs Directory: {dir_runs}\n",
    "\n",
    "    Notes:\n",
    "     - Parallel processing takes about 5 minutes without plotting and 10 minutes with plotting.\n",
    "     - Since running parallel, the df_patterns must be created afterwards by merging the results.\n",
    "     - 🐞 If `KeyError`: 'SPEI - Importance': Model performance and SHAP values and importance have not been calculated in `01_model_fitting.ipynb`\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Reduce df to missing files\n",
    "expected_file = \"runs_aggregated_by_spei_temp_pairs-filtered_by_roc-grouped.csv\"\n",
    "df_todo = df_all_concat.copy()\n",
    "df_todo[\"expected_file\"] = dir_save + df_todo[\"species\"] + \"/\" + expected_file\n",
    "\n",
    "for i, row in df_todo.iterrows():\n",
    "    if os.path.exists(row[\"expected_file\"]):\n",
    "        df_todo = df_todo.drop(i)\n",
    "\n",
    "# Turn df into list\n",
    "tmp_list = split_df_into_list_of_group_or_ns(df_todo, \"species\")\n",
    "# tmp_list = split_df_into_list_of_group_or_ns(df_todo)\n",
    "\n",
    "# Reimport for mp\n",
    "from random_forest_utils import response_classification\n",
    "\n",
    "run_mp(\n",
    "    response_classification,\n",
    "    tmp_list,\n",
    "    progress_bar=True,\n",
    "    num_cores=9,\n",
    "    scale_shap=scale_shap,\n",
    "    scale_response=scale_response,\n",
    "    roc_threshold=roc_threshold,\n",
    "    dir_save=dir_save,\n",
    "    make_plots=make_plot,\n",
    "    verbose=False,\n",
    ")\n",
    "chime.success()\n",
    "\n",
    "# Code for a single run:\n",
    "# df_test = tmp_list[0]\n",
    "# df_test = df_all_concat.query(\"species == 'Frangula alnus'\").copy()\n",
    "# response_classification(\n",
    "#     df_test,\n",
    "#     scale_shap=scale_shap,\n",
    "#     scale_response=scale_response,\n",
    "#     roc_threshold=roc_threshold,\n",
    "#     dir_save=dir_save,\n",
    "#     make_plots=True,\n",
    "#     verbose=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# ! Classification data\n",
    "use_class_data = \"mk\"\n",
    "# | mk - lm\n",
    "threshold_metric = \"pvalue\"\n",
    "# | Metric to use for response classification: pvalue - r2\n",
    "threshold_value = 0.01\n",
    "# | Value to use for response classification\n",
    "min_group_share = 0.6\n",
    "# | Groups with no pattern share above this will be classified as unclear\n",
    "\n",
    "# ! Aggregation settings\n",
    "method_scaling_importance = \"scaled_valid\"\n",
    "# | sum: counts the number of times a feature is selected, the more valid runs for a species, the higher the count\n",
    "# | scaled_all: percentage of occurences of feature in all runs for a species (including invalid runs)\n",
    "# | scaled_valid: percentage of occurences of feature in valid runs for a species (excluding invalid runs)\n",
    "\n",
    "# ! Chart settings\n",
    "normalized_change_chart = False  # Normalize change-chart?\n",
    "normalized_import_chart = False  # Normalize importance-chart?\n",
    "bars_in_double_chart = \"ntrees\"  # ntrees or nruns\n",
    "ntrees_log = False  # Log scale for ntrees\n",
    "\n",
    "# Display order\n",
    "sort_by = \"species\"  # change - species - runs - performance - importance - ntrees\n",
    "\n",
    "# Change order\n",
    "change_dict = {\n",
    "    \"warmer_drier\": \"#B2182B\",\n",
    "    \"cooler_drier\": \"#EF8A62\",\n",
    "    \"other\": \"lightgrey\",\n",
    "    \"cooler_wetter\": \"#67A9CF\",\n",
    "    \"warmer_wetter\": \"#2166AC\",\n",
    "}\n",
    "change_order = [k for k in change_dict.keys()]\n",
    "change_colors = [i for i in change_dict.values()]\n",
    "\n",
    "# For temperature boxplots\n",
    "one_point_per_pattern = True\n",
    "\n",
    "# ! Subsetting\n",
    "top_only = False\n",
    "top_n = 20\n",
    "\n",
    "# ! Other\n",
    "n_all_runs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = dir_save.split(\"_response_\")[0].rsplit(\"/\", 1)[0]\n",
    "dir_patterns = f\"{dir_root}/pattern_analysis/by_{use_class_data}/roc_{roc_threshold}-min_group_share_{min_group_share}/\"\n",
    "\n",
    "if use_class_data == \"lm\":\n",
    "    dir_patterns = f\"{dir_patterns}/{threshold_metric}_{threshold_value}-min_group_share_{min_group_share}/\"\n",
    "\n",
    "# Set tables directory\n",
    "dir_tables = f\"{dir_patterns}/tables\"\n",
    "\n",
    "os.makedirs(dir_patterns, exist_ok=True)\n",
    "os.makedirs(dir_tables, exist_ok=True)\n",
    "dir_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of each run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df_patterns from classifications done above\n",
    "files = glob.glob(\n",
    "    f\"{dir_save}/*/runs_aggregated_by_spei_temp_pairs-filtered_by_roc-grouped.csv\"\n",
    ")\n",
    "\n",
    "df_patterns = pd.concat([pd.read_csv(f) for f in files]).reset_index(drop=True)\n",
    "df_patterns.group_size.sum()\n",
    "df_patterns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Note that aggregation to combined temperature-spei response causes some species to not have 50 runs because correlation-removal sometimes resulted in picking temp OR spei feature.\"\n",
    ")\n",
    "df_patterns.groupby(\"species\").group_size.sum().sort_index().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate\n",
    "df_fin = []\n",
    "if threshold_metric == \"pvalue\":\n",
    "    imetric = \"pval_1\"\n",
    "elif threshold_metric == \"r2\":\n",
    "    imetric = \"r2\"\n",
    "\n",
    "# Loop over all species\n",
    "list_responses_spei = []\n",
    "list_responses_temp = []\n",
    "for ispecies in df_patterns.species.unique():\n",
    "\n",
    "    # ispecies = \"Fagus sylvatica\"  # ! DEBUG!\n",
    "\n",
    "    # print(f\" - {ispecies}\")\n",
    "\n",
    "    # Get species directory\n",
    "    idir = f\"{dir_save}/{ispecies}/single_response_classification/all_runs/\"\n",
    "\n",
    "    # Get empty list for groups\n",
    "    df_spec = []\n",
    "\n",
    "    for ifeature in [\"SPEI\", \"Temperature\"]:\n",
    "\n",
    "        if ifeature == \"SPEI\":\n",
    "            dataset = \"spei\"\n",
    "        else:\n",
    "            dataset = \"temp\"\n",
    "\n",
    "        idfs = glob.glob(f\"{idir}/{ifeature}*group*.csv\")\n",
    "        # print(f\"  - {ifeature} ({len(idfs)} files)\")\n",
    "        if len(idfs) == 0:\n",
    "            chime.error()\n",
    "            raise ValueError(\n",
    "                \"🟥 No files found. Did you create the response figures and csvs for today?\"\n",
    "            )\n",
    "        # Load all files\n",
    "        idfs = [pd.read_csv(i) for i in idfs]\n",
    "        idfs = pd.concat(idfs).reset_index(drop=True)\n",
    "\n",
    "        for g in sorted(idfs.run.unique()):\n",
    "            # Get group\n",
    "            gdf = idfs.query(\"run == @g\")\n",
    "\n",
    "            # Start new df\n",
    "            # display(gdf)\n",
    "            new_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"species\": ispecies,\n",
    "                    \"group\": gdf.group.iloc[0],\n",
    "                    f\"run\": gdf.run.iloc[0].split(\" (\")[0],\n",
    "                    f\"feature_{dataset}\": gdf.feature.iloc[0],\n",
    "                },\n",
    "                index=[0],\n",
    "            )\n",
    "\n",
    "            # Classify response depending on input\n",
    "            if threshold_metric != \"pvalue\":\n",
    "                chime.error()\n",
    "                raise ValueError(\"🟥 Only pvalue is implemented\")\n",
    "\n",
    "            # Get metric and slope\n",
    "            if use_class_data == \"lm\":\n",
    "                gmetric = gdf.query(\"method == 'linear'\")[\"coef1_pval\"].iloc[0]\n",
    "                gslope = gdf.query(\"method == 'linear'\")[\"coef1\"].iloc[0]\n",
    "            elif use_class_data == \"mk\":\n",
    "                gmetric = gdf.query(\"method == 'mk_raw'\")[\"p\"].iloc[0]\n",
    "                gslope = gdf.query(\"method == 'mk_raw'\")[\"tau\"].iloc[0]\n",
    "            else:\n",
    "                chime.error()\n",
    "                raise ValueError(\"🟥 use_class_data not recognized\")\n",
    "\n",
    "            # Classify response\n",
    "            if gmetric < threshold_value:\n",
    "                gtrend = detect_trend(gslope)\n",
    "            else:\n",
    "                gtrend = \"other\"\n",
    "\n",
    "            # Attach to new df\n",
    "            new_df[f\"response_{dataset}\"] = gtrend\n",
    "            if dataset == \"spei\":\n",
    "                list_responses_spei.append(new_df)\n",
    "            elif dataset == \"temp\":\n",
    "                list_responses_temp.append(new_df)\n",
    "\n",
    "# Clean df\n",
    "df_responses_spei = pd.concat(list_responses_spei).reset_index(drop=True)\n",
    "df_responses_temp = pd.concat(list_responses_temp).reset_index(drop=True)\n",
    "\n",
    "# Clean wording of increasing/decreasing\n",
    "df_responses_spei = df_responses_spei.replace(\n",
    "    {\n",
    "        \"response_spei\": {\n",
    "            \"increasing\": \"drier\",\n",
    "            \"decreasing\": \"wetter\",\n",
    "            \"unclear\": \"other\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "df_responses_temp = df_responses_temp.replace(\n",
    "    {\n",
    "        \"response_temp\": {\n",
    "            \"increasing\": \"warmer\",\n",
    "            \"decreasing\": \"cooler\",\n",
    "            \"unclear\": \"other\",\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge response dfs\n",
    "df_responses = pd.merge(\n",
    "    df_responses_spei, df_responses_temp, on=[\"species\", \"group\", \"run\"]\n",
    ").sort_values([\"species\", \"group\", \"run\"])\n",
    "\n",
    "# Add change\n",
    "df_responses[\"change\"] = (\n",
    "    df_responses[\"response_temp\"] + \"_\" + df_responses[\"response_spei\"]\n",
    ")\n",
    "# Clean wording of increasing/decreasing\n",
    "# Clean unclear merges\n",
    "df_responses[\"change\"] = df_responses[\"change\"].replace(\n",
    "    {\n",
    "        # SPEI_Temp\n",
    "        \"drier_other\": \"other\",\n",
    "        \"wetter_other\": \"other\",\n",
    "        \"other_warmer\": \"other\",\n",
    "        \"other_cooler\": \"other\",\n",
    "        \"other_other\": \"other\",\n",
    "        # Temp_SPEI\n",
    "        \"other_drier\": \"other\",\n",
    "        \"other_wetter\": \"other\",\n",
    "        \"warmer_other\": \"other\",\n",
    "        \"cooler_other\": \"other\",\n",
    "        # Other\n",
    "        \"unclear\": \"other\",\n",
    "    }\n",
    ")\n",
    "\n",
    "display(df_responses)\n",
    "print(f\"Variations of change: {df_responses.change.unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every species, attach how many runs there are\n",
    "df_nRunsSpecies = df_responses.groupby(\"species\").run.nunique().reset_index()\n",
    "df_nRunsSpecies = df_nRunsSpecies.rename(columns={\"run\": \"n_runs\"})\n",
    "df_nRunsSpecies[\"n_runs_rel\"] = (\n",
    "    (df_nRunsSpecies[\"n_runs\"] / 50 * 100).round().astype(int)\n",
    ")\n",
    "\n",
    "df_nRunsSpecies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Pattern Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_spei_patterns = []\n",
    "\n",
    "# Loop over all species\n",
    "all_species = df_responses_spei.species.unique().tolist()\n",
    "all_species = sorted(all_species)\n",
    "\n",
    "for ispecies in all_species:\n",
    "\n",
    "    # Get df\n",
    "    idf = df_responses_spei.query(\"species == @ispecies\")\n",
    "\n",
    "    # Loop over all features of that species\n",
    "    group_count = 0\n",
    "    all_features = idf.feature_spei.unique()\n",
    "\n",
    "    # Loop over all features\n",
    "    for ifeature in all_features:\n",
    "\n",
    "        # Get group count\n",
    "        group_count += 1\n",
    "\n",
    "        # Get df\n",
    "        idf = df_responses_spei.query(\n",
    "            \"species == @ispecies and feature_spei == @ifeature\"\n",
    "        )\n",
    "\n",
    "        # Get group size\n",
    "        group_size = idf.shape[0]\n",
    "\n",
    "        # Get share of each response\n",
    "        idf = (\n",
    "            idf.groupby(\"response_spei\")\n",
    "            .agg(group_size=(\"response_spei\", \"count\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "        idf[\"share\"] = (idf[\"group_size\"] / idf[\"group_size\"].sum() * 100).round(1)\n",
    "        idf[\"change_original\"] = idf[\"response_spei\"]\n",
    "        idf = idf.sort_values(\"share\", ascending=False)\n",
    "        idf = idf.reset_index(drop=True)\n",
    "\n",
    "        # Check if one group has above share threshold\n",
    "        if idf.iloc[0].share < min_group_share * 100:\n",
    "            idf[\"response_spei\"] = \"other\"\n",
    "            # idf[\"share\"] = np.nan\n",
    "            pass\n",
    "\n",
    "        # Keep only top 1\n",
    "        idf = idf.head(1)\n",
    "\n",
    "        # Add information\n",
    "        idf[\"species\"] = ispecies\n",
    "        idf[\"feature_spei\"] = ifeature\n",
    "        idf[\"group_spei\"] = f\"{group_count}\"\n",
    "\n",
    "        list_spei_patterns.append(idf)\n",
    "\n",
    "# Concatenate\n",
    "df_spei_patterns = pd.concat(list_spei_patterns).reset_index(drop=True)\n",
    "\n",
    "# Attach number of runs per species\n",
    "df_spei_patterns = pd.merge(\n",
    "    df_spei_patterns, df_nRunsSpecies, on=\"species\", how=\"left\"\n",
    ").rename(columns={\"run\": \"n_runs\"})\n",
    "\n",
    "# Calculate relative group sizes\n",
    "df_spei_patterns[\"group_size_rel_valruns\"] = (\n",
    "    (df_spei_patterns[\"group_size\"] / df_spei_patterns[\"n_runs\"] * 100)\n",
    "    .round()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "df_spei_patterns[\"group_size_rel_50runs\"] = (\n",
    "    (df_spei_patterns[\"group_size\"] / 50 * 100).round().astype(int)\n",
    ")\n",
    "\n",
    "df_spei_patterns = (\n",
    "    move_vars_to_front(df_spei_patterns, [\"species\", \"group_spei\"])\n",
    "    .sort_values([\"species\", \"group_spei\", \"share\"], ascending=[True, True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save df\n",
    "df_to_save = df_spei_patterns.sort_values(\n",
    "    [\"group_size_rel_50runs\", \"response_spei\"], ascending=False\n",
    ")\n",
    "df_to_save = move_vars_to_front(\n",
    "    df_to_save, [\"species\", \"group_size_rel_50runs\", \"response_spei\", \"feature_spei\"]\n",
    ")\n",
    "\n",
    "df_to_save.to_csv(\n",
    "    f\"{dir_patterns}/models_merged_by_response-spei.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "df_spei_patterns.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_temp_patterns = []\n",
    "\n",
    "# Loop over all species\n",
    "all_species = df_responses_temp.species.unique().tolist()\n",
    "all_species = sorted(all_species)\n",
    "\n",
    "for ispecies in all_species:\n",
    "\n",
    "    # Get df\n",
    "    idf = df_responses_temp.query(\"species == @ispecies\")\n",
    "\n",
    "    # Loop over all features of that species\n",
    "    group_count = 0\n",
    "    all_features = idf.feature_temp.unique()\n",
    "\n",
    "    # Loop over all features\n",
    "    for ifeature in all_features:\n",
    "\n",
    "        # Get group count\n",
    "        group_count += 1\n",
    "\n",
    "        # Get df\n",
    "        idf = df_responses_temp.query(\n",
    "            \"species == @ispecies and feature_temp == @ifeature\"\n",
    "        )\n",
    "\n",
    "        # Get group size\n",
    "        group_size = idf.shape[0]\n",
    "\n",
    "        # Get share of each response\n",
    "        idf = (\n",
    "            idf.groupby(\"response_temp\")\n",
    "            .agg(group_size=(\"response_temp\", \"count\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "        idf[\"share\"] = (idf[\"group_size\"] / idf[\"group_size\"].sum() * 100).round(1)\n",
    "        idf[\"change_original\"] = idf[\"response_temp\"]\n",
    "        idf = idf.sort_values(\"share\", ascending=False)\n",
    "        idf = idf.reset_index(drop=True)\n",
    "\n",
    "        # Check if one group has above share threshold\n",
    "        if idf.iloc[0].share < min_group_share * 100:\n",
    "            idf[\"response_temp\"] = \"other\"\n",
    "            # idf[\"share\"] = np.nan\n",
    "            pass\n",
    "\n",
    "        # Keep only top 1\n",
    "        idf = idf.head(1)\n",
    "\n",
    "        # Add information\n",
    "        idf[\"species\"] = ispecies\n",
    "        idf[\"feature_temp\"] = ifeature\n",
    "        idf[\"group_temp\"] = f\"{group_count}\"\n",
    "\n",
    "        list_temp_patterns.append(idf)\n",
    "\n",
    "# Concatenate\n",
    "df_temp_patterns = pd.concat(list_temp_patterns).reset_index(drop=True)\n",
    "\n",
    "# Attach number of runs per species\n",
    "df_temp_patterns = pd.merge(\n",
    "    df_temp_patterns, df_nRunsSpecies, on=\"species\", how=\"left\"\n",
    ").rename(columns={\"run\": \"n_runs\"})\n",
    "\n",
    "# Calculate relative group sizes\n",
    "df_temp_patterns[\"group_size_rel_valruns\"] = (\n",
    "    (df_temp_patterns[\"group_size\"] / df_temp_patterns[\"n_runs\"] * 100)\n",
    "    .round()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "df_temp_patterns[\"group_size_rel_50runs\"] = (\n",
    "    (df_temp_patterns[\"group_size\"] / 50 * 100).round().astype(int)\n",
    ")\n",
    "\n",
    "df_temp_patterns = (\n",
    "    move_vars_to_front(df_temp_patterns, [\"species\", \"group_temp\"])\n",
    "    .sort_values([\"species\", \"group_temp\", \"share\"], ascending=[True, True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save df\n",
    "df_to_save = df_temp_patterns.sort_values(\n",
    "    [\"group_size_rel_50runs\", \"response_temp\"], ascending=False\n",
    ")\n",
    "df_to_save = move_vars_to_front(\n",
    "    df_to_save, [\"species\", \"group_size_rel_50runs\", \"response_temp\", \"feature_temp\"]\n",
    ")\n",
    "\n",
    "df_to_save.to_csv(\n",
    "    f\"{dir_patterns}/models_merged_by_response-temp.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "df_temp_patterns.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spei-Temp Pair Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all species and groups\n",
    "list_both_patterns = []\n",
    "\n",
    "# For all species\n",
    "all_species = df_responses.species.unique()\n",
    "all_species = sorted(all_species.tolist())\n",
    "for ispecies in all_species:\n",
    "\n",
    "    # For all groups per species\n",
    "    all_groups = df_responses.query(\"species == @ispecies\").group.unique()\n",
    "    all_groups = sorted(all_groups.tolist())\n",
    "\n",
    "    for igroup in all_groups:\n",
    "\n",
    "        # Get df\n",
    "        idf = df_responses.query(\"species == @ispecies & group == @igroup\")\n",
    "\n",
    "        # Verbose\n",
    "        # print(f\"{ispecies} - {igroup}\")\n",
    "        # display(idf)\n",
    "\n",
    "        # Get group size\n",
    "        group_size = idf.shape[0]\n",
    "\n",
    "        # Get temp_spei pairs\n",
    "        idf[\"change\"] = idf[\"response_temp\"] + \"_\" + idf[\"response_spei\"]\n",
    "\n",
    "        # Get share of each change pair\n",
    "        idf = idf.groupby(\"change\").agg(group_size=(\"change\", \"count\")).reset_index()\n",
    "        idf[\"share\"] = (idf[\"group_size\"] / idf[\"group_size\"].sum() * 100).round(1)\n",
    "        idf = idf.sort_values(\"share\", ascending=False)\n",
    "        idf = idf.reset_index(drop=True)\n",
    "        idf[\"change_original\"] = idf[\"change\"]\n",
    "\n",
    "        # Check if one group has above share threshold\n",
    "        if idf.iloc[0].share < min_group_share * 100:\n",
    "            idf[\"change\"] = \"other\"\n",
    "            # idf[\"share\"] = np.nan\n",
    "            pass\n",
    "\n",
    "        # Keep only top 1\n",
    "        idf = idf.head(1)\n",
    "\n",
    "        # Add information\n",
    "        idf[\"species\"] = ispecies\n",
    "        idf[\"group\"] = igroup\n",
    "\n",
    "        list_both_patterns.append(idf)\n",
    "\n",
    "# Concatenate\n",
    "df_both_patterns = pd.concat(list_both_patterns).reset_index(drop=True)\n",
    "\n",
    "# Attach number of runs per species\n",
    "df_both_patterns = pd.merge(\n",
    "    df_both_patterns, df_nRunsSpecies, on=\"species\", how=\"left\"\n",
    ").rename(columns={\"run\": \"n_runs\"})\n",
    "\n",
    "# Calculate relative group sizes\n",
    "df_both_patterns[\"group_size_rel_valruns\"] = (\n",
    "    (df_both_patterns[\"group_size\"] / df_both_patterns[\"n_runs\"] * 100)\n",
    "    .round()\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "df_both_patterns[\"group_size_rel_50runs\"] = (\n",
    "    (df_both_patterns[\"group_size\"] / 50 * 100).round().astype(int)\n",
    ")\n",
    "\n",
    "df_both_patterns = (\n",
    "    move_vars_to_front(df_both_patterns, [\"species\", \"group\"])\n",
    "    .sort_values([\"species\", \"group\", \"share\"], ascending=[True, True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Attach information on feature\n",
    "df_both_patterns = pd.merge(\n",
    "    df_both_patterns,\n",
    "    df_responses[\n",
    "        [\n",
    "            \"species\",\n",
    "            \"group\",\n",
    "            \"feature_temp\",\n",
    "            \"feature_spei\",\n",
    "            # \"response_spei\",\n",
    "            # \"response_temp\",\n",
    "        ]\n",
    "    ].drop_duplicates(),\n",
    "    on=[\"species\", \"group\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "df_to_save = df_both_patterns.sort_values(\n",
    "    [\"group_size_rel_50runs\", \"change\"], ascending=False\n",
    ")\n",
    "df_to_save = move_vars_to_front(\n",
    "    df_to_save,\n",
    "    [\"species\", \"group_size_rel_50runs\", \"change\", \"feature_temp\", \"feature_spei\"],\n",
    ")\n",
    "\n",
    "df_to_save.to_csv(\n",
    "    f\"{dir_patterns}/models_merged_by_response-both_spei_and_temp.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "df_both_patterns.sort_values(\"n_runs\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for consistency of patterns\n",
    "print(\n",
    "    f\" --- Note that the total number of models varies because removing correlating features during model fitting resulted in some models only have one response (SPEI or TEMP) and some have both. --- \"\n",
    ")\n",
    "\n",
    "for df in [\n",
    "    df_spei_patterns,\n",
    "    df_temp_patterns,\n",
    "    df_both_patterns,\n",
    "]:\n",
    "\n",
    "    all_models = df.group_size.sum()\n",
    "    all_models = 2600\n",
    "    imean = df.share.mean()\n",
    "\n",
    "    all_models_9 = df.query(\"species in @top9\").shape[0]\n",
    "    all_models_9 = 450\n",
    "    imean_9 = df.query(\"species in @top9\").share.mean()\n",
    "\n",
    "    if \"response_spei\" in df.columns:\n",
    "        s = \"SPEI\"\n",
    "        df = (\n",
    "            # df.query(\"response_spei != change_original\")\n",
    "            df.query(\"share < @min_group_share * 100 and change_original != 'other'\")\n",
    "            .copy()\n",
    "            .sort_values(\"share\")\n",
    "        )\n",
    "    elif \"response_temp\" in df.columns:\n",
    "        s = \"TEMP\"\n",
    "        df = (\n",
    "            # df.query(\"response_temp != change_original\")\n",
    "            df.query(\"share < @min_group_share * 100 and change_original != 'other'\")\n",
    "            .copy()\n",
    "            .sort_values(\"share\")\n",
    "        )\n",
    "    else:\n",
    "        s = \"BOTH\"\n",
    "        df = (\n",
    "            # df.query(\"change != change_original\")\n",
    "            df.query(\"share < @min_group_share * 100 and change_original != 'other'\")\n",
    "            .copy()\n",
    "            .sort_values(\"share\")\n",
    "        )\n",
    "\n",
    "    # For all species\n",
    "    isum = df.group_size.sum()\n",
    "    imean_sub = df.share.mean()\n",
    "\n",
    "    print(f\"\\n--- {s} --- \")\n",
    "    print(\n",
    "        f\"All Species: Found {df.shape[0]} groups ({isum}/{all_models} = {isum/all_models*100:.2f}% models) with share < {min_group_share * 100}% for {s} patterns. Mean share all models: {round(imean, 1)}%. Mean share across non-consistent models: {round(imean_sub, 1)}%.\"\n",
    "    )\n",
    "    # display(df)\n",
    "\n",
    "    # For top9 species\n",
    "    df = df.query(\"species in @top9\").copy()\n",
    "    isum = df.group_size.sum()\n",
    "    imean_sub = df.share.mean()\n",
    "    print(\n",
    "        f\"Top9 Species: Found {df.shape[0]} groups ({isum}/{all_models_9} = {isum/all_models_9*100:.2f}% models) with share < {min_group_share * 100}% for {s} patterns. Mean share: {round(imean_9, 1)}%. Mean share across non-consistent models: {round(imean_sub, 1)}%.\"\n",
    "    )\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts per change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single change shares per species\n",
    "# ! Temperature\n",
    "tmp_list = []\n",
    "for ispecies in df_responses.species.unique():\n",
    "    idf = df_responses.query(\"species == @ispecies\")\n",
    "    idf[\"group_size\"] = 1  # TODO: Ugly debug hack\n",
    "    idf = idf.groupby(\"response_temp\").agg(group_size=(\"group_size\", \"sum\"))\n",
    "    df_pivot = (\n",
    "        idf.pivot_table(\n",
    "            index=lambda _: ispecies,\n",
    "            columns=\"response_temp\",\n",
    "            values=\"group_size\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"response_temp\": \"species\", \"other\": \"temp_unclear\"})\n",
    "    )\n",
    "\n",
    "    tmp_list.append(df_pivot)\n",
    "\n",
    "species_change_temp = pd.concat(tmp_list).reset_index(drop=True).fillna(0)\n",
    "\n",
    "# Check that all columns are present\n",
    "expected_columns = [\"species\", \"warmer\", \"cooler\", \"temp_unclear\"]\n",
    "for col in expected_columns:\n",
    "    if col not in species_change_temp.columns:\n",
    "        species_change_temp[col] = 0  # Add missing column with 0s\n",
    "species_change_temp[\"sum_changes_temp\"] = species_change_temp[\n",
    "    [\"warmer\", \"cooler\", \"temp_unclear\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "# ! Spei\n",
    "tmp_list = []\n",
    "for ispecies in df_spei_patterns.species.unique():\n",
    "    idf = df_responses.query(\"species == @ispecies\")\n",
    "    idf[\"group_size\"] = 1\n",
    "    idf = idf.groupby(\"response_spei\").agg(group_size=(\"group_size\", \"sum\"))\n",
    "    df_pivot = (\n",
    "        idf.pivot_table(\n",
    "            index=lambda _: ispecies,\n",
    "            columns=\"response_spei\",\n",
    "            values=\"group_size\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"response_spei\": \"species\", \"other\": \"spei_unclear\"})\n",
    "    )\n",
    "\n",
    "    tmp_list.append(df_pivot)\n",
    "\n",
    "\n",
    "species_change_spei = pd.concat(tmp_list).reset_index(drop=True).fillna(0)\n",
    "\n",
    "# Check that all columns are present\n",
    "expected_columns = [\"species\", \"drier\", \"wetter\", \"spei_unclear\"]\n",
    "for col in expected_columns:\n",
    "    if col not in species_change_spei.columns:\n",
    "        species_change_spei[col] = 0  # Add missing column with 0s\n",
    "species_change_spei[\"sum_changes_spei\"] = species_change_spei[\n",
    "    [\"drier\", \"wetter\", \"spei_unclear\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "# ! Merge\n",
    "species_change_single = pd.merge(\n",
    "    species_change_temp, species_change_spei, on=\"species\", how=\"outer\"\n",
    ").fillna(0)\n",
    "\n",
    "# Set index to species to make it easier to sum\n",
    "species_change_single = species_change_single.set_index(\"species\")\n",
    "\n",
    "# Take rowwise sum\n",
    "species_change_single[\"sum_changes\"] = species_change_single.sum(axis=1) / 2\n",
    "# Rearrange\n",
    "species_change_single = species_change_single[\n",
    "    [\n",
    "        \"warmer\",\n",
    "        \"cooler\",\n",
    "        \"drier\",\n",
    "        \"wetter\",\n",
    "        \"temp_unclear\",\n",
    "        \"spei_unclear\",\n",
    "        \"sum_changes\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "species_change_single.head(3)\n",
    "\n",
    "# Both\n",
    "tmp_list = []\n",
    "for ispecies in df_spei_patterns.species.unique():\n",
    "    idf = df_responses.query(\"species == @ispecies\")\n",
    "    idf[\"group_size\"] = 1  # TODO: Ugly debug hack\n",
    "    idf = idf.groupby(\"change\").agg(group_size=(\"group_size\", \"sum\"))\n",
    "    df_pivot = (\n",
    "        idf.pivot_table(\n",
    "            index=lambda _: ispecies,\n",
    "            columns=\"change\",\n",
    "            values=\"group_size\",\n",
    "            aggfunc=\"first\",\n",
    "        )\n",
    "        .fillna(0)\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"change\": \"species\", \"other\": \"other\"})\n",
    "    )\n",
    "\n",
    "    tmp_list.append(df_pivot)\n",
    "\n",
    "species_change_both = pd.concat(tmp_list).reset_index(drop=True).fillna(0)\n",
    "\n",
    "# Check that all columns are present\n",
    "expected_columns = [\n",
    "    \"species\",\n",
    "    \"warmer_drier\",\n",
    "    \"warmer_wetter\",\n",
    "    \"other\",\n",
    "    \"cooler_drier\",\n",
    "    \"cooler_wetter\",\n",
    "]\n",
    "\n",
    "for col in expected_columns:\n",
    "    if col not in species_change_both.columns:\n",
    "        species_change_both[col] = 0  # Add missing column with 0s\n",
    "species_change_both[\"sum_changes_temp\"] = species_change_temp[\n",
    "    [\"warmer\", \"cooler\", \"temp_unclear\"]\n",
    "].sum(axis=1)\n",
    "\n",
    "# Set index to species to make it easier to sum\n",
    "species_change_both = species_change_both.set_index(\"species\")\n",
    "\n",
    "# Take rowwise sum\n",
    "species_change_both[\"sum_changes\"] = species_change_both.sum(axis=1)\n",
    "\n",
    "species_change_both = species_change_both[\n",
    "    [\n",
    "        \"warmer_drier\",\n",
    "        \"warmer_wetter\",\n",
    "        \"other\",\n",
    "        \"cooler_drier\",\n",
    "        \"cooler_wetter\",\n",
    "        \"sum_changes\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "species_change = species_change_both.copy()  # just to match the old code\n",
    "\n",
    "# ! Display\n",
    "display(\"Temperature\", species_change_temp.head(3))\n",
    "display(\"SPEI\", species_change_spei.head(3))\n",
    "display(\"Merged\", species_change_both.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check pattern distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pattern_dist(df_in, var_in, dir_patterns, do_return=False, do_plot=False):\n",
    "    # Make check what variables are needed\n",
    "    if var_in == \"change\":\n",
    "        var_in = \"change\"\n",
    "    elif var_in == \"response_spei\":\n",
    "        var_in = \"response_spei\"\n",
    "    elif var_in == \"response_temp\":\n",
    "        var_in = \"response_temp\"\n",
    "    else:\n",
    "        chime.error()\n",
    "        raise ValueError(\"🟥 var_in not recognized\")\n",
    "\n",
    "    # Sum up runs per group\n",
    "    df_in = df_in.groupby(var_in).agg(value=(\"group_size\", \"sum\")).reset_index()\n",
    "    # Take percentage and turn into int\n",
    "    df_in[\"value\"] = df_in[\"value\"] / df_in[\"value\"].sum() * 100\n",
    "    # Round to int\n",
    "    for i in range(df_in.shape[0]):\n",
    "\n",
    "        # Get value\n",
    "        x = df_in.loc[i, \"value\"].copy()\n",
    "        # Get second position\n",
    "        x2 = int(str(x).split(\".\")[1][0])\n",
    "        if x2 >= 5:\n",
    "            x = int(x) + 1\n",
    "        else:\n",
    "            x = int(x)\n",
    "        # Set value\n",
    "        df_in.loc[i, \"value\"] = x\n",
    "\n",
    "    df_in[\"value\"] = df_in[\"value\"].astype(int)\n",
    "\n",
    "    # Safety check\n",
    "    if df_in.value.sum() != 100:\n",
    "        # display(df_in)\n",
    "        print(\n",
    "            f\"🟥 Values do not sum up to 100% but to {df_in.value.sum()} because of rounding! Adjusting difference.\"\n",
    "        )\n",
    "        # Get difference between sum and 100\n",
    "        diff = 100 - df_in.value.sum()\n",
    "\n",
    "        # Check if difference is too large\n",
    "        if np.abs(diff) > 2:\n",
    "            chime.error()\n",
    "            raise ValueError(f\"🟥 Difference is too large: {diff}\")\n",
    "        else:\n",
    "            # Adjust largest group by difference (impact of adjusting by +- 1% is marginal)\n",
    "            # Get index of largest group\n",
    "            idx = df_in.value.idxmax()\n",
    "            old_value = df_in.loc[idx, \"value\"]\n",
    "            new_value = old_value + diff\n",
    "            # Adjust value\n",
    "            print(f\"🟢 Adjusting {old_value} by {diff} to {new_value}.\")\n",
    "            df_in.loc[idx, \"value\"] = new_value\n",
    "        # Safety check\n",
    "        if df_in.value.sum() != 100:\n",
    "            chime.error()\n",
    "            display(df_in)\n",
    "            print(\n",
    "                f\"🟥 Values STILL do not sum up to 100% but to {df_in.value.sum()} because of rounding!\"\n",
    "            )\n",
    "            raise\n",
    "\n",
    "    # Sort by percentage\n",
    "    df_in = df_in.sort_values(\"value\", ascending=True)\n",
    "    # Add percentage to label\n",
    "    df_in[var_in] = df_in[var_in] + \" (\" + df_in[\"value\"].astype(str) + \"%)\"\n",
    "    # Plot it\n",
    "    df_in.plot(kind=\"barh\", x=var_in, y=\"value\", color=\"grey\", legend=False)\n",
    "    plt.xlabel(\"Share of all runs (%)\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{dir_patterns}/change_counts_before_merging_unclear_{var_in}.png\")\n",
    "    if do_plot:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    df_in = df_in.sort_values(\"value\", ascending=False)\n",
    "    if do_return:\n",
    "        return df_in\n",
    "    display(df_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of patterns before merging\n",
    "plot_pattern_dist(df_both_patterns, \"change\", dir_patterns)\n",
    "plot_pattern_dist(df_spei_patterns, \"response_spei\", dir_patterns)\n",
    "plot_pattern_dist(df_temp_patterns, \"response_temp\", dir_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics per species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get runs per species and group\n",
    "df_runs_per_species = []\n",
    "\n",
    "for ispecies in df_responses.species.unique():\n",
    "    for igroup in df_responses.query(\"species == @ispecies\").group.unique():\n",
    "        idf = df_responses.query(\"species == @ispecies & group == @igroup\")\n",
    "        new_df = pd.DataFrame(\n",
    "            {\n",
    "                \"species\": ispecies,\n",
    "                \"group\": igroup,\n",
    "                \"n_runs\": idf.run.nunique(),\n",
    "                \"runs\": [idf.run.unique().tolist()],\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "        # idf = idf.groupby(\"species\").agg(n_runs=(\"run\", \"nunique\")).reset_index()\n",
    "        df_runs_per_species.append(new_df)\n",
    "\n",
    "df_runs_per_species = pd.concat(df_runs_per_species).reset_index(drop=True)\n",
    "display(df_runs_per_species.head(5))\n",
    "\n",
    "\n",
    "# Get runs per species\n",
    "df_runs_per_species_and_group = []\n",
    "\n",
    "for ispecies in df_responses.species.unique():\n",
    "    idf = df_responses.query(\"species == @ispecies\")\n",
    "    new_df = pd.DataFrame(\n",
    "        {\n",
    "            \"species\": ispecies,\n",
    "            \"n_runs\": idf.run.nunique(),\n",
    "            \"runs\": [idf.run.unique().tolist()],\n",
    "            \"n_groups\": idf.group.nunique(),\n",
    "            \"groups\": [idf.group.unique().tolist()],\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    # idf = idf.groupby(\"species\").agg(n_runs=(\"run\", \"nunique\")).reset_index()\n",
    "    df_runs_per_species_and_group.append(new_df)\n",
    "\n",
    "df_runs_per_species_and_group = pd.concat(df_runs_per_species_and_group).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "df_runs_per_species_and_group.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model runs\n",
    "df_tmp1 = df_all_concat.copy()\n",
    "df_tmp1[\"merge_var\"] = df_tmp1[\"subset_group\"].str.split(r\" \\(\").str[0]\n",
    "df_tmp1[\"merge_var\"] = df_tmp1[\"species\"] + \" - \" + df_tmp1[\"merge_var\"]\n",
    "\n",
    "df_tmp2 = df_responses.copy()\n",
    "df_tmp2[\"merge_var\"] = df_tmp2[\"species\"] + \" - \" + df_tmp2[\"run\"]\n",
    "\n",
    "df_filtered_raw = df_tmp2[[\"merge_var\", \"group\"]].merge(\n",
    "    df_tmp1, on=\"merge_var\", how=\"left\"\n",
    ")\n",
    "df_patterns_red = df_filtered_raw.drop(columns=[\"merge_var\"])\n",
    "\n",
    "# Loop over species and change and get model performance per run\n",
    "tmp_list = []\n",
    "for ispecies in df_patterns_red.species.unique():\n",
    "    # Get idf\n",
    "    idf = df_patterns_red.query(\"species == @ispecies\")\n",
    "    idf_runs = df_runs_per_species_and_group.query(\"species == @ispecies\")\n",
    "    # Safety check, not more than 50 runs\n",
    "    if idf.shape[0] > 50:\n",
    "        raise ValueError(\"🟥 More than 50 runs found\")\n",
    "    # Get roc values\n",
    "    roc_vals = idf[\"test_boot_mean\"].values\n",
    "    spei_vals = idf[\"SPEI - Importance\"].values\n",
    "    temp_vals = idf[\"Temperature - Importance\"].values\n",
    "    # Make new df\n",
    "    new_df = pd.DataFrame(\n",
    "        {\n",
    "            \"species\": ispecies,\n",
    "            \"n_runs\": idf_runs[\"n_runs\"].sum(),\n",
    "            \"runs\": [idf_runs[\"runs\"].values[0]],\n",
    "            \"mean_roc\": roc_vals.mean(),\n",
    "            \"std_roc\": roc_vals.std(),\n",
    "            \"mean_spei\": spei_vals.mean(),\n",
    "            \"std_spei\": spei_vals.std(),\n",
    "            \"mean_temp\": temp_vals.mean(),\n",
    "            \"std_temp\": temp_vals.std(),\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    # Append\n",
    "    tmp_list.append(new_df)\n",
    "\n",
    "# Concat\n",
    "df_patterns_red = pd.concat(tmp_list)\n",
    "# Add total importance for scaling\n",
    "df_patterns_red[\"spei+temp\"] = (\n",
    "    df_patterns_red[\"mean_spei\"] + df_patterns_red[\"mean_temp\"]\n",
    ")\n",
    "\n",
    "# Sort as in the species_change (needs to be inverted)\n",
    "species_order = species_change.index\n",
    "\n",
    "# Sort by species_order\n",
    "df_patterns_red = (\n",
    "    df_patterns_red.set_index(\"species\").reindex(species_order).reset_index()\n",
    ")\n",
    "\n",
    "df_patterns_red.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_mean = df_patterns_red[\"mean_roc\"].mean().round(2)\n",
    "roc_std = df_patterns_red[\"mean_roc\"].std().round(2)\n",
    "rocstd_mean = df_patterns_red[\"std_roc\"].mean().round(2)\n",
    "rocstd_std = df_patterns_red[\"std_roc\"].std().round(2)\n",
    "\n",
    "print(f\"ROC mean: {roc_mean} ± {roc_std}\")\n",
    "print(f\"ROC std mean: {rocstd_mean} ± {rocstd_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate pattern per species and group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean unclear merges\n",
    "df_both_patterns[\"change\"] = df_both_patterns[\"change\"].replace(\n",
    "    {\n",
    "        # SPEI_Temp\n",
    "        \"drier_other\": \"other\",\n",
    "        \"wetter_other\": \"other\",\n",
    "        \"other_warmer\": \"other\",\n",
    "        \"other_cooler\": \"other\",\n",
    "        \"other_other\": \"other\",\n",
    "        # Temp_SPEI\n",
    "        \"other_drier\": \"other\",\n",
    "        \"other_wetter\": \"other\",\n",
    "        \"warmer_other\": \"other\",\n",
    "        \"cooler_other\": \"other\",\n",
    "        # Other\n",
    "        \"unclear\": \"other\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Dissect feature names\n",
    "# Both patterns\n",
    "df_both_patterns[\"spei_duration\"] = (\n",
    "    df_both_patterns[\"feature_spei\"]\n",
    "    .str.split(\"-\")\n",
    "    .str[0]\n",
    "    .str.replace(\"spei\", \"\")\n",
    "    .astype(int)\n",
    ")\n",
    "df_both_patterns[\"spei_season\"] = (\n",
    "    df_both_patterns[\"feature_spei\"].str.split(\"-\").str[1].str.split(\"_\").str[0]\n",
    ")\n",
    "df_both_patterns[\"spei_anom\"] = df_both_patterns[\"feature_spei\"].str.split(\"_\").str[-1]\n",
    "\n",
    "df_both_patterns[\"temp_season\"] = df_both_patterns[\"feature_temp\"].str.split(\"_\").str[1]\n",
    "df_both_patterns[\"temp_metric\"] = df_both_patterns[\"feature_temp\"].str.split(\"_\").str[0]\n",
    "df_both_patterns[\"temp_anom\"] = df_both_patterns[\"feature_temp\"].str.split(\"_\").str[2]\n",
    "\n",
    "# SPEI only\n",
    "df_spei_patterns[\"spei_duration\"] = (\n",
    "    df_spei_patterns[\"feature_spei\"]\n",
    "    .str.split(\"-\")\n",
    "    .str[0]\n",
    "    .str.replace(\"spei\", \"\")\n",
    "    .astype(int)\n",
    ")\n",
    "df_spei_patterns[\"spei_season\"] = (\n",
    "    df_spei_patterns[\"feature_spei\"].str.split(\"-\").str[1].str.split(\"_\").str[0]\n",
    ")\n",
    "df_spei_patterns[\"spei_anom\"] = df_spei_patterns[\"feature_spei\"].str.split(\"_\").str[-1]\n",
    "\n",
    "# Temperature only\n",
    "df_temp_patterns[\"temp_season\"] = df_temp_patterns[\"feature_temp\"].str.split(\"_\").str[1]\n",
    "df_temp_patterns[\"temp_metric\"] = df_temp_patterns[\"feature_temp\"].str.split(\"_\").str[0]\n",
    "df_temp_patterns[\"temp_anom\"] = df_temp_patterns[\"feature_temp\"].str.split(\"_\").str[2]\n",
    "\n",
    "# Show\n",
    "display(df_both_patterns.head(5))\n",
    "display(df_spei_patterns.head(5))\n",
    "display(df_temp_patterns.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! DF FOR TABLES AND MATRICES\n",
    "# Settings\n",
    "turn_months_into_seasons = True\n",
    "\n",
    "# ! Processing spei-temp pairs ------------------------------------------------\n",
    "# Create simplified SPEI variable\n",
    "df_both_patterns[\"spei_simple\"] = (\n",
    "    df_both_patterns[\"spei_season\"]\n",
    "    + \"-\"\n",
    "    + df_both_patterns[\"spei_duration\"].astype(str)\n",
    ")\n",
    "\n",
    "df_both_patterns[\"spei_simpler\"] = df_both_patterns[\"spei_simple\"].replace(\n",
    "    {\n",
    "        \"-1$\": \"-short\",\n",
    "        \"-3$\": \"-short\",\n",
    "        \"-6$\": \"-medium\",\n",
    "        \"-9$\": \"-medium\",\n",
    "        \"-12$\": \"-medium\",\n",
    "        \"-15$\": \"-long\",\n",
    "        \"-18$\": \"-long\",\n",
    "        \"-21$\": \"-long\",\n",
    "        \"-24$\": \"-long\",\n",
    "    },\n",
    "    regex=True,\n",
    ")\n",
    "\n",
    "df_both_patterns[\"spei_duration_simple\"] = df_both_patterns[\"spei_duration\"].replace(\n",
    "    {\n",
    "        1: \"short\",\n",
    "        3: \"short\",\n",
    "        6: \"medium\",\n",
    "        9: \"medium\",\n",
    "        12: \"medium\",\n",
    "        15: \"long\",\n",
    "        18: \"long\",\n",
    "        21: \"long\",\n",
    "        24: \"long\",\n",
    "    },\n",
    "    regex=True,\n",
    ")\n",
    "\n",
    "df = df_both_patterns.sort_values(\n",
    "    [\n",
    "        \"species\",\n",
    "        \"change\",\n",
    "        \"spei_simple\",\n",
    "        \"temp_season\",\n",
    "    ]\n",
    ").copy()\n",
    "\n",
    "# Turn months into seasons\n",
    "if turn_months_into_seasons:\n",
    "    df = df.replace(\n",
    "        {\n",
    "            \"may\": \"spr\",\n",
    "            \"feb\": \"win\",\n",
    "            \"aug\": \"sum\",\n",
    "            \"nov\": \"aut\",\n",
    "        },\n",
    "        regex=True,\n",
    "    )\n",
    "\n",
    "# Clean temperature variables\n",
    "df = df.replace(\n",
    "    {\n",
    "        \"tmin\": \"min\",\n",
    "        \"tmoy\": \"mean\",\n",
    "        \"tmax\": \"max\",\n",
    "    },\n",
    "    regex=True,\n",
    ")\n",
    "\n",
    "# Attach group sizes\n",
    "tmp = (\n",
    "    df.groupby(\"species\")\n",
    "    .agg({\"group_size\": \"sum\"})\n",
    "    .reset_index()\n",
    "    .rename({\"group_size\": \"group_size_tot\"}, axis=1)\n",
    ")\n",
    "df = df.merge(tmp, on=\"species\", how=\"left\")\n",
    "df[\"group_size_rel_all\"] = df[\"group_size\"] / n_all_runs * 100\n",
    "df[\"group_size_rel_val\"] = df[\"group_size\"] / df[\"group_size_tot\"] * 100\n",
    "\n",
    "# ! Aggregation ------------------------------------------------\n",
    "# Get number of changes per species\n",
    "df_nchanges_per_species_2patterns = (\n",
    "    df.groupby(\"species\").agg(n_changes=(\"change\", f_nuniques)).reset_index()\n",
    ")\n",
    "\n",
    "# Extract variables and % of total runs\n",
    "df_list = []\n",
    "\n",
    "# Loop over all species\n",
    "for ispecies in df.species.unique():\n",
    "\n",
    "    # ispecies = \"Fagus sylvatica\"  # ! DEBUG\n",
    "\n",
    "    # Filter for species\n",
    "    idf = df.query(\"species == @ispecies\")\n",
    "\n",
    "    # display(idf)\n",
    "\n",
    "    # Loop over all change patterns\n",
    "    for ichange in change_order:\n",
    "        df_out = pd.DataFrame({\"species\": ispecies, \"change\": ichange}, index=[0])\n",
    "\n",
    "        # Loop over all features\n",
    "        for ifeature in [\n",
    "            \"spei_simple\",\n",
    "            \"spei_simpler\",\n",
    "            \"temp_season\",\n",
    "            \"feature_spei\",\n",
    "            \"feature_temp\",\n",
    "            \"spei_season\",\n",
    "            \"spei_duration\",\n",
    "            \"spei_duration_simple\",\n",
    "            \"spei_anom\",\n",
    "            \"temp_anom\",\n",
    "            \"temp_metric\",\n",
    "        ]:\n",
    "\n",
    "            # Filter for change\n",
    "            iidf = idf.query(\"change == @ichange\")\n",
    "\n",
    "            # display(ichange, iidf)\n",
    "\n",
    "            # Count runs per feature\n",
    "            iidf_agg = (\n",
    "                iidf.groupby([ifeature])\n",
    "                .agg(proportion=(\"group_size\", \"sum\"))\n",
    "                .reset_index()\n",
    "                .sort_values(\"proportion\", ascending=False)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            # Turn into percentages\n",
    "            # Either pick sum of all valid runs or n_all_runs\n",
    "            iidf_agg[\"proportion\"] = (\n",
    "                (iidf_agg[\"proportion\"] / iidf_agg[\"proportion\"].sum() * 100)\n",
    "                # (iidf_agg[\"proportion\"] / n_all_runs * 100)\n",
    "                .round(0).astype(int)\n",
    "            )\n",
    "\n",
    "            # display(iidf_agg)\n",
    "\n",
    "            # Add top 1 feature\n",
    "            # if iidf_agg.shape[0] == 0:\n",
    "            #     df_out[f\"best_{ifeature}\"] = \"NA\"\n",
    "            # else:\n",
    "            #     df_out[f\"best_{ifeature}\"] = iidf_agg.loc[0, ifeature]\n",
    "\n",
    "            # Loop over each group and create string\n",
    "            istring = \"\"\n",
    "            for i in range(iidf_agg.shape[0]):\n",
    "                if i == 0:\n",
    "                    istring = f\"{iidf_agg.loc[i, ifeature]} ({iidf_agg.loc[i, 'proportion']}%)\"\n",
    "\n",
    "                else:\n",
    "                    istring = (\n",
    "                        istring\n",
    "                        + \", \"\n",
    "                        + f\"{iidf_agg.loc[i, ifeature]} ({iidf_agg.loc[i, 'proportion']}%)\"\n",
    "                    )\n",
    "\n",
    "                df_out[ifeature] = istring\n",
    "                df_out[f\"best_{ifeature}\"] = iidf_agg.loc[0, ifeature]\n",
    "\n",
    "                # print(istring)\n",
    "                # display(i, \"iidf_agg\", iidf_agg)\n",
    "                # display(df_out)\n",
    "                # raise\n",
    "\n",
    "            # Count features per change\n",
    "            df_out[f\"n_features_{ifeature}\"] = iidf_agg.shape[0]\n",
    "            # Append df\n",
    "            df_list.append(df_out)\n",
    "\n",
    "        # if ispecies == \"Acer campestre\":\n",
    "        # display(pd.concat(df_list, axis=0))\n",
    "        # raise\n",
    "\n",
    "# Concatenate\n",
    "df_tmp = pd.concat(df_list, axis=0).drop_duplicates().reset_index(drop=True).dropna()\n",
    "\n",
    "# Add total number of runs\n",
    "tmp = species_change.drop(\"sum_changes\", axis=1).reset_index()\n",
    "tmp = pd.melt(\n",
    "    tmp,\n",
    "    id_vars=[\"species\"],\n",
    "    value_vars=tmp.columns,\n",
    "    var_name=\"change\",\n",
    "    value_name=\"nruns\",\n",
    ")\n",
    "df_tmp = df_tmp.merge(tmp, on=[\"species\", \"change\"], how=\"left\")\n",
    "\n",
    "# Add percentage of runs relative to all runs (including invalid)\n",
    "df_tmp[\"nruns_rel\"] = df_tmp[\"nruns\"] / n_all_runs * 100\n",
    "df_tmp[\"nruns_rel\"] = df_tmp[\"nruns_rel\"].round(0).astype(int)\n",
    "\n",
    "# Add percentage of runs relative to valid runs only\n",
    "df_valid_runs = (\n",
    "    df_tmp.groupby(\"species\")\n",
    "    .agg({\"nruns\": \"sum\"})\n",
    "    .rename(columns={\"nruns\": \"nruns_val\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_tmp = df_tmp.merge(df_valid_runs, on=\"species\")\n",
    "df_tmp[\"nruns_val_rel\"] = df_tmp[\"nruns\"] / df_tmp[\"nruns_val\"] * 100\n",
    "df_tmp[\"nruns_val_rel\"] = df_tmp[\"nruns_val_rel\"].round(0).astype(int)\n",
    "df_tmp[\"nruns_val\"] = df_tmp[\"nruns_val\"].round(0).astype(int)\n",
    "\n",
    "# Add genus and family\n",
    "tmp = (\n",
    "    df_top_species[[\"species\", \"genus_lat\", \"family_lat\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_tmp = df_tmp.merge(tmp, on=\"species\", how=\"left\")\n",
    "\n",
    "# Add ntrees\n",
    "df_tmp = df_tmp.merge(df_ntrees[[\"species\", \"n_trees_total\"]], on=\"species\", how=\"left\")\n",
    "\n",
    "# Rearrange cols\n",
    "df_tmp = move_vars_to_front(\n",
    "    df_tmp,\n",
    "    [\n",
    "        \"family_lat\",\n",
    "        \"genus_lat\",\n",
    "        \"species\",\n",
    "        \"change\",\n",
    "        \"nruns\",\n",
    "        \"nruns_rel\",\n",
    "        \"best_spei_simple\",\n",
    "        \"best_spei_simpler\",\n",
    "        \"best_temp_season\",\n",
    "        \"spei_simple\",\n",
    "        \"spei_simpler\",\n",
    "        \"temp_season\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Attach n_changes\n",
    "df_tmp = df_tmp.merge(df_nchanges_per_species_2patterns, on=\"species\", how=\"left\")\n",
    "\n",
    "# Overwrite best features to add percentages\n",
    "df_tmp[\"best_spei_simple\"] = df_tmp[\"spei_simple\"].str.split(\",\").str[0].str.strip()\n",
    "df_tmp[\"best_spei_simpler\"] = df_tmp[\"spei_simpler\"].str.split(\",\").str[0].str.strip()\n",
    "df_tmp[\"best_temp_season\"] = df_tmp[\"temp_season\"].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "# Add change percentage variable\n",
    "df_tmp[\"change_perc\"] = df_tmp[\"change\"] + \" (\" + df_tmp[\"nruns_rel\"].astype(str) + \"%)\"\n",
    "\n",
    "# Encode change\n",
    "df_tmp[\"change\"] = (\n",
    "    df_tmp[\"change\"].astype(\"category\").cat.set_categories(change_order, ordered=True)\n",
    ")\n",
    "\n",
    "# Rearrange cols\n",
    "df_occ = (\n",
    "    move_vars_to_front(\n",
    "        df_tmp,\n",
    "        [\n",
    "            \"species\",\n",
    "            \"family_lat\",\n",
    "            \"genus_lat\",\n",
    "            \"change_perc\",\n",
    "            \"best_spei_simple\",\n",
    "            \"best_spei_simpler\",\n",
    "            \"best_temp_season\",\n",
    "            \"n_changes\",\n",
    "            \"n_trees_total\",\n",
    "        ],\n",
    "    )\n",
    "    .sort_values([\"n_trees_total\", \"change\"], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Processing spei only ------------------------------------------------\n",
    "# Create simplified SPEI variable\n",
    "df_spei_patterns[\"spei_simple\"] = (\n",
    "    df_spei_patterns[\"spei_season\"]\n",
    "    + \"-\"\n",
    "    + df_spei_patterns[\"spei_duration\"].astype(str)\n",
    ")\n",
    "\n",
    "df_spei_patterns[\"spei_simpler\"] = df_spei_patterns[\"spei_simple\"].replace(\n",
    "    {\n",
    "        \"-1$\": \"-short\",\n",
    "        \"-3$\": \"-short\",\n",
    "        \"-6$\": \"-medium\",\n",
    "        \"-9$\": \"-medium\",\n",
    "        \"-12$\": \"-medium\",\n",
    "        \"-15$\": \"-long\",\n",
    "        \"-18$\": \"-long\",\n",
    "        \"-21$\": \"-long\",\n",
    "        \"-24$\": \"-long\",\n",
    "    },\n",
    "    regex=True,\n",
    ")\n",
    "\n",
    "df_spei_patterns[\"spei_duration_simple\"] = df_spei_patterns[\"spei_duration\"].replace(\n",
    "    {\n",
    "        1: \"short\",\n",
    "        3: \"short\",\n",
    "        6: \"medium\",\n",
    "        9: \"medium\",\n",
    "        12: \"medium\",\n",
    "        15: \"long\",\n",
    "        18: \"long\",\n",
    "        21: \"long\",\n",
    "        24: \"long\",\n",
    "    },\n",
    "    regex=True,\n",
    ")\n",
    "\n",
    "df_spei_patterns_red = df_spei_patterns.sort_values(\n",
    "    [\n",
    "        \"species\",\n",
    "        \"response_spei\",\n",
    "        \"spei_simple\",\n",
    "    ]\n",
    ").copy()\n",
    "\n",
    "# Turn months into seasons\n",
    "if turn_months_into_seasons:\n",
    "    df_spei_patterns_red = df_spei_patterns_red.replace(\n",
    "        {\n",
    "            \"may\": \"spr\",\n",
    "            \"feb\": \"win\",\n",
    "            \"aug\": \"sum\",\n",
    "            \"nov\": \"aut\",\n",
    "        },\n",
    "        regex=True,\n",
    "    )\n",
    "\n",
    "# Attach group sizes\n",
    "tmp = (\n",
    "    df_spei_patterns_red.groupby(\"species\")\n",
    "    .agg({\"group_size\": \"sum\"})\n",
    "    .reset_index()\n",
    "    .rename({\"group_size\": \"group_size_tot\"}, axis=1)\n",
    ")\n",
    "df_spei_patterns_red = df_spei_patterns_red.merge(tmp, on=\"species\", how=\"left\")\n",
    "df_spei_patterns_red[\"group_size_rel_all\"] = (\n",
    "    df_spei_patterns_red[\"group_size\"] / n_all_runs * 100\n",
    ")\n",
    "df_spei_patterns_red[\"group_size_rel_val\"] = (\n",
    "    df_spei_patterns_red[\"group_size\"] / df_spei_patterns_red[\"group_size_tot\"] * 100\n",
    ")\n",
    "\n",
    "# ! Aggregation ------------------------------------------------\n",
    "# Get number of changes per species\n",
    "df_nchanges_per_species_2patterns = (\n",
    "    df_spei_patterns_red.groupby(\"species\")\n",
    "    .agg(n_changes=(\"response_spei\", f_nuniques))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Extract variables and % of total runs\n",
    "df_list = []\n",
    "\n",
    "# Loop over all species\n",
    "for ispecies in df.species.unique():\n",
    "\n",
    "    # ispecies = \"Fagus sylvatica\"  # ! DEBUG\n",
    "\n",
    "    # Filter for species\n",
    "    idf = df_spei_patterns_red.query(\"species == @ispecies\")\n",
    "\n",
    "    # display(idf)\n",
    "\n",
    "    # Loop over all change patterns\n",
    "    for ichange in df_spei_patterns_red.response_spei.unique():\n",
    "        df_out = pd.DataFrame(\n",
    "            {\"species\": ispecies, \"response_spei\": ichange}, index=[0]\n",
    "        )\n",
    "\n",
    "        # Loop over all features\n",
    "        for ifeature in [\n",
    "            \"spei_simple\",\n",
    "            \"spei_simpler\",\n",
    "            \"feature_spei\",\n",
    "            \"spei_season\",\n",
    "            \"spei_duration\",\n",
    "            \"spei_duration_simple\",\n",
    "            \"spei_anom\",\n",
    "        ]:\n",
    "\n",
    "            # Filter for change\n",
    "            iidf = idf.query(\"response_spei == @ichange\")\n",
    "\n",
    "            # display(ichange, iidf)\n",
    "\n",
    "            # Count runs per feature\n",
    "            iidf_agg = (\n",
    "                iidf.groupby([ifeature])\n",
    "                .agg(proportion=(\"group_size\", \"sum\"))\n",
    "                .reset_index()\n",
    "                .sort_values(\"proportion\", ascending=False)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            # Turn into percentages\n",
    "            # Either pick sum of all valid runs or n_all_runs\n",
    "            iidf_agg[\"proportion\"] = (\n",
    "                (iidf_agg[\"proportion\"] / iidf_agg[\"proportion\"].sum() * 100)\n",
    "                # (iidf_agg[\"proportion\"] / n_all_runs * 100)\n",
    "                .round(0).astype(int)\n",
    "            )\n",
    "\n",
    "            # display(iidf_agg)\n",
    "\n",
    "            # Add top 1 feature\n",
    "            # if iidf_agg.shape[0] == 0:\n",
    "            #     df_out[f\"best_{ifeature}\"] = \"NA\"\n",
    "            # else:\n",
    "            #     df_out[f\"best_{ifeature}\"] = iidf_agg.loc[0, ifeature]\n",
    "\n",
    "            # Loop over each group and create string\n",
    "            istring = \"\"\n",
    "            for i in range(iidf_agg.shape[0]):\n",
    "                if i == 0:\n",
    "                    istring = f\"{iidf_agg.loc[i, ifeature]} ({iidf_agg.loc[i, 'proportion']}%)\"\n",
    "\n",
    "                else:\n",
    "                    istring = (\n",
    "                        istring\n",
    "                        + \", \"\n",
    "                        + f\"{iidf_agg.loc[i, ifeature]} ({iidf_agg.loc[i, 'proportion']}%)\"\n",
    "                    )\n",
    "\n",
    "                df_out[ifeature] = istring\n",
    "                df_out[f\"best_{ifeature}\"] = iidf_agg.loc[0, ifeature]\n",
    "\n",
    "                # print(istring)\n",
    "                # display(i, \"iidf_agg\", iidf_agg)\n",
    "                # display(df_out)\n",
    "                # raise\n",
    "\n",
    "            # Count features per change\n",
    "            df_out[f\"n_features_{ifeature}\"] = iidf_agg.shape[0]\n",
    "            # Append df\n",
    "            df_list.append(df_out)\n",
    "\n",
    "        # if ispecies == \"Acer campestre\":\n",
    "        # display(pd.concat(df_list, axis=0))\n",
    "        # raise\n",
    "\n",
    "# Concatenate\n",
    "df_tmp = pd.concat(df_list, axis=0).drop_duplicates().reset_index(drop=True).dropna()\n",
    "\n",
    "# Add total number of runs\n",
    "tmp = (\n",
    "    species_change_spei.rename(\n",
    "        {\"spei_unclear\": \"other\", \"sum_changes_spei\": \"sum_changes\"}, axis=1\n",
    "    )\n",
    "    .drop(\"sum_changes\", axis=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "tmp = pd.melt(\n",
    "    tmp,\n",
    "    id_vars=[\"species\"],\n",
    "    value_vars=tmp.columns,\n",
    "    var_name=\"response_spei\",\n",
    "    value_name=\"nruns\",\n",
    ")\n",
    "df_tmp = df_tmp.merge(tmp, on=[\"species\", \"response_spei\"], how=\"left\")\n",
    "\n",
    "# Add percentage of runs relative to all runs (including invalid)\n",
    "df_tmp[\"nruns_rel\"] = df_tmp[\"nruns\"] / n_all_runs * 100\n",
    "df_tmp[\"nruns_rel\"] = df_tmp[\"nruns_rel\"].round(0).astype(int)\n",
    "\n",
    "# Add percentage of runs relative to valid runs only\n",
    "df_valid_runs = (\n",
    "    df_tmp.groupby(\"species\")\n",
    "    .agg({\"nruns\": \"sum\"})\n",
    "    .rename(columns={\"nruns\": \"nruns_val\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_tmp = df_tmp.merge(df_valid_runs, on=\"species\")\n",
    "df_tmp[\"nruns_val_rel\"] = df_tmp[\"nruns\"] / df_tmp[\"nruns_val\"] * 100\n",
    "df_tmp[\"nruns_val_rel\"] = df_tmp[\"nruns_val_rel\"].round(0).astype(int)\n",
    "df_tmp[\"nruns_val\"] = df_tmp[\"nruns_val\"].round(0).astype(int)\n",
    "\n",
    "# Add genus and family\n",
    "tmp = (\n",
    "    df_top_species[[\"species\", \"genus_lat\", \"family_lat\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_tmp = df_tmp.merge(tmp, on=\"species\", how=\"left\")\n",
    "\n",
    "# Add ntrees\n",
    "df_tmp = df_tmp.merge(df_ntrees[[\"species\", \"n_trees_total\"]], on=\"species\", how=\"left\")\n",
    "\n",
    "# Rearrange cols\n",
    "df_tmp = move_vars_to_front(\n",
    "    df_tmp,\n",
    "    [\n",
    "        \"family_lat\",\n",
    "        \"genus_lat\",\n",
    "        \"species\",\n",
    "        \"response_spei\",\n",
    "        \"nruns\",\n",
    "        \"nruns_rel\",\n",
    "        \"best_spei_simple\",\n",
    "        \"best_spei_simpler\",\n",
    "        \"spei_simple\",\n",
    "        \"spei_simpler\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Attach n_changes\n",
    "df_tmp = df_tmp.merge(df_nchanges_per_species_2patterns, on=\"species\", how=\"left\")\n",
    "\n",
    "# Overwrite best features to add percentages\n",
    "df_tmp[\"best_spei_simple\"] = df_tmp[\"spei_simple\"].str.split(\",\").str[0].str.strip()\n",
    "df_tmp[\"best_spei_simpler\"] = df_tmp[\"spei_simpler\"].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "# Add change percentage variable\n",
    "df_tmp[\"change_perc\"] = (\n",
    "    df_tmp[\"response_spei\"] + \" (\" + df_tmp[\"nruns_rel\"].astype(str) + \"%)\"\n",
    ")\n",
    "\n",
    "# Encode change\n",
    "df_tmp[\"response_spei\"] = (\n",
    "    df_tmp[\"response_spei\"]\n",
    "    .astype(\"category\")\n",
    "    .cat.set_categories(change_order, ordered=True)\n",
    ")\n",
    "\n",
    "# Rearrange cols\n",
    "df_occ_spei = (\n",
    "    move_vars_to_front(\n",
    "        df_tmp,\n",
    "        [\n",
    "            \"species\",\n",
    "            \"family_lat\",\n",
    "            \"genus_lat\",\n",
    "            \"change_perc\",\n",
    "            \"best_spei_simple\",\n",
    "            \"best_spei_simpler\",\n",
    "            \"n_changes\",\n",
    "            \"n_trees_total\",\n",
    "        ],\n",
    "    )\n",
    "    .sort_values([\"n_trees_total\", \"response_spei\"], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Processing temp only ------------------------------------------------\n",
    "df_temp_patterns_red = df_temp_patterns.sort_values(\n",
    "    [\n",
    "        \"species\",\n",
    "        \"response_temp\",\n",
    "        \"temp_season\",\n",
    "    ]\n",
    ").copy()\n",
    "\n",
    "# Turn months into seasons\n",
    "if turn_months_into_seasons:\n",
    "    df_temp_patterns_red = df_temp_patterns_red.replace(\n",
    "        {\n",
    "            \"may\": \"spr\",\n",
    "            \"feb\": \"win\",\n",
    "            \"aug\": \"sum\",\n",
    "            \"nov\": \"aut\",\n",
    "        },\n",
    "        regex=True,\n",
    "    )\n",
    "\n",
    "# Clean temperature variables\n",
    "df_temp_patterns_red = df_temp_patterns_red.replace(\n",
    "    {\n",
    "        \"tmin\": \"min\",\n",
    "        \"tmoy\": \"mean\",\n",
    "        \"tmax\": \"max\",\n",
    "    },\n",
    "    regex=True,\n",
    ")\n",
    "\n",
    "# Attach group sizes\n",
    "tmp = (\n",
    "    df_temp_patterns_red.groupby(\"species\")\n",
    "    .agg({\"group_size\": \"sum\"})\n",
    "    .reset_index()\n",
    "    .rename({\"group_size\": \"group_size_tot\"}, axis=1)\n",
    ")\n",
    "df_temp_patterns_red = df_temp_patterns_red.merge(tmp, on=\"species\", how=\"left\")\n",
    "df_temp_patterns_red[\"group_size_rel_all\"] = (\n",
    "    df_temp_patterns_red[\"group_size\"] / n_all_runs * 100\n",
    ")\n",
    "df_temp_patterns_red[\"group_size_rel_val\"] = (\n",
    "    df_temp_patterns_red[\"group_size\"] / df_temp_patterns_red[\"group_size_tot\"] * 100\n",
    ")\n",
    "\n",
    "# ! Aggregation ------------------------------------------------\n",
    "# Get number of changes per species\n",
    "df_nchanges_per_species_2patterns = (\n",
    "    df_temp_patterns_red.groupby(\"species\")\n",
    "    .agg(n_changes=(\"response_temp\", f_nuniques))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Extract variables and % of total runs\n",
    "df_list = []\n",
    "\n",
    "# Loop over all species\n",
    "for ispecies in df.species.unique():\n",
    "\n",
    "    # ispecies = \"Fagus sylvatica\"  # ! DEBUG\n",
    "\n",
    "    # Filter for species\n",
    "    idf = df_temp_patterns_red.query(\"species == @ispecies\")\n",
    "\n",
    "    # display(idf)\n",
    "\n",
    "    # Loop over all change patterns\n",
    "    for ichange in df_temp_patterns_red[\"response_temp\"].unique():\n",
    "        df_out = pd.DataFrame(\n",
    "            {\"species\": ispecies, \"response_temp\": ichange}, index=[0]\n",
    "        )\n",
    "\n",
    "        # Loop over all features\n",
    "        for ifeature in [\n",
    "            \"temp_season\",\n",
    "            \"temp_metric\",\n",
    "            \"temp_anom\",\n",
    "            \"feature_temp\",\n",
    "        ]:\n",
    "\n",
    "            # Filter for change\n",
    "            iidf = idf.query(\"response_temp == @ichange\")\n",
    "\n",
    "            # display(ichange, iidf)\n",
    "\n",
    "            # Count runs per feature\n",
    "            iidf_agg = (\n",
    "                iidf.groupby([ifeature])\n",
    "                .agg(proportion=(\"group_size\", \"sum\"))\n",
    "                .reset_index()\n",
    "                .sort_values(\"proportion\", ascending=False)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "            # Turn into percentages\n",
    "            # Either pick sum of all valid runs or n_all_runs\n",
    "            iidf_agg[\"proportion\"] = (\n",
    "                (iidf_agg[\"proportion\"] / iidf_agg[\"proportion\"].sum() * 100)\n",
    "                # (iidf_agg[\"proportion\"] / n_all_runs * 100)\n",
    "                .round(0).astype(int)\n",
    "            )\n",
    "\n",
    "            # display(iidf_agg)\n",
    "\n",
    "            # Add top 1 feature\n",
    "            # if iidf_agg.shape[0] == 0:\n",
    "            #     df_out[f\"best_{ifeature}\"] = \"NA\"\n",
    "            # else:\n",
    "            #     df_out[f\"best_{ifeature}\"] = iidf_agg.loc[0, ifeature]\n",
    "\n",
    "            # Loop over each group and create string\n",
    "            istring = \"\"\n",
    "            for i in range(iidf_agg.shape[0]):\n",
    "                if i == 0:\n",
    "                    istring = f\"{iidf_agg.loc[i, ifeature]} ({iidf_agg.loc[i, 'proportion']}%)\"\n",
    "\n",
    "                else:\n",
    "                    istring = (\n",
    "                        istring\n",
    "                        + \", \"\n",
    "                        + f\"{iidf_agg.loc[i, ifeature]} ({iidf_agg.loc[i, 'proportion']}%)\"\n",
    "                    )\n",
    "\n",
    "                df_out[ifeature] = istring\n",
    "                df_out[f\"best_{ifeature}\"] = iidf_agg.loc[0, ifeature]\n",
    "\n",
    "                # print(istring)\n",
    "                # display(i, \"iidf_agg\", iidf_agg)\n",
    "                # display(df_out)\n",
    "                # raise\n",
    "\n",
    "            # Count features per change\n",
    "            df_out[f\"n_features_{ifeature}\"] = iidf_agg.shape[0]\n",
    "            # Append df\n",
    "            df_list.append(df_out)\n",
    "\n",
    "        # if ispecies == \"Acer campestre\":\n",
    "        # display(pd.concat(df_list, axis=0))\n",
    "        # raise\n",
    "\n",
    "# Concatenate\n",
    "df_tmp = pd.concat(df_list, axis=0).drop_duplicates().reset_index(drop=True).dropna()\n",
    "\n",
    "# Add total number of runs\n",
    "tmp = (\n",
    "    species_change_temp.rename(\n",
    "        {\"temp_unclear\": \"other\", \"sum_changes_temp\": \"sum_changes\"}, axis=1\n",
    "    )\n",
    "    .drop(\"sum_changes\", axis=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "tmp = pd.melt(\n",
    "    tmp,\n",
    "    id_vars=[\"species\"],\n",
    "    value_vars=tmp.columns,\n",
    "    var_name=\"response_temp\",\n",
    "    value_name=\"nruns\",\n",
    ")\n",
    "df_tmp = df_tmp.merge(tmp, on=[\"species\", \"response_temp\"], how=\"left\")\n",
    "\n",
    "# Add percentage of runs relative to all runs (including invalid)\n",
    "df_tmp[\"nruns_rel\"] = df_tmp[\"nruns\"] / n_all_runs * 100\n",
    "df_tmp[\"nruns_rel\"] = df_tmp[\"nruns_rel\"].round(0).astype(int)\n",
    "\n",
    "# Add percentage of runs relative to valid runs only\n",
    "df_valid_runs = (\n",
    "    df_tmp.groupby(\"species\")\n",
    "    .agg({\"nruns\": \"sum\"})\n",
    "    .rename(columns={\"nruns\": \"nruns_val\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_tmp = df_tmp.merge(df_valid_runs, on=\"species\")\n",
    "df_tmp[\"nruns_val_rel\"] = df_tmp[\"nruns\"] / df_tmp[\"nruns_val\"] * 100\n",
    "df_tmp[\"nruns_val_rel\"] = df_tmp[\"nruns_val_rel\"].round(0).astype(int)\n",
    "df_tmp[\"nruns_val\"] = df_tmp[\"nruns_val\"].round(0).astype(int)\n",
    "\n",
    "# Add genus and family\n",
    "tmp = (\n",
    "    df_top_species[[\"species\", \"genus_lat\", \"family_lat\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_tmp = df_tmp.merge(tmp, on=\"species\", how=\"left\")\n",
    "\n",
    "# Add ntrees\n",
    "df_tmp = df_tmp.merge(df_ntrees[[\"species\", \"n_trees_total\"]], on=\"species\", how=\"left\")\n",
    "\n",
    "# Rearrange cols\n",
    "df_tmp = move_vars_to_front(\n",
    "    df_tmp,\n",
    "    [\n",
    "        \"family_lat\",\n",
    "        \"genus_lat\",\n",
    "        \"species\",\n",
    "        \"response_temp\",\n",
    "        \"nruns\",\n",
    "        \"nruns_rel\",\n",
    "        \"best_temp_season\",\n",
    "        \"temp_season\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Attach n_changes\n",
    "df_tmp = df_tmp.merge(df_nchanges_per_species_2patterns, on=\"species\", how=\"left\")\n",
    "\n",
    "# Overwrite best features to add percentages\n",
    "df_tmp[\"best_temp_season\"] = df_tmp[\"temp_season\"].str.split(\",\").str[0].str.strip()\n",
    "\n",
    "# Add change percentage variable\n",
    "df_tmp[\"change_perc\"] = (\n",
    "    df_tmp[\"response_temp\"] + \" (\" + df_tmp[\"nruns_rel\"].astype(str) + \"%)\"\n",
    ")\n",
    "\n",
    "# Encode change\n",
    "df_tmp[\"response_temp\"] = (\n",
    "    df_tmp[\"response_temp\"]\n",
    "    .astype(\"category\")\n",
    "    .cat.set_categories(change_order, ordered=True)\n",
    ")\n",
    "\n",
    "# Rearrange cols\n",
    "df_occ_temp = (\n",
    "    move_vars_to_front(\n",
    "        df_tmp,\n",
    "        [\n",
    "            \"species\",\n",
    "            \"family_lat\",\n",
    "            \"genus_lat\",\n",
    "            \"change_perc\",\n",
    "            \"best_temp_season\",\n",
    "            \"best_temp_anom\",\n",
    "            \"n_changes\",\n",
    "            \"n_trees_total\",\n",
    "        ],\n",
    "    )\n",
    "    .sort_values([\"n_trees_total\", \"response_temp\"], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_occ_temp.species.nunique())\n",
    "# display(df_occ_spei.species.nunique())\n",
    "# display(df_occ.species.nunique())\n",
    "\n",
    "# display(df_occ_temp.head(3))\n",
    "# display(df_occ_spei.head(3))\n",
    "# display(df_occ.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot-Specific DFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pie_charts = df.copy()\n",
    "df_for_pie_charts_single_temp = df_for_pie_charts.drop(columns=[\"change\"]).rename(\n",
    "    columns={\"response_temp\": \"change\"}\n",
    ")\n",
    "df_for_pie_charts_single_spei = df_for_pie_charts.drop(columns=[\"change\"]).rename(\n",
    "    columns={\"response_spei\": \"change\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dfs\n",
    "df_plot = df_patterns_red.merge(species_change, on=\"species\")\n",
    "\n",
    "# Attach number of trees\n",
    "df_plot = df_plot.merge(df_ntrees, on=\"species\")\n",
    "\n",
    "# Reduce to top 20 if needed\n",
    "if top_only:\n",
    "    df_plot = df_plot.query(\"species in @top9\")\n",
    "\n",
    "# Fix sorting\n",
    "if sort_by == \"change\":  # change - species - runs - performance\n",
    "    if normalized_change_chart:\n",
    "        sort_list = change_order + [\"species\"]\n",
    "        tf_list = ([True] * len(change_order)) + [False]\n",
    "    else:\n",
    "        sort_list = [\"sum_changes\"] + change_order + [\"species\"]\n",
    "        tf_list = [True] + ([True] * len(change_order)) + [False]\n",
    "        sort_list = change_order + [\"species\"]\n",
    "        tf_list = ([True] * len(change_order)) + [False]\n",
    "\n",
    "    df_plot = df_plot.sort_values(sort_list, ascending=tf_list)\n",
    "\n",
    "elif sort_by == \"species\":\n",
    "    df_plot = df_plot.sort_values([\"species\", \"species\"], ascending=[False, False])\n",
    "elif sort_by == \"nruns\":\n",
    "    df_plot = df_plot.sort_values([\"sum_changes\", \"species\"], ascending=[True, False])\n",
    "elif sort_by == \"performance\":\n",
    "    df_plot = df_plot.sort_values([\"mean_roc\", \"species\"], ascending=[True, False])\n",
    "elif sort_by == \"importance\":\n",
    "    df_plot = df_plot.sort_values([\"spei+temp\", \"species\"], ascending=[True, False])\n",
    "elif sort_by == \"ntrees\":\n",
    "    df_plot = df_plot.sort_values([\"n_trees_total\", \"species\"], ascending=[True, False])\n",
    "else:\n",
    "    chime.error()\n",
    "    raise ValueError(f\"🟥 sort_by not recognized: {sort_by}\")\n",
    "\n",
    "# Show\n",
    "df_plot_from_mulitplot = (\n",
    "    df_plot.copy().sort_values(\"n_trees_total\", ascending=True).reset_index(drop=True)\n",
    ")\n",
    "df_plot_from_mulitplot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reduced version too\n",
    "df_occ_red = df_occ.copy()\n",
    "\n",
    "# Set order of change variable\n",
    "df_occ_red[\"change\"] = pd.Categorical(\n",
    "    df_occ_red[\"change\"],\n",
    "    categories=[\n",
    "        \"warmer_drier\",\n",
    "        \"warmer_wetter\",\n",
    "        \"other\",\n",
    "        \"cooler_wetter\",\n",
    "        \"cooler_drier\",\n",
    "    ],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Attach mean spei and temp\n",
    "df_occ_red = df_occ_red.merge(\n",
    "    df_plot[[\"species\", \"mean_spei\", \"mean_temp\", \"spei+temp\"]],\n",
    "    on=\"species\",\n",
    "    how=\"left\",\n",
    ")\n",
    "df_occ_red[\"temp_spei_ratio\"] = df_occ_red[\"mean_temp\"] / df_occ_red[\"mean_spei\"]\n",
    "\n",
    "df_occ_red.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species-Level Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Importance per Species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance_and_feat(df_in, category):\n",
    "\n",
    "    feat = (\n",
    "        idf[f\"{category} - Metrics\"].value_counts().index[0].split(\"'\")[1].split(\"'\")[0]\n",
    "    )\n",
    "    mean = idf[f\"{category} - Importance\"].dropna().values.mean().round(2)\n",
    "    std = idf[f\"{category} - Importance\"].dropna().values.std().round(2)\n",
    "\n",
    "    return feat, mean, std\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# For supplementary table\n",
    "tab_supp_features = df_filtered_raw.drop(columns=[\"merge_var\"])\n",
    "\n",
    "# Loop over species and change and get model performance per run\n",
    "tmp_list = []\n",
    "fig_list = []\n",
    "for ispecies in tab_supp_features.species.unique():\n",
    "    # Get idf\n",
    "    idf = tab_supp_features.query(\"species == @ispecies\")\n",
    "    idf_runs = df_runs_per_species_and_group.query(\"species == @ispecies\")\n",
    "    # Safety check, not more than 50 runs\n",
    "    if idf.shape[0] > 50:\n",
    "        raise ValueError(\"🟥 More than 50 runs found\")\n",
    "    # Make new df\n",
    "    mean_roc = idf[\"test_boot_mean\"].values.mean().round(2)\n",
    "    std_roc = idf[\"test_boot_mean\"].values.std().round(2)\n",
    "    new_df = pd.DataFrame(\n",
    "        {\n",
    "            \"species\": ispecies,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    new_df_fig = new_df.copy()\n",
    "\n",
    "    new_df[\"ROC-AUC\"] = f\"{mean_roc} +- {std_roc}\"\n",
    "\n",
    "    # Get mean feature importance and most common feature\n",
    "    for cat in [\n",
    "        \"Tree Size\",\n",
    "        \"Light Competition\",\n",
    "        \"Stand Structure\",\n",
    "        \"Species Competition\",\n",
    "        \"Temperature\",\n",
    "        \"SPEI\",\n",
    "        \"Topography\",\n",
    "        \"Soil Water Conditions\",\n",
    "        \"Soil Fertility\",\n",
    "        \"Management\",\n",
    "        \"NDVI\",\n",
    "    ]:\n",
    "\n",
    "        feat, mean, std = get_importance_and_feat(idf, cat)\n",
    "        # new_df[f\"{cat} METRIC\"] = f\"{mean} +- {std}\"\n",
    "        # new_df[f\"{cat} FEATURE\"] = f\"{feat}\"\n",
    "        # new_df[f\"{cat} FULL\"] = f\"{mean} +- {std} ({feat})\"\n",
    "        new_df[f\"{cat}\"] = f\"{mean} +- {std} ({feat})\"\n",
    "        new_df_fig[f\"{cat}\"] = mean\n",
    "        new_df[f\"N Models\"] = idf.query(\"test_boot_mean > @roc_threshold\").shape[0]\n",
    "        new_df = move_vars_to_front(new_df, [\"N Models\", \"ROC-AUC\"])\n",
    "\n",
    "    # Append\n",
    "    tmp_list.append(new_df)\n",
    "    fig_list.append(new_df_fig)\n",
    "\n",
    "# ! Concat\n",
    "tab_supp_features = pd.concat(tmp_list)\n",
    "# Add total importance for scaling\n",
    "# tab_supp_features[\"spei+temp\"] = (\n",
    "# tab_supp_features[\"mean_spei\"] + tab_supp_features[\"mean_temp\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# Sort alphabetically by species_order\n",
    "species_order = species_change.index\n",
    "tab_supp_features = (\n",
    "    tab_supp_features.set_index(\"species\").reindex(species_order).reset_index()\n",
    ")\n",
    "\n",
    "# ! Report\n",
    "tmp_t9 = tab_supp_features.query(\"species in @top9\")\n",
    "nmodels_top9 = tmp_t9[\"N Models\"].sum()\n",
    "percmodels_top9 = nmodels_top9 / (9 * 50) * 100\n",
    "\n",
    "tmp52 = tab_supp_features.copy()\n",
    "nmodels_52 = tmp52[\"N Models\"].sum()\n",
    "percmodels_52 = nmodels_52 / (52 * 50) * 100\n",
    "\n",
    "print(\n",
    "    f\"Keeping ROC-AUC > {roc_threshold}: {nmodels_52} / {52*50} models ({percmodels_52:.1f}%) of the 52 species were used in the final analysis (= {100-percmodels_52:.1f}% were removed).\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Keeping ROC-AUC > {roc_threshold}: {nmodels_top9} / {9*50} models ({percmodels_top9:.1f}%) of the top 9 species were used in the final analysis (= {100-percmodels_top9:.1f}% were removed).\"\n",
    ")\n",
    "\n",
    "# display(tab_supp_features.head(5))\n",
    "\n",
    "# ! Save\n",
    "# Attach number of runs above\n",
    "tab_supp_features.to_csv(f\"{dir_tables}/category_importance.csv\", index=False)\n",
    "\n",
    "# Make sure per species, the sum of all importances is 100, by scaling\n",
    "df_for_import_fig = pd.concat(fig_list).set_index(\"species\")\n",
    "df_for_import_fig = df_for_import_fig.div(df_for_import_fig.sum(axis=1), axis=0) * 100\n",
    "df_for_import_fig = df_for_import_fig.reset_index()\n",
    "df = df_for_import_fig.copy()\n",
    "\n",
    "species = df[\"species\"][::-1]\n",
    "categorys = df.columns[1:]  # Exclude 'species'\n",
    "values = df[categorys].to_numpy()\n",
    "colors_cat = {\n",
    "    # Forest Dynamics\n",
    "    \"Tree Size\": \"#8e4f09\",\n",
    "    \"Light Competition\": \"#a8703c\",\n",
    "    \"Stand Structure\": \"#c0926a\",\n",
    "    \"Species Competition\": \"#d7b59a\",\n",
    "    # Climate\n",
    "    \"Temperature\": \"#D14916\",\n",
    "    \"SPEI\": \"#f09c7b\",\n",
    "    # Topography\n",
    "    \"Topography\": \"#707070\",\n",
    "    # Soil\n",
    "    \"Soil Water Conditions\": \"#4e7ac7\",\n",
    "    \"Soil Fertility\": \"#7693d3\",\n",
    "    # Management\n",
    "    \"Management\": \"#33691e\",\n",
    "    # NDVI\n",
    "    \"NDVI\": \"#FFC412\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# ! Define bottom for stacking\n",
    "bottom = np.zeros(len(species))\n",
    "\n",
    "# Loop through each categorys to stack the bars\n",
    "for idx, category in enumerate(categorys):\n",
    "    ax.barh(\n",
    "        species,\n",
    "        values[:, idx],\n",
    "        label=category,\n",
    "        left=bottom,\n",
    "        color=colors_cat[category],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        height=1,\n",
    "    )\n",
    "    bottom += values[:, idx]  # Update bottom for stacking\n",
    "\n",
    "ax.legend(title=\"Feature Category\", bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "ax.set_ylim(-0.5, 51.45)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_xlabel(\"Importance (%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of runs per change\n",
    "# ! For all species\n",
    "tmp_both = plot_pattern_dist(\n",
    "    df_both_patterns, \"change\", dir_patterns, do_return=True\n",
    ").rename(columns={\"value\": \"group_size_rel_1\"})\n",
    "tmp_both[\"change_clean\"] = tmp_both[\"change\"].str.split(r\" \\(\").str[0]\n",
    "tmp_both[\"change_simple\"] = tmp_both[\"change\"].str.split(r\" \\(\").str[0]\n",
    "\n",
    "\n",
    "tmp_spei = plot_pattern_dist(\n",
    "    df_spei_patterns,\n",
    "    \"response_spei\",\n",
    "    dir_patterns,\n",
    "    do_return=True,\n",
    ")\n",
    "tmp_spei[\"change_simple\"] = tmp_spei[\"response_spei\"].str.split(r\" \\(\").str[0]\n",
    "# Add other if not present\n",
    "display(tmp_spei)\n",
    "if \"other\" not in tmp_spei[\"change_simple\"].values:\n",
    "    row = {\n",
    "        \"response_spei\": \"other (0%)\",\n",
    "        \"value\": 0,\n",
    "        \"change_simple\": \"other\",\n",
    "    }\n",
    "    tmp_spei = pd.concat([tmp_spei, pd.DataFrame([row])], ignore_index=True)\n",
    "tmp_spei = tmp_spei.replace(\"other\", \"other (spei)\")\n",
    "display(tmp_spei)\n",
    "\n",
    "tmp_temp = plot_pattern_dist(\n",
    "    df_temp_patterns,\n",
    "    \"response_temp\",\n",
    "    dir_patterns,\n",
    "    do_return=True,\n",
    ")\n",
    "tmp_temp[\"change_simple\"] = tmp_temp[\"response_temp\"].str.split(r\" \\(\").str[0]\n",
    "# Add other if not present\n",
    "if \"other\" not in tmp_temp[\"change_simple\"].values:\n",
    "    row = {\n",
    "        \"response_temp\": \"other (0%)\",\n",
    "        \"value\": 0,\n",
    "        \"change_simple\": \"other\",\n",
    "    }\n",
    "    tmp_temp = pd.concat([tmp_temp, pd.DataFrame([row])], ignore_index=True)\n",
    "tmp_spei = tmp_spei.replace(\"other\", \"other (spei)\")\n",
    "tmp_temp = tmp_temp.replace(\"other\", \"other (temp)\")\n",
    "\n",
    "patterns_merged = (\n",
    "    pd.concat([tmp_temp, tmp_spei, tmp_both], axis=0)\n",
    "    .reset_index(drop=True)\n",
    "    .rename(columns={\"value\": \"group_size_rel\"})\n",
    ")\n",
    "\n",
    "# Merge double columns for group size\n",
    "patterns_merged[\"group_size_rel\"] = (\n",
    "    patterns_merged[\"group_size_rel\"].fillna(0).astype(int)\n",
    ")\n",
    "patterns_merged[\"group_size_rel_1\"] = (\n",
    "    patterns_merged[\"group_size_rel_1\"].fillna(0).astype(int)\n",
    ")\n",
    "patterns_merged[\"group_size_rel\"] = (\n",
    "    patterns_merged[\"group_size_rel\"] + patterns_merged[\"group_size_rel_1\"]\n",
    ")\n",
    "\n",
    "# Merge double columns for change description\n",
    "\n",
    "patterns_merged = patterns_merged.drop(columns=[\"group_size_rel_1\"])\n",
    "display(patterns_merged)\n",
    "\n",
    "# Summary of runs per change\n",
    "# ! For top9 species\n",
    "tmp_both = plot_pattern_dist(\n",
    "    df_both_patterns.query(\"species in @top9\"), \"change\", dir_patterns, do_return=True\n",
    ").rename(columns={\"value\": \"group_size_rel_1\"})\n",
    "tmp_both[\"change_clean\"] = tmp_both[\"change\"].str.split(r\" \\(\").str[0]\n",
    "tmp_both[\"change_simple\"] = tmp_both[\"change\"].str.split(r\" \\(\").str[0]\n",
    "\n",
    "\n",
    "tmp_spei = plot_pattern_dist(\n",
    "    df_spei_patterns.query(\"species in @top9\"),\n",
    "    \"response_spei\",\n",
    "    dir_patterns,\n",
    "    do_return=True,\n",
    ")\n",
    "tmp_spei[\"change_simple\"] = tmp_spei[\"response_spei\"].str.split(r\" \\(\").str[0]\n",
    "tmp_spei = tmp_spei.replace(\"other\", \"other (spei)\")\n",
    "\n",
    "tmp_temp = plot_pattern_dist(\n",
    "    df_temp_patterns.query(\"species in @top9\"),\n",
    "    \"response_temp\",\n",
    "    dir_patterns,\n",
    "    do_return=True,\n",
    ")\n",
    "tmp_temp[\"change_simple\"] = tmp_temp[\"response_temp\"].str.split(r\" \\(\").str[0]\n",
    "tmp_temp = tmp_temp.replace(\"other\", \"other (temp)\")\n",
    "\n",
    "patterns_merged_t9 = (\n",
    "    pd.concat([tmp_temp, tmp_spei, tmp_both], axis=0)\n",
    "    .reset_index(drop=True)\n",
    "    .rename(columns={\"value\": \"group_size_rel\"})\n",
    ")\n",
    "\n",
    "# Check if all changes are present\n",
    "for c in patterns_merged.change_simple.unique():\n",
    "    if c not in patterns_merged_t9.change_simple.unique():\n",
    "        print(f\"{c} is missing, adding a row for it\")\n",
    "        if c == \"other (temp)\":\n",
    "            new_row = pd.DataFrame(\n",
    "                {\n",
    "                    \"group_size_rel\": 0,\n",
    "                    \"change_simple\": c,\n",
    "                    \"response_temp\": \"other (0%)\",\n",
    "                },\n",
    "                index=[0],\n",
    "            )\n",
    "\n",
    "            # Insert row at third position\n",
    "            patterns_merged_t9 = pd.concat(\n",
    "                [patterns_merged_t9.iloc[:2], new_row, patterns_merged_t9.iloc[2:]]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            chime.error()\n",
    "            raise ValueError(f\"🟥 adding entry for '{c}' is not yet implemented!\")\n",
    "\n",
    "# Merge double columns for group size\n",
    "patterns_merged_t9[\"group_size_rel\"] = (\n",
    "    patterns_merged_t9[\"group_size_rel\"].fillna(0).astype(int)\n",
    ")\n",
    "patterns_merged_t9[\"group_size_rel_1\"] = (\n",
    "    patterns_merged_t9[\"group_size_rel_1\"].fillna(0).astype(int)\n",
    ")\n",
    "patterns_merged_t9[\"group_size_rel\"] = (\n",
    "    patterns_merged_t9[\"group_size_rel\"] + patterns_merged_t9[\"group_size_rel_1\"]\n",
    ")\n",
    "\n",
    "# Merge double columns for change description\n",
    "patterns_merged_t9 = patterns_merged_t9.drop(columns=[\"group_size_rel_1\"])\n",
    "\n",
    "patterns_merged_t9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all model runs\n",
    "all_dfs = df_all_concat.copy()\n",
    "\n",
    "# Some checks\n",
    "# print(\"Runs per model:\")\n",
    "# display(all_dfs[\"species\"].sort_values().value_counts())\n",
    "# display(all_dfs[\"subset_group\"].sort_values().value_counts())\n",
    "\n",
    "# Filter for roc_threshold\n",
    "n_species = all_dfs[\"species\"].nunique()\n",
    "n_runs = 50\n",
    "\n",
    "supposed_n_runs = n_species * n_runs\n",
    "actual_n_runs = all_dfs.shape[0]\n",
    "all_dfs = all_dfs.query(\"test_boot_mean >= @roc_threshold\")\n",
    "valid_n_runs = all_dfs.shape[0]\n",
    "\n",
    "print(\n",
    "    f\"Total number of runs: {supposed_n_runs} = {n_species} species * {n_runs} runs per species\"\n",
    ")\n",
    "print(f\"Actual number of runs: {actual_n_runs}\")\n",
    "print(\n",
    "    f\"Number of valid runs: {valid_n_runs}, which is {valid_n_runs / supposed_n_runs * 100:.2f}% of total runs and {valid_n_runs / actual_n_runs * 100:.2f}% of actual runs\"\n",
    ")\n",
    "\n",
    "# Turn character lists into literals\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)\n",
    "imps = [c for c in all_dfs.columns if \"Importance\" in c]\n",
    "vals = [c for c in all_dfs.columns if \"Values\" in c]\n",
    "metrics = [c for c in all_dfs.columns if \"Metrics\" in c]\n",
    "for m in metrics:\n",
    "    all_dfs[m] = all_dfs[m].replace({np.nan: \"['NA']\"})\n",
    "    all_dfs[m] = all_dfs[m].apply(ast.literal_eval)\n",
    "    all_dfs[m] = all_dfs[m].apply(lambda x: x[0])\n",
    "    all_dfs[m] = all_dfs[m].replace({\"NA\": np.nan})\n",
    "\n",
    "for i in imps:\n",
    "    all_dfs[i] = all_dfs[i].astype(float)\n",
    "\n",
    "for v in vals:\n",
    "    all_dfs[v] = all_dfs[v].replace({np.nan: \"['NA']\"})\n",
    "    all_dfs[v] = all_dfs[v].apply(ast.literal_eval)\n",
    "    all_dfs[v] = all_dfs[v].apply(lambda x: x[0])\n",
    "    all_dfs[v] = all_dfs[v].replace({\"NA\": np.nan})\n",
    "    all_dfs[v] = all_dfs[v].astype(float)\n",
    "\n",
    "display(all_dfs.head(3))\n",
    "\n",
    "# Calculate weights of species\n",
    "df_w = (\n",
    "    all_dfs.groupby(\"species\")\n",
    "    .agg(\n",
    "        n_runs=(\"subset_group\", \"nunique\"),\n",
    "        n_runs_rel=(\"subset_group\", lambda x: x.nunique() / n_runs * 100),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "display(df_w.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_dataset_boxplot(None, all_dfs, imps, base_fontsize=11)\n",
    "ax_dataset_boxplot(None, all_dfs, imps, base_fontsize=11, all_or_top9=\"top9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2: Importance + Climate Influence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_importance_df(all_or_top9):\n",
    "\n",
    "    # Get importance df\n",
    "    ximp = ax_dataset_boxplot(\n",
    "        None,\n",
    "        all_dfs,\n",
    "        imps,\n",
    "        base_fontsize=11,\n",
    "        all_or_top9=all_or_top9,\n",
    "        return_dfimp=True,\n",
    "    ).mean()\n",
    "\n",
    "    # Get different aggregates\n",
    "    imp_stand = (\n",
    "        ximp[\"Light Competition\"]\n",
    "        + ximp[\"Species Competition\"]\n",
    "        + ximp[\"Stand Structure\"]\n",
    "        + ximp[\"Tree Size\"]\n",
    "    ).round(0)\n",
    "\n",
    "    imp_climate = (ximp[\"Temperature\"] + ximp[\"SPEI\"]).round(2)\n",
    "\n",
    "    imp_soil = (ximp[\"Soil Fertility\"] + ximp[\"Soil Water Conditions\"]).round(0)\n",
    "\n",
    "    print(f\" --- {all_or_top9} ---\")\n",
    "    display(ximp.round(2))\n",
    "    print(f\"Sum of Stand-describing variables: {imp_stand}\")\n",
    "    print(f\"Sum of Climate-describing variables: {imp_climate}\")\n",
    "    print(f\"Sum of Soil-describing variables: {imp_soil} \\n\\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for all_or_top9 in [\"all\", \"top9\"]:\n",
    "\n",
    "    filedir = f\"{dir_patterns}/fig_dataset_pattern\"\n",
    "    os.makedirs(filedir, exist_ok=True)\n",
    "    color_temp = \"#77422C\"\n",
    "    color_spei = \"#D1A289\"\n",
    "    color_rest = \"lightgrey\"\n",
    "    color_wd = sns.color_palette(\"Reds\", 3)[-1]\n",
    "    color_ww = sns.color_palette(\"Blues\", 3)[-1]\n",
    "    color_other = \"lightgrey\"\n",
    "\n",
    "    print(\n",
    "        \"🚨 The order of labels may be wrong and needs to be adjusted by hand! Compare to displayed df!\"\n",
    "    )\n",
    "\n",
    "    if all_or_top9 == \"all\":\n",
    "        tmp_in = patterns_merged.copy()\n",
    "        ytick_labels = [\n",
    "            \"Warmer\".title(),\n",
    "            \"Cooler\".title(),\n",
    "            \"Other\".title(),\n",
    "            \"Drier\".title(),\n",
    "            \"Wetter\".title(),\n",
    "            \"Other\".title(),\n",
    "            \"Warmer + Drier\".title(),\n",
    "            \"Warmer + Wetter\".title(),\n",
    "            \"Other\".title(),\n",
    "            \"Cooler + Wetter\".title(),\n",
    "            \"Cooler + Drier\".title(),\n",
    "        ]\n",
    "    else:\n",
    "        tmp_in = patterns_merged_t9.copy()\n",
    "        ytick_labels = [\n",
    "            \"Warmer\".title(),\n",
    "            \"Cooler\".title(),\n",
    "            \"Other\".title(),\n",
    "            \"Drier\".title(),\n",
    "            \"Wetter\".title(),\n",
    "            \"Other\".title(),\n",
    "            \"Warmer + Drier\".title(),\n",
    "            \"Warmer + Wetter\".title(),\n",
    "            \"Cooler + Drier\".title(),\n",
    "            \"Other\".title(),\n",
    "            \"Cooler + Wetter\".title(),\n",
    "        ]\n",
    "\n",
    "    analyse_importance_df(all_or_top9)\n",
    "    display(tmp_in)\n",
    "\n",
    "    plot_bars_dataset_pattern(\n",
    "        tmp_in,\n",
    "        all_dfs,\n",
    "        all_or_top9=all_or_top9,\n",
    "        color_temp=color_temp,\n",
    "        color_spei=color_spei,\n",
    "        color_rest=color_rest,\n",
    "        color_wd=color_wd,\n",
    "        color_ww=color_ww,\n",
    "        color_other=color_other,\n",
    "        filepath=f\"{filedir}/{all_or_top9}.png\",\n",
    "        # filepath=None,\n",
    "        ytick_labels=ytick_labels,\n",
    "    )\n",
    "    print(\n",
    "        f\"---------------------------------------------------------------------------------------------------------------------------------------\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Barplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get change labels\n",
    "# ! All\n",
    "change_labels = []\n",
    "\n",
    "for c in change_dict.keys():\n",
    "    _perc = (\n",
    "        patterns_merged.query(\"change_simple == @c\").loc[:, \"group_size_rel\"].values[0]\n",
    "    )\n",
    "    change_labels.append(f'{c.replace(\"_\", \" + \")} ({_perc}%)')\n",
    "display(change_labels)\n",
    "\n",
    "# ! Top 9\n",
    "change_labels_t9 = []\n",
    "for c in change_dict.keys():\n",
    "    _perc = (\n",
    "        patterns_merged_t9.query(\"change_simple == @c\")\n",
    "        .loc[:, \"group_size_rel\"]\n",
    "        .values[0]\n",
    "    )\n",
    "    change_labels_t9.append(f'{c.replace(\"_\", \" + \")} ({_perc}%)')\n",
    "display(change_labels_t9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "df_plot_tmp = df_plot_from_mulitplot.copy()\n",
    "\n",
    "fig, (ax0, ax2, ax1) = plt.subplots(\n",
    "    1, 3, figsize=(12, 12), gridspec_kw={\"width_ratios\": [2.75, 1, 1]}\n",
    ")\n",
    "\n",
    "# ! PLOT: Change bar chart\n",
    "if not normalized_change_chart:\n",
    "    for change in change_order:\n",
    "        df_plot_tmp[change] = df_plot_tmp[change].copy() / n_all_runs * 100\n",
    "\n",
    "species = df_plot_tmp.species\n",
    "changes = change_order\n",
    "values = 0\n",
    "bottom = None\n",
    "for change, color in zip(changes, change_colors):\n",
    "    values = df_plot_tmp[change].copy()\n",
    "    ax0.barh(\n",
    "        species,\n",
    "        values,\n",
    "        left=bottom,\n",
    "        label=change,\n",
    "        color=color,\n",
    "        height=1,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    if bottom is None:\n",
    "        bottom = values\n",
    "    else:\n",
    "        bottom += values\n",
    "\n",
    "# Adding title and labels\n",
    "if normalized_change_chart:\n",
    "    xlab = \"Percentage of valid runs (%)\"\n",
    "else:\n",
    "    # xlab = f\"Percentage of all {n_all_runs} runs (%)\"\n",
    "    xlab = f\"Percentage of all {n_all_runs} runs\"\n",
    "\n",
    "ax0.set_xlabel(xlab, weight=\"bold\")\n",
    "ax0.set_ylabel(\"\")\n",
    "ax0.legend(\n",
    "    labels=change_labels,\n",
    "    bbox_to_anchor=(0.5, -0.075),\n",
    "    loc=\"upper center\",\n",
    "    ncol=2,\n",
    "    frameon=False,\n",
    ").set_title(\n",
    "    title=\"Short-term climatic condition\\npromoting mortality\", prop={\"weight\": \"bold\"}\n",
    ")\n",
    "\n",
    "# Adjust ticks size\n",
    "ax0.tick_params(axis=\"y\", which=\"major\", labelsize=11)\n",
    "\n",
    "# Add dotted line at 50\n",
    "# ax0.axvline(x=50, color=\"lightgrey\", linestyle=\"--\", linewidth=0.75)\n",
    "\n",
    "# ! PLOT: Valid runs and ROC-AUC\n",
    "# Bar chart for n_runs\n",
    "if bars_in_double_chart == \"ntrees\":\n",
    "    if ntrees_log:\n",
    "        xbars = np.log10(df_plot_tmp[\"n_trees_total\"].copy())\n",
    "        xlab = \"$\\it{N}$ Trees ($10^x$)\"\n",
    "    else:\n",
    "        xbars = df_plot_tmp[\"n_trees_total\"].copy() / 10000\n",
    "        # xlab = \"$\\it{N}$ Trees (x 10'000)\"\n",
    "        xlab = \"Trees (x 10'000)\"\n",
    "elif bars_in_double_chart == \"nruns\":\n",
    "    xbars = df_plot_tmp[\"n_runs\"].copy()\n",
    "    xlab = \"$\\it{N}$ Runs\"\n",
    "else:\n",
    "    chime.error()\n",
    "    raise ValueError(\"🟥 bars_in_double_chart not recognized\")\n",
    "\n",
    "ax1.barh(\n",
    "    df_plot_tmp[\"species\"],\n",
    "    xbars,\n",
    "    color=\"darkgrey\",\n",
    "    edgecolor=\"black\",\n",
    "    height=1,\n",
    "    # alpha=0.8,\n",
    ")\n",
    "ax1.set_xlabel(xlab, weight=\"bold\")\n",
    "\n",
    "# Point chart for mean_roc with error bars for std_roc\n",
    "ax1_2 = ax1.twiny()\n",
    "ax1_2.errorbar(\n",
    "    df_plot_tmp[\"mean_roc\"],\n",
    "    df_plot_tmp[\"species\"],\n",
    "    xerr=df_plot_tmp[\"std_roc\"],\n",
    "    fmt=\"o\",\n",
    "    color=\"darkgreen\",\n",
    "    ecolor=\"darkgreen\",\n",
    "    capsize=0,\n",
    ")\n",
    "\n",
    "ax1_2.set_xlabel(\"ROC-AUC\", color=\"darkgreen\", weight=\"bold\")\n",
    "ax1_2.set_xlim(0.6, 1)\n",
    "ax1_2.tick_params(axis=\"x\", colors=\"darkgreen\")\n",
    "\n",
    "# Adjust the position of the secondary x-axis\n",
    "ax1_2.spines[\"bottom\"].set_position((\"outward\", 45))\n",
    "ax1_2.xaxis.set_ticks_position(\"bottom\")\n",
    "ax1_2.xaxis.set_label_position(\"bottom\")\n",
    "\n",
    "# Set the ticks and labels for the secondary x-axis\n",
    "ax1_2.set_xticks([0.6, 0.8, 1.0])\n",
    "ax1_2.set_xticklabels([0.6, 0.8, 1.0])\n",
    "\n",
    "# Color the second axis in darkgreen\n",
    "ax1_2.spines[\"bottom\"].set_color(\"darkgreen\")\n",
    "ax1_2.xaxis.label.set_color(\"darkgreen\")\n",
    "ax1_2.tick_params(axis=\"x\", colors=\"darkgreen\")\n",
    "\n",
    "# Hide the original top spine\n",
    "# ax1_2.spines[\"top\"].set_visible(False)\n",
    "# ax1.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Move y axis to the right\n",
    "ax1.yaxis.tick_right()\n",
    "\n",
    "# ! PLOT: Plotting the stacked bars\n",
    "if normalized_import_chart:\n",
    "    df_plot_tmp[\"mean_temp\"] = df_plot_tmp[\"mean_temp\"] / df_plot_tmp[\"spei+temp\"] * 100\n",
    "    df_plot_tmp[\"mean_spei\"] = df_plot_tmp[\"mean_spei\"] / df_plot_tmp[\"spei+temp\"] * 100\n",
    "\n",
    "\n",
    "ax2.barh(\n",
    "    df_plot_tmp[\"species\"],\n",
    "    df_plot_tmp[\"mean_temp\"],\n",
    "    height=1,\n",
    "    color=\"gold\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Temp.\",\n",
    ")\n",
    "ax2.barh(\n",
    "    df_plot_tmp[\"species\"],\n",
    "    df_plot_tmp[\"mean_spei\"],\n",
    "    left=df_plot_tmp[\"mean_temp\"],\n",
    "    height=1,\n",
    "    color=\"brown\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"SPEI\",\n",
    ")\n",
    "\n",
    "# Adding labels, legend, and title\n",
    "ax2.set_xlabel(\"Climate sensitivity\\n(Mean Import., %)\", weight=\"bold\")\n",
    "ax2.set_ylabel(\"\")\n",
    "# ax2.set_title(\"Mean SPEI and Mean Temperature by Species\")\n",
    "ax2.legend(\n",
    "    # For having it to the right\n",
    "    # bbox_to_anchor=(1, 1),\n",
    "    # loc=\"best\",\n",
    "    # ncol=1,\n",
    "    frameon=False,\n",
    "    # For having it below:\n",
    "    bbox_to_anchor=(0.5, -0.075),\n",
    "    loc=\"upper center\",\n",
    "    # ncol=len(changes),\n",
    "    ncol=1,\n",
    ").set_title(title=\"Dataset\", prop={\"weight\": \"bold\"})\n",
    "\n",
    "# Put y axis on right\n",
    "# ax2.yaxis.tick_right()\n",
    "# Remove y axis\n",
    "# ax2.set_ylabel()\n",
    "ax2.yaxis.set_ticks([])\n",
    "ax1_2.yaxis.set_ticks([])\n",
    "\n",
    "# Remove all right and top spines\n",
    "ax0.spines[\"right\"].set_visible(False)\n",
    "ax0.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1_2.spines[\"right\"].set_visible(False)\n",
    "ax1_2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Adjust y-limits to remove extra white space\n",
    "ax0.set_ylim(-0.5, len(species) - 0.5)\n",
    "ax1.set_ylim(-0.5, len(species) - 0.5)\n",
    "ax2.set_ylim(-0.5, len(species) - 0.5)\n",
    "\n",
    "# Add letters to subplots\n",
    "ax0.text(0, 1.01, \"a\", transform=ax0.transAxes, size=15, weight=\"bold\")\n",
    "ax1.text(0, 1.01, \"c\", transform=ax1.transAxes, size=15, weight=\"bold\")\n",
    "ax2.text(0, 1.01, \"b\", transform=ax2.transAxes, size=15, weight=\"bold\")\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\n",
    "#     f\"{dir_patterns}/01-multi_bar_response_patterns-all_species-sort_by_{sort_by}.png\"\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "df_plot_tmp = df_plot.copy()\n",
    "df_plot_tmp = df_plot_tmp.query(\"species in @top9\").reset_index(drop=True).copy()\n",
    "\n",
    "fig, (ax0, ax2, ax1) = plt.subplots(\n",
    "    1, 3, figsize=(12, 5), gridspec_kw={\"width_ratios\": [2.75, 1, 1]}\n",
    ")\n",
    "\n",
    "# ! PLOT: Change bar chart\n",
    "if not normalized_change_chart:\n",
    "    for change in change_order:\n",
    "        df_plot_tmp[change] = df_plot_tmp[change].copy() / n_all_runs * 100\n",
    "\n",
    "species = df_plot_tmp.species\n",
    "changes = change_order\n",
    "values = 0\n",
    "bottom = None\n",
    "for change, color in zip(changes, change_colors):\n",
    "    values = df_plot_tmp[change].copy()\n",
    "    ax0.barh(\n",
    "        species,\n",
    "        values,\n",
    "        left=bottom,\n",
    "        label=change,\n",
    "        color=color,\n",
    "        height=1,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    if bottom is None:\n",
    "        bottom = values\n",
    "    else:\n",
    "        bottom += values\n",
    "\n",
    "# Adding title and labels\n",
    "if normalized_change_chart:\n",
    "    xlab = \"Percentage of valid runs (%)\"\n",
    "else:\n",
    "    # xlab = f\"Percentage of all {n_all_runs} runs (%)\"\n",
    "    xlab = f\"Percentage of all {n_all_runs} runs\"\n",
    "\n",
    "ax0.set_xlabel(xlab, weight=\"bold\")\n",
    "ax0.set_ylabel(\"\")\n",
    "ax0.legend(\n",
    "    labels=change_labels_t9,\n",
    "    bbox_to_anchor=(0.5, -0.45),\n",
    "    loc=\"upper center\",\n",
    "    ncol=2,\n",
    "    frameon=False,\n",
    "    # ).set_title(title=\"Condition Increasing Mortality\", prop={\"weight\": \"bold\"})\n",
    ").set_title(\n",
    "    title=\"Short-term climatic condition promoting mortality\", prop={\"weight\": \"bold\"}\n",
    ")\n",
    "\n",
    "# Adjust ticks size\n",
    "ax0.tick_params(axis=\"y\", which=\"major\", labelsize=11)\n",
    "\n",
    "# Add dotted line at 50\n",
    "# ax0.axvline(x=50, color=\"lightgrey\", linestyle=\"--\", linewidth=0.75)\n",
    "\n",
    "# ! PLOT: Valid runs and ROC-AUC\n",
    "# Bar chart for n_runs\n",
    "if bars_in_double_chart == \"ntrees\":\n",
    "    if ntrees_log:\n",
    "        xbars = np.log10(df_plot_tmp[\"n_trees_total\"].copy())\n",
    "        xlab = \"$\\it{N}$ Trees ($10^x$)\"\n",
    "    else:\n",
    "        xbars = df_plot_tmp[\"n_trees_total\"].copy() / 10000\n",
    "        # xlab = \"$\\it{N}$ Trees (x 10'000)\"\n",
    "        xlab = \"Trees (x 10'000)\"\n",
    "elif bars_in_double_chart == \"nruns\":\n",
    "    xbars = df_plot_tmp[\"n_runs\"].copy()\n",
    "    xlab = \"$\\it{N}$ Runs\"\n",
    "else:\n",
    "    chime.error()\n",
    "    raise ValueError(\"🟥 bars_in_double_chart not recognized\")\n",
    "\n",
    "ax1.barh(\n",
    "    df_plot_tmp[\"species\"],\n",
    "    xbars,\n",
    "    color=\"darkgrey\",\n",
    "    edgecolor=\"black\",\n",
    "    height=1,\n",
    "    # alpha=0.8,\n",
    ")\n",
    "ax1.set_xlabel(xlab, weight=\"bold\")\n",
    "\n",
    "# Point chart for mean_roc with error bars for std_roc\n",
    "ax1_2 = ax1.twiny()\n",
    "ax1_2.errorbar(\n",
    "    df_plot_tmp[\"mean_roc\"],\n",
    "    df_plot_tmp[\"species\"],\n",
    "    xerr=df_plot_tmp[\"std_roc\"],\n",
    "    fmt=\"o\",\n",
    "    color=\"darkgreen\",\n",
    "    ecolor=\"darkgreen\",\n",
    "    capsize=0,\n",
    ")\n",
    "\n",
    "ax1_2.set_xlabel(\"ROC-AUC\", color=\"darkgreen\", weight=\"bold\")\n",
    "ax1_2.set_xlim(0.6, 1)\n",
    "ax1_2.tick_params(axis=\"x\", colors=\"darkgreen\")\n",
    "\n",
    "# Adjust the position of the secondary x-axis\n",
    "ax1_2.spines[\"bottom\"].set_position((\"outward\", 45))\n",
    "ax1_2.xaxis.set_ticks_position(\"bottom\")\n",
    "ax1_2.xaxis.set_label_position(\"bottom\")\n",
    "\n",
    "# Set the ticks and labels for the secondary x-axis\n",
    "ax1_2.set_xticks([0.6, 0.8, 1.0])\n",
    "ax1_2.set_xticklabels([0.6, 0.8, 1.0])\n",
    "\n",
    "# Color the second axis in darkgreen\n",
    "ax1_2.spines[\"bottom\"].set_color(\"darkgreen\")\n",
    "ax1_2.xaxis.label.set_color(\"darkgreen\")\n",
    "ax1_2.tick_params(axis=\"x\", colors=\"darkgreen\")\n",
    "\n",
    "# Hide the original top spine\n",
    "# ax1_2.spines[\"top\"].set_visible(False)\n",
    "# ax1.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Move y axis to the right\n",
    "ax1.yaxis.tick_right()\n",
    "\n",
    "# ! PLOT: Climate Sensitivity\n",
    "if normalized_import_chart:\n",
    "    df_plot_tmp[\"mean_temp\"] = df_plot_tmp[\"mean_temp\"] / df_plot_tmp[\"spei+temp\"] * 100\n",
    "    df_plot_tmp[\"mean_spei\"] = df_plot_tmp[\"mean_spei\"] / df_plot_tmp[\"spei+temp\"] * 100\n",
    "\n",
    "\n",
    "ax2.barh(\n",
    "    df_plot_tmp[\"species\"],\n",
    "    df_plot_tmp[\"mean_temp\"],\n",
    "    height=1,\n",
    "    color=\"gold\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Temperature\",\n",
    ")\n",
    "ax2.barh(\n",
    "    df_plot_tmp[\"species\"],\n",
    "    df_plot_tmp[\"mean_spei\"],\n",
    "    left=df_plot_tmp[\"mean_temp\"],\n",
    "    height=1,\n",
    "    color=\"brown\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"SPEI\",\n",
    ")\n",
    "\n",
    "# Adding labels, legend, and title\n",
    "ax2.set_xlabel(\"Climate sensitivity\\n(Mean Import., %)\", weight=\"bold\")\n",
    "ax2.set_ylabel(\"\")\n",
    "# ax2.set_title(\"Mean SPEI and Mean Temperature by Species\")\n",
    "ax2.legend(\n",
    "    # For having it to the right\n",
    "    # bbox_to_anchor=(1, 1),\n",
    "    # loc=\"best\",\n",
    "    # ncol=1,\n",
    "    frameon=False,\n",
    "    # For having it below:\n",
    "    bbox_to_anchor=(0.5, -0.45),\n",
    "    loc=\"upper center\",\n",
    "    # ncol=len(changes),\n",
    "    ncol=1,\n",
    ").set_title(title=\"Dataset\", prop={\"weight\": \"bold\"})\n",
    "\n",
    "# Put y axis on right\n",
    "# ax2.yaxis.tick_right()\n",
    "# Remove y axis\n",
    "# ax2.set_ylabel()\n",
    "ax2.yaxis.set_ticks([])\n",
    "ax1_2.yaxis.set_ticks([])\n",
    "\n",
    "# Remove all right and top spines\n",
    "ax0.spines[\"right\"].set_visible(False)\n",
    "ax0.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1_2.spines[\"right\"].set_visible(False)\n",
    "ax1_2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Adjust y-limits to remove extra white space\n",
    "ax0.set_ylim(-0.5, len(species) - 0.5)\n",
    "ax1.set_ylim(-0.5, len(species) - 0.5)\n",
    "ax2.set_ylim(-0.5, len(species) - 0.5)\n",
    "\n",
    "# Add letters to subplots\n",
    "ax0.text(0, 1.03, \"a\", transform=ax0.transAxes, size=15, weight=\"bold\")\n",
    "ax1.text(0, 1.03, \"c\", transform=ax1.transAxes, size=15, weight=\"bold\")\n",
    "ax2.text(0, 1.03, \"b\", transform=ax2.transAxes, size=15, weight=\"bold\")\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    f\"{dir_patterns}/01-multi_bar_response_patterns-top9_species-sort_by_{sort_by}.png\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Feature per Species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! Settings ------------------------------\n",
    "pd.set_option(\"display.max_colwidth\", 100000)\n",
    "\n",
    "# tmp_specs = [\"Quercus robur\", \"Quercus petraea\", \"Quercus ilex\", \"Quercus pubescens\"]\n",
    "# tmp_specs = [\"Abies alba\", \"Picea abies\", \"Pinus sylvestris\"]\n",
    "# tmp_specs = top9\n",
    "# tmp_specs = top9 + [\"Quercus ilex\"]\n",
    "tmp_specs = all_species\n",
    "\n",
    "# ! SPEI ------------------------------\n",
    "df_tmp = df_occ_spei.query(\"species in @tmp_specs\")[\n",
    "    [\n",
    "        \"species\",\n",
    "        \"change_perc\",\n",
    "        \"spei_duration_simple\",\n",
    "        \"spei_anom\",\n",
    "        \"spei_season\",\n",
    "        # \"best_spei_simpler\",\n",
    "        # \"spei_simpler\",\n",
    "        # \"best_feature_spei\",\n",
    "        \"feature_spei\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Get index for fuzzy matching change\n",
    "df_tmp_drier = df_tmp.query(\n",
    "    \"change_perc.str.contains('drier', case=False)\", engine=\"python\"\n",
    ")\n",
    "df_tmp_wetter = df_tmp.query(\n",
    "    \"change_perc.str.contains('wetter', case=False)\", engine=\"python\"\n",
    ")\n",
    "\n",
    "df_tmp_other = df_tmp.query(\n",
    "    \"change_perc.str.contains('other', case=False)\", engine=\"python\"\n",
    ")\n",
    "\n",
    "df_tmp_all = pd.concat([df_tmp_drier, df_tmp_wetter, df_tmp_other], axis=0).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Sort by species and order\n",
    "tmp_order = [\"drier\", \"wetter\", \"other\"]\n",
    "df_tmp_all[\"change\"] = df_tmp_all[\"change_perc\"].str.split(\" \").str[0]\n",
    "df_tmp_all[\"change\"] = pd.Categorical(\n",
    "    df_tmp_all[\"change\"], categories=tmp_order, ordered=True\n",
    ")\n",
    "df_tmp_all = df_tmp_all.sort_values(\n",
    "    [\"species\", \"change\"], ascending=[True, True]\n",
    ").reset_index(drop=True)\n",
    "df_tmp_all = df_tmp_all.drop(columns=[\"change\"])\n",
    "\n",
    "\n",
    "# Save only if all species were selected\n",
    "if tmp_specs == all_species:\n",
    "    df_tmp_all.to_csv(f\"{dir_tables}/spei_features_all_species.csv\", index=False)\n",
    "\n",
    "display(df_tmp_all)\n",
    "\n",
    "# ! TEMP ------------------------------\n",
    "df_tmp = df_occ_temp.query(\"species in @tmp_specs\")[\n",
    "    [\n",
    "        \"species\",\n",
    "        \"change_perc\",\n",
    "        \"temp_metric\",\n",
    "        \"temp_anom\",\n",
    "        \"temp_season\",\n",
    "        # \"best_spei_simpler\",\n",
    "        # \"spei_simpler\",\n",
    "        # \"best_feature_spei\",\n",
    "        \"feature_temp\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Get index for fuzzy matching change\n",
    "df_tmp_warmer = df_tmp.query(\n",
    "    \"species in @tmp_specs and change_perc.str.contains('warmer', case=False)\",\n",
    "    engine=\"python\",\n",
    ")\n",
    "df_tmp_cooler = df_tmp.query(\n",
    "    \"species in @tmp_specs and change_perc.str.contains('cooler', case=False)\",\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "df_tmp_other = df_tmp.query(\n",
    "    \"species in @tmp_specs and change_perc.str.contains('other', case=False)\",\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "df_tmp_all = pd.concat(\n",
    "    [df_tmp_warmer, df_tmp_cooler, df_tmp_other], axis=0\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Sort by species and order\n",
    "tmp_order = [\"warmer\", \"cooler\", \"other\"]\n",
    "df_tmp_all[\"change\"] = df_tmp_all[\"change_perc\"].str.split(\" \").str[0]\n",
    "df_tmp_all[\"change\"] = pd.Categorical(\n",
    "    df_tmp_all[\"change\"], categories=tmp_order, ordered=True\n",
    ")\n",
    "df_tmp_all = df_tmp_all.sort_values(\n",
    "    [\"species\", \"change\"], ascending=[True, True]\n",
    ").reset_index(drop=True)\n",
    "df_tmp_all = df_tmp_all.drop(columns=[\"change\"])\n",
    "\n",
    "\n",
    "# Save only if all species were selected\n",
    "if tmp_specs == all_species:\n",
    "    df_tmp_all.to_csv(f\"{dir_tables}/temp_features_all_species.csv\", index=False)\n",
    "\n",
    "df_tmp_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Occurrence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all datasets\n",
    "df_filtered = all_dfs.copy()\n",
    "\n",
    "# Add run count\n",
    "df_filtered = df_filtered.merge(df_w, on=\"species\", how=\"left\")\n",
    "\n",
    "# Merge species and run into one variable\n",
    "df_filtered[\"species\"] = df_filtered[\"species\"] + \" - \" + df_filtered[\"subset_group\"]\n",
    "\n",
    "all_datasets = [\n",
    "    \"Tree Size\",\n",
    "    \"Light Competition\",\n",
    "    \"Stand Structure\",\n",
    "    \"Species Competition\",\n",
    "    \"Temperature\",\n",
    "    \"SPEI\",\n",
    "    \"Topography\",\n",
    "    \"Soil Water Conditions\",\n",
    "    \"Soil Fertility\",\n",
    "    \"Management\",\n",
    "    \"NDVI\",\n",
    "]\n",
    "\n",
    "# Remove SPEI and Temp because too many levels\n",
    "all_datasets = [i for i in all_datasets if \"SPEI\" not in i and \"Temperature\" not in i]\n",
    "\n",
    "# Set index\n",
    "df_filtered = df_filtered.set_index(\"species\")\n",
    "\n",
    "# Get number of valid models\n",
    "n_valid_models = df_filtered.shape[0]\n",
    "\n",
    "# Initiate lists\n",
    "dic_values = {}\n",
    "dic_counts = {}\n",
    "dic_preds = {}\n",
    "\n",
    "# Loop through all datasets\n",
    "for d in all_datasets:\n",
    "    # Get dataset subset and remove NA values\n",
    "    d_vars = df_filtered.filter(regex=d, axis=1).columns.tolist()\n",
    "    df_i = df_filtered[d_vars + [\"n_runs\"]].dropna()\n",
    "\n",
    "    # Initiate list\n",
    "    dataset_features = []\n",
    "\n",
    "    # Loop over all species\n",
    "    for i, species in enumerate(df_i.index):\n",
    "\n",
    "        # Set species\n",
    "        df_i[\"Species\"] = species\n",
    "\n",
    "        # Get metrics and values of dataset\n",
    "        imetrics = df_i.loc[species][f\"{d} - Metrics\"]\n",
    "        ivalues = df_i.loc[species][f\"{d} - Values\"]\n",
    "\n",
    "        imetrics = [imetrics]\n",
    "        ivalues = [ivalues]\n",
    "\n",
    "        # Get run count\n",
    "        iruns = df_i.loc[species][\"n_runs\"]\n",
    "\n",
    "        # Turn string into literal\n",
    "        # imetrics = ast.literal_eval(imetrics)\n",
    "        # ivalues = ast.literal_eval(ivalues)\n",
    "        # Loop over spei metrics\n",
    "\n",
    "        for j, jmetric in enumerate(imetrics):\n",
    "            # Check if metric is already in dictionary\n",
    "            if jmetric not in dic_values:\n",
    "                dic_values[jmetric] = []\n",
    "                dic_counts[jmetric] = 0\n",
    "\n",
    "            # Attach value to dictionary\n",
    "            dic_values[jmetric].append(ivalues[j])\n",
    "            # Add count to dictionary\n",
    "            # if method_scaling_importance == \"sum\":\n",
    "            #     addit = 1\n",
    "            # elif method_scaling_importance == \"scaled_valid\":\n",
    "            #     addit = 1 / iruns\n",
    "            # elif method_scaling_importance == \"scaled_all\":\n",
    "            #     addit = 1 / n_runs\n",
    "            # else:\n",
    "            #     chime.error()\n",
    "            #     raise ValueError(\n",
    "            #         f\"🟥 method_scaling_importance not recognized: {method_scaling_importance}\"\n",
    "            #     )\n",
    "            addit = 1\n",
    "            dic_counts[jmetric] += addit\n",
    "\n",
    "            # Attach metric to dictionary\n",
    "            dataset_features.append(jmetric)\n",
    "\n",
    "    # Remove metric duplicates\n",
    "    dataset_features = list(set(dataset_features))\n",
    "    # Attach to dictionary\n",
    "    dic_preds[d] = dataset_features\n",
    "\n",
    "# dic\n",
    "# display(dic_preds)\n",
    "# display(dic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dictionary into dataframe\n",
    "df_counts = []\n",
    "for k in dic_counts.keys():\n",
    "    df_counts.append(pd.DataFrame({k: dic_counts[k]}, index=[0]))\n",
    "df_counts = pd.concat(df_counts, axis=1).T[0]\n",
    "\n",
    "# Round values to 0 decimals\n",
    "for i, n in enumerate(df_counts):\n",
    "    n = round(n, 2)\n",
    "    df_counts.iloc[i] = n\n",
    "\n",
    "# Order dic_preds by order from above\n",
    "order_l = list(colors_cat.keys())\n",
    "# Remove Temperature and SPEI because too many levels\n",
    "order_l.remove(\"SPEI\")\n",
    "order_l.remove(\"Temperature\")\n",
    "dic_ord = {order_l[i]: dic_preds[order_l[i]] for i in range(len(order_l))}\n",
    "\n",
    "# ! Counts plot\n",
    "# Create a 2x6 grid of subplots\n",
    "fig, axs = plt.subplots(\n",
    "    3,\n",
    "    3,\n",
    "    figsize=(14, 8),\n",
    "    sharex=True,\n",
    "    # sharey=True,\n",
    ")\n",
    "\n",
    "# Flatten the axs array\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop over each axis\n",
    "for i, ax in enumerate(axs):\n",
    "    # Check if i is smaller than the number of datasets\n",
    "    if i < len(dic_ord.keys()):\n",
    "        # Get variables of that dataset\n",
    "        cat = list(dic_ord.keys())[i]\n",
    "        df_i = df_counts[dic_ord[cat]].sort_values(ascending=False)\n",
    "        df_i = df_i / n_valid_models * 100\n",
    "        df_i = df_i.round(2)\n",
    "        # Create barplot\n",
    "        sns.barplot(\n",
    "            data=df_i, ax=ax, orient=\"h\", edgecolor=\"black\", color=colors_cat[cat]\n",
    "        )  # Colors is from the category importance stack barchart\n",
    "        # Add labels\n",
    "        ax.bar_label(\n",
    "            ax.containers[0],\n",
    "            label_type=\"edge\",\n",
    "            fontsize=10,\n",
    "            padding=3,\n",
    "            fontweight=\"bold\",\n",
    "            labels=[f\"{i:.0f} %\" for i in df_i],\n",
    "        )\n",
    "        # ax.set_xlabel(\"Number of Occurrences (weighted by number of valid runs)\")\n",
    "        # Set title\n",
    "        ax.set_title(list(dic_ord.keys())[i], weight=\"bold\")\n",
    "        # ax.set_xlim(0, df_counts.max().max() * 1.25)\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.set_xlabel(\"\")\n",
    "    else:\n",
    "        # Remove axis\n",
    "        ax.remove()\n",
    "\n",
    "\n",
    "if method_scaling_importance == \"sum\":\n",
    "    xlab = f\"Total occurences of feature across all models and species (%) \"  # (N = {supposed_n_runs})\"\n",
    "elif method_scaling_importance == \"scaled_valid\":\n",
    "    xlab = f\"Relative occurences of feature weighted by all runs per species (%) \"  # (N = {n_runs}, %)\"\n",
    "elif method_scaling_importance == \"scaled_all\":\n",
    "    xlab = f\"Relative occurences of feature weighted by valid runs per species (%)\"\n",
    "else:\n",
    "    chime.error()\n",
    "    raise ValueError(\n",
    "        f\"🟥 method_scaling_importance not recognized: {method_scaling_importance}\"\n",
    "    )\n",
    "\n",
    "xlab = \"Relative occurences of features across all species and models (%)\"\n",
    "\n",
    "# Give x label\n",
    "fig.text(\n",
    "    0.5,\n",
    "    -0.01,\n",
    "    xlab,\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    rotation=\"horizontal\",\n",
    "    fontsize=12,\n",
    "    weight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "os.makedirs(f\"{dir_patterns}/feature_counts\", exist_ok=True)\n",
    "plt.savefig(f\"{dir_patterns}/feature_counts/feature_counts.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie Charts (Fig. 3 & 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_pie = f\"{dir_patterns}pie_charts\"\n",
    "dir_pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_top9 = True\n",
    "add_boxes = False\n",
    "scale_by_nmodels = False\n",
    "scale_by100perc = True\n",
    "\n",
    "make_pie_chart_temp_v3(\n",
    "    single_or_double_change=\"double\",\n",
    "    dfchange1=df_for_pie_charts_single_temp,\n",
    "    dfchange2=df_for_pie_charts,\n",
    "    only_top9=only_top9,\n",
    "    size_scale=0.9,\n",
    "    fig_scale=1.5,\n",
    "    scale_by_nmodels=scale_by_nmodels,\n",
    "    scale_by100perc=scale_by100perc,\n",
    "    min_pie_size=0.0,\n",
    "    add_boxes=False,\n",
    "    dir_pie=dir_pie,\n",
    "    add_perc_to_legend=True,\n",
    "    verbose=True,\n",
    "    merge_legend=True,\n",
    ")\n",
    "\n",
    "make_pie_chart_spei_v4(\n",
    "    single_or_double_change=\"double\",\n",
    "    dfchange1=df_for_pie_charts_single_spei,\n",
    "    dfchange2=df_for_pie_charts,\n",
    "    only_top9=only_top9,\n",
    "    size_scale=1.1,\n",
    "    fig_scale=1.5,\n",
    "    scale_by_nmodels=scale_by_nmodels,\n",
    "    scale_by100perc=scale_by100perc,\n",
    "    min_pie_size=0.1,\n",
    "    add_boxes=False,\n",
    "    dir_pie=dir_pie,\n",
    "    add_perc_to_legend=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "df_tmp = df_for_pie_charts.query(\"species in @top9\")\n",
    "\n",
    "print(f\"SPEI ANOMALY DISTRIBUTION -------------------------\")\n",
    "print(f\"- warmer + drier conditions:\")\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier'\")[[\"spei_anom\"]]  # , \"spei_duration\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .sort_index()\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier'\")[[\"spei_season\"]]  # , \"spei_duration\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .sort_index()\n",
    "    .round(2)\n",
    ")\n",
    "print(f\"- warmer + wetter conditions:\")\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_wetter'\")[[\"spei_anom\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_wetter'\")[[\"spei_season\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(f\"TEMPERATURE ANOMALY DISTRIBUTION -------------------------\")\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier' or change == 'warmer_wetter'\")[\n",
    "        [\"temp_anom\", \"temp_season\"]\n",
    "    ]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier' or change == 'warmer_wetter'\")[[\"temp_anom\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier' or change == 'warmer_wetter'\")[\n",
    "        [\"temp_metric\"]\n",
    "    ]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_top9 = False\n",
    "add_boxes = False\n",
    "scale_by_nmodels = False\n",
    "scale_by100perc = True\n",
    "\n",
    "make_pie_chart_temp_v3(\n",
    "    single_or_double_change=\"double\",\n",
    "    dfchange1=df_for_pie_charts_single_temp,\n",
    "    dfchange2=df_for_pie_charts,\n",
    "    only_top9=only_top9,\n",
    "    size_scale=0.9,\n",
    "    fig_scale=1.5,\n",
    "    scale_by_nmodels=scale_by_nmodels,\n",
    "    scale_by100perc=scale_by100perc,\n",
    "    add_boxes=add_boxes,\n",
    "    dir_pie=dir_pie,\n",
    "    merge_legend=True,\n",
    "    add_perc_to_legend=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "make_pie_chart_spei_v4(\n",
    "    single_or_double_change=\"double\",\n",
    "    dfchange1=df_for_pie_charts_single_spei,\n",
    "    dfchange2=df_for_pie_charts,\n",
    "    only_top9=only_top9,\n",
    "    size_scale=1.1,\n",
    "    fig_scale=1.5,\n",
    "    scale_by_nmodels=scale_by_nmodels,\n",
    "    scale_by100perc=scale_by100perc,\n",
    "    add_boxes=add_boxes,\n",
    "    dir_pie=dir_pie,\n",
    "    add_perc_to_legend=True,\n",
    "    min_pie_size=0.1,\n",
    ")\n",
    "\n",
    "# Percentages ------------------------------------------------\n",
    "df_tmp = df_for_pie_charts.copy()\n",
    "\n",
    "print(f\"SPEI ANOMALY DISTRIBUTION -------------------------\")\n",
    "print(f\"- warmer + drier conditions:\")\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier'\")[[\"spei_anom\"]]  # , \"spei_duration\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .sort_index()\n",
    "    .round(2)\n",
    ")\n",
    "print(f\"- warmer + wetter conditions:\")\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_wetter'\")[[\"spei_anom\"]]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(f\"TEMPERATURE ANOMALY DISTRIBUTION -------------------------\")\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier' or change == 'warmer_wetter'\")[\n",
    "        [\"temp_anom\", \"temp_season\"]\n",
    "    ]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_tmp.query(\"change == 'warmer_drier' or change == 'warmer_wetter'\")[\n",
    "        [\"temp_metric\"]\n",
    "    ]\n",
    "    .value_counts(normalize=True)\n",
    "    .round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Find all metric files\n",
    "roc_paths = glob.glob(f\"{dir_runs}/run_*/**/rf_performance/roc_auc.csv\")\n",
    "pr_paths = glob.glob(f\"{dir_runs}/run_*/**/rf_performance/pr_auc.csv\")\n",
    "clf_paths = glob.glob(\n",
    "    f\"{dir_runs}/run_*/**/rf_performance/classification_metrics_fixed_threshold.csv\"\n",
    ")\n",
    "\n",
    "df_mean = get_metrics_for_all_models_and_species(\n",
    "    \"mean\",\n",
    "    roc_threshold,\n",
    "    roc_paths,\n",
    "    pr_paths,\n",
    "    clf_paths,\n",
    ")\n",
    "\n",
    "df_sd = get_metrics_for_all_models_and_species(\n",
    "    \"sd\",\n",
    "    roc_threshold,\n",
    "    roc_paths,\n",
    "    pr_paths,\n",
    "    clf_paths,\n",
    ")\n",
    "\n",
    "df_mean.to_csv(f\"{dir_tables}/model_performance_summary.csv\", index=False)\n",
    "df_sd.to_csv(f\"{dir_tables}/model_performance_summary_sd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_forest_utils import get_metrics_for_all_models_and_species\n",
    "\n",
    "df_metrics_plot = get_metrics_for_all_models_and_species(\n",
    "    \"mean\",\n",
    "    roc_threshold,\n",
    "    roc_paths,\n",
    "    pr_paths,\n",
    "    clf_paths,\n",
    "    return_before_aggregation=True,\n",
    ")\n",
    "\n",
    "# ! Filter by test ROC AUC threshold\n",
    "drop_pairs = df_metrics_plot.query(\n",
    "    \"metric == 'roc_auc' and dataset == 'test' and mean < @roc_threshold\"\n",
    ")[[\"model\", \"species\"]].drop_duplicates()\n",
    "df_metrics_plot = df_metrics_plot.merge(\n",
    "    drop_pairs, on=[\"model\", \"species\"], how=\"left\", indicator=True\n",
    ")\n",
    "df_metrics_plot = df_metrics_plot[df_metrics_plot[\"_merge\"] == \"left_only\"].drop(\n",
    "    columns=\"_merge\"\n",
    ")\n",
    "\n",
    "# ! Show only test dataset\n",
    "df_metrics_plot = df_metrics_plot.query(\"dataset == 'test'\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter necessary columns\n",
    "df_filtered = df_metrics_plot[[\"species\", \"metric\", \"mean\"]]\n",
    "df_filtered[\"species\"] = df_filtered[\"species\"].apply(shorten_species_names)\n",
    "\n",
    "# Get unique metrics and sort species by occurrence\n",
    "metrics = df_filtered[\"metric\"].unique()\n",
    "species_order = df_ntrees.species.tolist()\n",
    "species_order = [shorten_species_names(s) for s in species_order]\n",
    "\n",
    "# Set up the figure with one subplot per metric\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(metrics), figsize=(10, 12), sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(\n",
    "        data=df_filtered[df_filtered[\"metric\"] == metric],\n",
    "        y=\"species\",\n",
    "        x=\"mean\",\n",
    "        ax=ax,\n",
    "        order=species_order,\n",
    "        orient=\"h\",\n",
    "        fliersize=2,\n",
    "        linewidth=1,\n",
    "        width=0.5,\n",
    "    )\n",
    "    # ax.set_title(metric)\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    # Add grid lines\n",
    "    ax.grid(axis=\"y\", linestyle=\"dotted\", alpha=0.5)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\n",
    "            \"Species (sorted by decreasing number of trees per species)\",\n",
    "            weight=\"bold\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if metric == \"roc_auc\":\n",
    "        ax.set_xlabel(\"ROC AUC\", weight=\"bold\", fontsize=14)\n",
    "    elif metric == \"pr_auc\":\n",
    "        ax.set_xlabel(\"PR AUC\", weight=\"bold\", fontsize=14)\n",
    "    elif metric == \"f1\":\n",
    "        ax.set_xlabel(\"F1 Score\", weight=\"bold\", fontsize=14)\n",
    "    elif metric == \"precision\":\n",
    "        ax.set_xlabel(\"Precision\", weight=\"bold\", fontsize=14)\n",
    "    elif metric == \"recall\":\n",
    "        ax.set_xlabel(\"Recall\", weight=\"bold\", fontsize=14)\n",
    "\n",
    "# Remove spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(f\"{dir_patterns}/model_performance_boxplots.png\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Level Analyses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from IPython.display import clear_output  # For clearing the output of a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_change_variable(pattern, df_both_patterns, df_temp_patterns, df_spei_patterns):\n",
    "\n",
    "    # Get classification variable\n",
    "    if \"_\" in pattern:\n",
    "        return (\"change\", \"group\", df_both_patterns)\n",
    "\n",
    "    if pattern == \"warmer\" or pattern == \"cooler\":\n",
    "        return (\"response_temp\", \"group_temp\", df_temp_patterns)\n",
    "\n",
    "    if pattern == \"drier\" or pattern == \"wetter\":\n",
    "        return (\"response_spei\", \"group_spei\", df_spei_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_response_for_species_and_run(\n",
    "    irun, ispecies, ipattern_shap, add_response=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Process SHAP data and append the merged results to a given list.\n",
    "\n",
    "    Parameters:\n",
    "        irun (str): The run identifier.\n",
    "        ispecies (str): The species identifier.\n",
    "        ipattern_shap (str): The response pattern ('drier', 'wetter', 'warmer', 'cooler').\n",
    "    \"\"\"\n",
    "    # Get run-species directory\n",
    "    idir_species = glob.glob(f\"{dir_runs}/run_{irun}/{ispecies}/\")\n",
    "    if len(idir_species) > 1:\n",
    "        print(f\"{irun} - {ispecies}: {idir_species}\")\n",
    "        raise ValueError(\"🚨 More than one species dir found. Needs debugging...\")\n",
    "    idir_species = idir_species[0]\n",
    "\n",
    "    # Load feature data\n",
    "    X_shap = pd.read_csv(f\"{idir_species}/final_model/X_test.csv\", index_col=0)\n",
    "    feature_names = pd.Series(X_shap.columns)\n",
    "\n",
    "    # Identify the response variable\n",
    "    if ipattern_shap in [\"drier\", \"wetter\"]:\n",
    "        feat_response = glmm_get_spei_var(feature_names)\n",
    "    elif ipattern_shap in [\"warmer\", \"cooler\"]:\n",
    "        feat_response = glmm_get_temp_var(feature_names)\n",
    "    else:\n",
    "        raise ValueError(f\"🚨 Response variable not valid: {ipattern_shap}\")\n",
    "\n",
    "    pos_response = feature_names[feature_names == feat_response].index[0]\n",
    "\n",
    "    # Load SHAP data\n",
    "    shap_values = load_shap(f\"{idir_species}/shap/approximated/shap_values_test.pkl\")\n",
    "    shap_values = shap_values[:, pos_response, 1].values\n",
    "\n",
    "    # Attach SHAP values to feature data\n",
    "    X_shap[\"shap\"] = shap_values\n",
    "    X_shap[\"index\"] = X_shap.index\n",
    "\n",
    "    # Scale SHAP values\n",
    "    minmax = MinMaxScaler()\n",
    "    X_shap[\"shap\"] = minmax.fit_transform(\n",
    "        X_shap[\"shap\"].values.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "\n",
    "    # Load tree ID data\n",
    "    X_treeid = pd.read_csv(f\"{idir_species}/treeid/X_test_treeid.csv\").drop(\n",
    "        columns=[\"Unnamed: 0\"]\n",
    "    )\n",
    "    X_treeid = X_treeid.query(\"index in @X_shap.index\").copy()\n",
    "    X_treeid[\"index\"] = X_treeid.index.tolist()\n",
    "\n",
    "    # Merge SHAP values and tree ID data\n",
    "    X_merged = pd.merge(\n",
    "        X_shap[[\"index\", \"shap\"]], X_treeid[[\"index\", \"tree_id\"]], on=\"index\"\n",
    "    )\n",
    "\n",
    "    if add_response:\n",
    "        X_merged[feat_response] = X_shap[feat_response].values\n",
    "\n",
    "    return X_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_per_tree(\n",
    "    ispecies,\n",
    "    ipattern,\n",
    "    ipattern_shap,\n",
    "    subset_of_runs,\n",
    "    df_single_responses,\n",
    "    df_both_patterns,\n",
    "    df_temp_patterns,\n",
    "    df_spei_patterns,\n",
    "    nfi_sub,\n",
    "    add_anomaly_type_filter=None,\n",
    "    add_season_filter=None,\n",
    "    add_return_runs_only=False,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    - ispecies: requested species\n",
    "    - ipattern: requested pattern to filter for\n",
    "    - ipattern_shap: requested SHAP values to return\n",
    "    - subset_of_runs: how to filter for runs\n",
    "      - all: all runs, where single classification matches pattern\n",
    "      - grouped: all runs, where grouped classification matches pattern\n",
    "      - grouped_subset: all runs where grouped AND single classification matches pattern\n",
    "\n",
    "    - df_single_responses: Dataframe of responses classified at run-level\n",
    "    - df_both_patterns: Dataframe of responses classified at spei-temp pair group-level\n",
    "    - df_temp_patterns: Dataframe of temperature responses classified at feature group-level\n",
    "    - df_spei_patterns: Dataframe of spei responses classified at feature group-level\n",
    "\n",
    "    - nfi_sub: NFI data with tree_id, species, coordinates, tree_state_change\n",
    "    \"\"\"\n",
    "\n",
    "    # ! Check inputed pattern combination --------------------------------------------------\n",
    "    if \"_\" in ipattern and ipattern_shap is None:\n",
    "        print(f\"🚨 Requested SHAP values not in ipattern: {ipattern_shap} > {ipattern}\")\n",
    "        raise\n",
    "\n",
    "    if \"_\" in ipattern and ipattern_shap not in ipattern:\n",
    "        print(f\"🚨 Requested SHAP values not in ipattern: {ipattern_shap} > {ipattern}\")\n",
    "        raise\n",
    "\n",
    "    elif \"_\" not in ipattern and ipattern != ipattern_shap:\n",
    "        print(f\"🚨 Requested SHAP values not in ipattern: {ipattern_shap} > {ipattern}\")\n",
    "        raise\n",
    "\n",
    "    # ! Get change_var and grouped response df --------------------------------------------------\n",
    "    # Note: Input dfs are global variables and are not passed as arguments!\n",
    "    change_var, group_var, df_grouped_responses = get_change_variable(\n",
    "        ipattern, df_both_patterns, df_temp_patterns, df_spei_patterns\n",
    "    )\n",
    "\n",
    "    # Filter for the species\n",
    "    df_single_responses = df_single_responses.query(\"species == @ispecies\").sort_values(\n",
    "        \"run\"\n",
    "    )\n",
    "    df_grouped_responses = df_grouped_responses.query(\"species == @ispecies\")\n",
    "\n",
    "    # Get all runs with the pattern\n",
    "    all_runs = (\n",
    "        df_single_responses.query(f\"{change_var} == @ipattern\")[\"run\"].unique().tolist()\n",
    "    )\n",
    "\n",
    "    # Get all groups with the pattern\n",
    "    all_groups = (\n",
    "        df_grouped_responses.query(f\"{change_var} == @ipattern\")[group_var]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    # Turn into integer\n",
    "    all_groups = [int(x) for x in all_groups]\n",
    "\n",
    "    # print(f\" > Runs: {all_runs.__len__()}: {all_runs}\")\n",
    "\n",
    "    # Filter for runs\n",
    "    if subset_of_runs == \"all\":\n",
    "        df_single_responses = df_single_responses.query(f\"run in @all_runs\")\n",
    "    elif subset_of_runs == \"grouped\":\n",
    "        df_single_responses = df_single_responses.query(f\"group in @all_groups\")\n",
    "    elif subset_of_runs == \"grouped_subset\":\n",
    "        df_single_responses = df_single_responses.query(\n",
    "            f\"run in @all_runs and group in @all_groups\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"🚨 Subset of runs not valid: {subset_of_runs}\")\n",
    "\n",
    "    # ! Filter for SPEI anomaly type --------------------------------------------------\n",
    "    if add_anomaly_type_filter is not None:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"🚨 Careful: Filtering for anomalies is activated! {add_anomaly_type_filter}\"\n",
    "            )\n",
    "        valid_endings = [\"_min\", \"_mean\", \"_max\"]\n",
    "        if add_anomaly_type_filter not in valid_endings:\n",
    "            raise ValueError(\n",
    "                f\"🚨 add_anomaly_type_filter not valid: {add_anomaly_type_filter}. Valid are: {valid_endings}\"\n",
    "            )\n",
    "\n",
    "        # Filter for given anomaly\n",
    "        if ipattern_shap in [\"warmer\", \"cooler\"]:\n",
    "            anomaly_filter_variable = \"feature_temp\"\n",
    "            anomaly_pattern = f\"{add_anomaly_type_filter}\"\n",
    "        elif ipattern_shap in [\"drier\", \"wetter\"]:\n",
    "            anomaly_filter_variable = \"feature_spei\"\n",
    "            anomaly_pattern = f\"{add_anomaly_type_filter}\"\n",
    "\n",
    "        # Get all features matching anomaly ending\n",
    "        anomaly_subset = [\n",
    "            f\n",
    "            for f in df_single_responses[anomaly_filter_variable].unique()\n",
    "            if add_anomaly_type_filter in f\n",
    "        ]\n",
    "\n",
    "        # Filter for drought within one year\n",
    "        df_single_responses = df_single_responses[\n",
    "            df_single_responses[anomaly_filter_variable].isin(anomaly_subset)\n",
    "        ]\n",
    "\n",
    "    # return df_single_responses # For debugging\n",
    "    # ! Filter for season --------------------------------------------------\n",
    "    if add_season_filter is not None:\n",
    "        if verbose:\n",
    "            print(f\"🚨 Careful: Filtering for season is activated! {add_season_filter}\")\n",
    "        # valid_seasons = [\"ann\", \"spr\", \"sum\", \"aut\", \"win\"]\n",
    "        valid_seasons = [\"spr\"]  # currently only allowing spring\n",
    "        if add_season_filter not in valid_seasons:\n",
    "            raise ValueError(\n",
    "                f\"🚨 add_season_filter not valid: {add_season_filter}. Valid are: {valid_seasons}\"\n",
    "            )\n",
    "\n",
    "        # Get all features matching season ending\n",
    "        season_subset = [\n",
    "            f\n",
    "            for f in df_single_responses[\"feature_temp\"].unique()\n",
    "            if add_season_filter in f\n",
    "        ]\n",
    "\n",
    "        # Filter for drought within one year\n",
    "        df_single_responses = df_single_responses[\n",
    "            df_single_responses[\"feature_temp\"].isin(season_subset)\n",
    "        ]\n",
    "\n",
    "    # ! Continue --------------------------------------------------\n",
    "    # Get final runs to load\n",
    "    all_runs = df_single_responses[\"run\"].unique().tolist()\n",
    "    if add_return_runs_only:\n",
    "        return all_runs\n",
    "    # print(f\" > Runs: {all_runs.__len__()}: {all_runs}\")\n",
    "\n",
    "    # display(df_single_responses)\n",
    "    # print(f\" > Runs: {all_runs.__len__()} - Groups: {all_groups.__len__()}\")\n",
    "    # print(f\" > Runs: {all_runs} - Groups: {all_groups}\")\n",
    "\n",
    "    # Safety check\n",
    "    if all_runs.__len__() == 0:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"🚨 No runs found for: Species: {ispecies}  \\t| Pattern: {ipattern} \\t| SHAP Values for {ipattern_shap} \\t| Grouping Filter: {subset_of_runs}\\t| Anomaly Filter: {add_anomaly_type_filter} \\t| Season Filter: {add_season_filter}\"\n",
    "            )\n",
    "        return []\n",
    "\n",
    "    # ! Start loop to get data --------------------------------------------------\n",
    "    # Get empty list to store results\n",
    "    df = []\n",
    "\n",
    "    for irun in all_runs:\n",
    "\n",
    "        # Get run-species directory\n",
    "        idir_species = glob.glob(f\"{dir_runs}/run_{irun}/{ispecies}/\")\n",
    "        if idir_species.__len__() > 1:\n",
    "            print(\n",
    "                f\"all_runs: {all_runs} \\nirun: {irun} \\nispecies: {ispecies} \\nidir_species: {idir_species}\"\n",
    "            )\n",
    "            raise ValueError(f\"🚨 More than one species dir found. Needs debugging...\")\n",
    "        idir_species = idir_species[0]\n",
    "\n",
    "        # print(f\" Working on: {idir_species}\")\n",
    "\n",
    "        # ! Load feature data\n",
    "        X_shap = pd.read_csv(f\"{idir_species}/final_model/X_test.csv\", index_col=0)\n",
    "        feature_names = pd.Series(X_shap.columns)\n",
    "        # print(ipattern_shap)\n",
    "        if ipattern_shap == \"drier\" or ipattern_shap == \"wetter\":\n",
    "            feat_response = glmm_get_spei_var(feature_names)\n",
    "        elif ipattern_shap == \"warmer\" or ipattern_shap == \"cooler\":\n",
    "            feat_response = glmm_get_temp_var(feature_names)\n",
    "        else:\n",
    "            raise ValueError(f\"🚨 Response variable not valid: {ipattern_shap}\")\n",
    "\n",
    "        pos_response = feature_names[feature_names == feat_response].index[0]\n",
    "\n",
    "        # ! Load SHAP data\n",
    "        shap_values = load_shap(\n",
    "            f\"{idir_species}/shap/approximated/shap_values_test.pkl\"\n",
    "        )\n",
    "        shap_values = shap_values[:, pos_response, 1].values\n",
    "\n",
    "        # Attach shap values to X_shap\n",
    "        X_shap[\"shap\"] = shap_values\n",
    "        X_shap[\"index\"] = X_shap.index\n",
    "\n",
    "        # Scale shap to make it the effect comparable across species\n",
    "        minmax = MinMaxScaler()\n",
    "        X_shap[\"shap\"] = minmax.fit_transform(\n",
    "            X_shap[\"shap\"].values.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "\n",
    "        # ! Load tree id data\n",
    "        X_treeid = pd.read_csv(f\"{idir_species}/treeid/X_test_treeid.csv\").drop(\n",
    "            columns=[\"Unnamed: 0\"]\n",
    "        )\n",
    "        # Filter tree id rows for X_shap index (X_shap kept the original index)\n",
    "        X_treeid = X_treeid.query(\"index in @X_shap.index\").copy()\n",
    "        X_treeid[\"index\"] = X_treeid.index.tolist()\n",
    "        # Attach treeid to shap data\n",
    "        X_merged = pd.merge(\n",
    "            X_shap[[\"index\", \"shap\"]], X_treeid[[\"index\", \"tree_id\"]], on=\"index\"\n",
    "        )\n",
    "\n",
    "        # ! Append to list\n",
    "        df.append(get_shap_response_for_species_and_run(irun, ispecies, ipattern_shap))\n",
    "\n",
    "    # ! Finalize tree-level data --------------------------------------------------\n",
    "    # Concatenate\n",
    "    df = pd.concat(df)\n",
    "\n",
    "    # Safety Check: Check if df is not empty\n",
    "    if df.shape[0] == 0:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\" 🚨 No runs found for: Species: {ispecies} - Pattern: {ipattern} - SHAP Values: {ipattern_shap} | Filtering for: {subset_of_runs} | anomaly subset: {add_anomaly_type_filter}\"\n",
    "            )\n",
    "        return []\n",
    "    # else:\n",
    "    #     print(\n",
    "    #         f\" ✅ Species: {ispecies} - Pattern: {ipattern} - SHAP Values: {ipattern_shap} | Filtering for: {subset_of_runs}\"\n",
    "    #     )\n",
    "\n",
    "    # Take mean and std of shap per tree (if same tree occurred in test set of multiple runs)\n",
    "    df = (\n",
    "        df.groupby(\"tree_id\")[\"shap\"]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .reset_index()\n",
    "        .sort_values(\"mean\")\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns={\"mean\": \"shap\"})\n",
    "    df_di = nfi_sub.query(\"species_lat2 == @ispecies\")\n",
    "\n",
    "    # Attach coordinates\n",
    "    df = pd.merge(df, nfi_sub, on=\"tree_id\", how=\"left\")\n",
    "\n",
    "    # Rename shap value to ipattern_shap\n",
    "    df = df.rename(columns={\"shap\": ipattern_shap})\n",
    "\n",
    "    # Add number of runs used\n",
    "    df[\"n_runs\"] = len(all_runs)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dens_get_data(subset_of_runs, **kwargs):\n",
    "    # Get data for all species\n",
    "    i = 0\n",
    "    df_species = []\n",
    "\n",
    "    # Hardcoding patterns\n",
    "    ipattern_x = \"drier\"\n",
    "    ipattern_shap_x = \"drier\"\n",
    "\n",
    "    ipattern_y = \"wetter\"\n",
    "    ipattern_shap_y = \"wetter\"\n",
    "\n",
    "    # for ispecies in (\n",
    "    #     df_plot.sort_values(\"n_trees_total\", ascending=False).head(20).species.tolist()\n",
    "    # ):\n",
    "\n",
    "    add_season_filter = None  #\n",
    "\n",
    "    for ispecies in species_in_final_anlysis:\n",
    "\n",
    "        # print(ispecies, i)\n",
    "        df_1 = get_shap_per_tree(\n",
    "            ispecies=ispecies,\n",
    "            ipattern=ipattern_x,\n",
    "            ipattern_shap=ipattern_shap_x,\n",
    "            subset_of_runs=subset_of_runs,\n",
    "            df_single_responses=df_responses.copy(),\n",
    "            df_both_patterns=df_both_patterns.copy(),\n",
    "            df_temp_patterns=df_temp_patterns.copy(),\n",
    "            df_spei_patterns=df_spei_patterns.copy(),\n",
    "            add_anomaly_type_filter=add_anomaly_type_filter,\n",
    "            add_season_filter=add_season_filter,\n",
    "            nfi_sub=nfi_sub.copy(),\n",
    "        )\n",
    "\n",
    "        df_2 = get_shap_per_tree(\n",
    "            ispecies=ispecies,\n",
    "            ipattern=ipattern_y,\n",
    "            ipattern_shap=ipattern_shap_y,\n",
    "            subset_of_runs=subset_of_runs,\n",
    "            df_single_responses=df_responses.copy(),\n",
    "            df_both_patterns=df_both_patterns.copy(),\n",
    "            df_temp_patterns=df_temp_patterns.copy(),\n",
    "            df_spei_patterns=df_spei_patterns.copy(),\n",
    "            add_anomaly_type_filter=add_anomaly_type_filter,\n",
    "            add_season_filter=add_season_filter,\n",
    "            nfi_sub=nfi_sub.copy(),\n",
    "        )\n",
    "\n",
    "        if len(df_1) == 0:\n",
    "            print(f\"No data available for {ipattern_x} - {ipattern_shap_x}\")\n",
    "            continue\n",
    "        elif len(df_2) == 0:\n",
    "            print(f\"No data available for {ipattern_y} - {ipattern_shap_y}\")\n",
    "            continue\n",
    "        else:\n",
    "            # Merge the two dataframes\n",
    "            df_lm = pd.merge(\n",
    "                df_1,\n",
    "                df_2,\n",
    "                on=\"tree_id\",\n",
    "                how=\"inner\",\n",
    "            )[\n",
    "                [\"tree_id\", ichange_x, ichange_y]\n",
    "            ].sort_values(ichange_x)\n",
    "\n",
    "            df_lm[\"species\"] = ispecies\n",
    "            df_species.append(df_lm)\n",
    "\n",
    "            # plot_kde_with_regression(df_lm[\"drier\"], df_lm[\"wetter\"], ax=axs[i])\n",
    "            # axs[i].set_title(ispecies)\n",
    "        # i += 1\n",
    "\n",
    "    clear_output()\n",
    "    return df_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dens_add_stats(df_species, nfi_sub, species_change_single):\n",
    "    #\n",
    "    # ! Combine all species\n",
    "    df_species_all = pd.concat(df_species)\n",
    "\n",
    "    # ! Check how many of the trees were used in both datasets - add to title or to plots?\n",
    "    nfi_sub_alivedead = nfi_sub.query(\n",
    "        \"tree_state_change == 'alive_dead' or tree_state_change == 'alive_alive'\"\n",
    "    )\n",
    "    nfi_sub_alivedead_grouped = (\n",
    "        nfi_sub_alivedead.groupby(\"species_lat2\")\n",
    "        .agg({\"tree_id\": \"count\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"tree_id\": \"n_trees_all\", \"species_lat2\": \"species\"})\n",
    "    )\n",
    "\n",
    "    df_species_all_grouped = (\n",
    "        df_species_all.groupby(\"species\")\n",
    "        .agg({\"tree_id\": \"count\"})\n",
    "        .reset_index()\n",
    "        .rename(columns={\"tree_id\": \"n_trees_shared\"})\n",
    "    )\n",
    "\n",
    "    df_species_all_grouped = pd.merge(\n",
    "        df_species_all_grouped, nfi_sub_alivedead_grouped, on=\"species\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_species_all_grouped[\"n_trees_shared_perc\"] = (\n",
    "        (\n",
    "            df_species_all_grouped[\"n_trees_shared\"]\n",
    "            / df_species_all_grouped[\"n_trees_all\"]\n",
    "            * 100\n",
    "        )\n",
    "        .round(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    df_species_all = df_species_all.merge(\n",
    "        df_species_all_grouped, on=\"species\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # ! Add some text vars\n",
    "    df_species_all[\"species\"] = df_species_all[\"species\"].replace(\n",
    "        {\"Populus\": \"Populus spp.\"}\n",
    "    )\n",
    "\n",
    "    df_species_all[\"species_short\"] = (\n",
    "        df_species_all[\"species\"].str.split(\" \", n=1, expand=True)[0].str[0]\n",
    "        + \". \"\n",
    "        + df_species_all[\"species\"].str.split(\" \", n=1, expand=True)[1]\n",
    "    )\n",
    "\n",
    "    df_species_all[\"species_short\"] = df_species_all[\"species_short\"].replace(\n",
    "        {\"P. spp.\": \"Populus\"}\n",
    "    )\n",
    "\n",
    "    df_species_all[\"txt_perc\"] = (\n",
    "        \" (\" + df_species_all[\"n_trees_shared_perc\"].astype(str) + \"%)\"\n",
    "    )\n",
    "\n",
    "    df_species_all[\"species_perc\"] = (\n",
    "        df_species_all[\"species\"] + df_species_all[\"txt_perc\"]\n",
    "    )\n",
    "\n",
    "    df_species_all[\"species_short_perc\"] = (\n",
    "        df_species_all[\"species_short\"] + df_species_all[\"txt_perc\"]\n",
    "    )\n",
    "\n",
    "    # df_species_all[\n",
    "    #     [\"species\", \"species_short\", \"n_trees_shared\", \"n_trees_all\", \"n_trees_shared_perc\"]\n",
    "    # ].drop_duplicates().sort_values(\"species\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # # ! Attach model split information\n",
    "    df_xxx = species_change_single.copy().reset_index()\n",
    "\n",
    "    df_xxx[\"nValidModels\"] = (\n",
    "        df_xxx[\"warmer\"] + df_xxx[\"cooler\"] + df_xxx[\"temp_unclear\"]\n",
    "    )\n",
    "\n",
    "    # Turn every numeric into a string\n",
    "    for col in df_xxx.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df_xxx[col]) and col != \"nValidModels\":\n",
    "            # df_xxx[col] = df_xxx[col] / df_xxx[\"nValidModels\"] * 100\n",
    "            df_xxx[col] = df_xxx[col].round(0)\n",
    "            df_xxx[col] = df_xxx[col].astype(int).astype(str)\n",
    "\n",
    "    df_xxx[\"split_temp_wco\"] = \"\"\n",
    "    df_xxx[\"split_spei_dwo\"] = \"\"\n",
    "    df_xxx[\"split_both_xxx\"] = \"\"\n",
    "\n",
    "    df_xxx[\"split_temp_wco\"] = (\n",
    "        \"wa/co/ns = \"\n",
    "        + df_xxx[\"warmer\"]\n",
    "        + \"/\"\n",
    "        + df_xxx[\"cooler\"]\n",
    "        + \"/\"\n",
    "        + df_xxx[\"temp_unclear\"]\n",
    "    )\n",
    "    df_xxx[\"split_spei_dwo\"] = (\n",
    "        \"dr/we/ns = \"\n",
    "        + df_xxx[\"drier\"]\n",
    "        + \"/\"\n",
    "        + df_xxx[\"wetter\"]\n",
    "        + \"/\"\n",
    "        + df_xxx[\"spei_unclear\"]\n",
    "    )\n",
    "\n",
    "    df_species_all = df_species_all.merge(\n",
    "        df_xxx[[\"species\", \"split_temp_wco\", \"split_spei_dwo\", \"split_both_xxx\"]],\n",
    "        on=\"species\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    df_xxx\n",
    "    # df_species_all\n",
    "\n",
    "    return df_species_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde_with_regression(\n",
    "    x,\n",
    "    y,\n",
    "    ax=None,\n",
    "    add_equation=True,\n",
    "    corr_method=\"r\",\n",
    "    ncountours=11,\n",
    "    shared_perc=None,\n",
    "    sharedy=0.1,\n",
    "    corr_text_y=0.2,\n",
    "    text_3=None,\n",
    "    ylims=None,\n",
    "    xlims=None,\n",
    "    force11=False,\n",
    "):\n",
    "\n",
    "    # ncontour check\n",
    "    if ncountours not in [5, 6, 11]:\n",
    "        raise ValueError(\n",
    "            f\"🚨 ncountours not valid: {ncountours}. For 10-spacing, use '11'\"\n",
    "        )\n",
    "\n",
    "    # Ensure input is numpy array\n",
    "    x = np.array(x).reshape(-1, 1)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if corr_method == \"lm\":\n",
    "        # Fit linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(x, y)\n",
    "        y_pred = model.predict(x)\n",
    "\n",
    "        # Calculate R-squared\n",
    "        r_squared = model.score(x, y)\n",
    "\n",
    "        # Calculate p-value\n",
    "        n = len(y)\n",
    "        residuals = y - y_pred\n",
    "        sse = np.sum(residuals**2)\n",
    "        mse = sse / (n - 2)\n",
    "        se_slope = np.sqrt(mse / np.sum((x - np.mean(x)) ** 2))\n",
    "        t_stat = model.coef_[0] / se_slope\n",
    "        p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=n - 2))\n",
    "\n",
    "        # Determine line style based on significance\n",
    "        linestyle = \"dotted\" if p_value > 0.05 else \"solid\"\n",
    "\n",
    "        # Display regression equation\n",
    "        coef = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        equation = (\n",
    "            f\"{coef:.2f}x + {intercept:.2f} | R² = {r_squared:.2f} | p = {p_value:.2f}\"\n",
    "        )\n",
    "\n",
    "        # Get Regression line\n",
    "        # Reduce x-range to 10% and 90% quantiles to avoid extrapolation\n",
    "        x_range = np.quantile(x.flatten(), [0.05, 0.95])\n",
    "        x_range = np.linspace(x_range[0], x_range[1], 100).reshape(-1, 1)\n",
    "        y_pred = model.predict(x_range)\n",
    "\n",
    "    elif corr_method == \"r\":\n",
    "        # Calculate pearson-r\n",
    "        # r = stats.pearsonr(x.flatten(), y)[0]\n",
    "        # p = stats.pearsonr(x.flatten(), y)[1]\n",
    "\n",
    "        # Calculate spearman-r\n",
    "        spearman_ = spearmanr(x.flatten(), y)\n",
    "        r = spearman_.statistic\n",
    "        p = spearman_.pvalue\n",
    "\n",
    "        # Formula\n",
    "        sign = \"^{\\\\asterisk}\" if p < 0.05 else \"\"\n",
    "        # equation = f\"r = {r:.2f}\"\n",
    "        equation = f\"$r = {r:.2f}{sign}$\"\n",
    "        print(f\"r = {r:.2f} | p = {p:.2f}\")\n",
    "    elif corr_method == \"mk\":\n",
    "        # Calculate Mann-Kendall\n",
    "        mk = get_mk_test_raw(x, y, pval_threshold=0.05)\n",
    "        sign = \"^{\\\\asterisk}\" if mk[\"p\"].values[0] < 0.05 else \"\"\n",
    "        tau = mk[\"tau\"].values[0].round(2)\n",
    "        equation = f\"$\\\\tau = {tau}{sign}$\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"🚨 corr_method not valid: {corr_method}\")\n",
    "\n",
    "    # If no ax is provided, create a new figure and axis\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # KDE plot with colorbar normalized to 0-100%\n",
    "    kde = sns.kdeplot(\n",
    "        x=x.flatten(),\n",
    "        y=y.flatten(),\n",
    "        cmap=\"Purples\",\n",
    "        fill=True,\n",
    "        alpha=0.8,\n",
    "        levels=ncountours,\n",
    "        ax=ax,\n",
    "        cbar=True,\n",
    "        cbar_kws={\"label\": \"Density (%)\"},\n",
    "    )\n",
    "\n",
    "    # Update the colorbar ticks to reflect 0-100% in a continuous way\n",
    "    cbar = kde.collections[-1].colorbar\n",
    "    if ncountours == 5:\n",
    "        cbar.set_ticklabels([\"0\", \"25\", \"50\", \"75\", \"100\"])\n",
    "    elif ncountours == 6:\n",
    "        cbar.set_ticklabels([\"0\", \"20\", \"40\", \"60\", \"80\", \"100\"])\n",
    "    elif ncountours == 11:\n",
    "        cbar.set_ticklabels([\"0\", \"\", \"20\", \"\", \"40\", \"\", \"60\", \"\", \"80\", \"\", \"100\"])\n",
    "\n",
    "    # LM\n",
    "    if corr_method == \"lm\":\n",
    "        ax.plot(x_range, y_pred, color=\"red\", linewidth=2, linestyle=linestyle)\n",
    "\n",
    "    # Add limits\n",
    "    if xlims is not None:\n",
    "        ax.set_xlim(xlims)\n",
    "\n",
    "    if ylims is not None:\n",
    "        ax.set_ylim(ylims)\n",
    "\n",
    "    # Make sure the aspect ratio is 1:1\n",
    "    if force11:\n",
    "        y0, y1 = ax.get_ylim()\n",
    "        x0, x1 = ax.get_xlim()\n",
    "        ax.set_aspect(abs(x1 - x0) / abs(y1 - y0))\n",
    "\n",
    "    if add_equation:\n",
    "        ax.text(\n",
    "            0.05,\n",
    "            corr_text_y,\n",
    "            equation,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment=\"top\",\n",
    "            color=\"black\",\n",
    "            fontweight=\"normal\",\n",
    "        )\n",
    "\n",
    "    if shared_perc is not None:\n",
    "        ax.text(\n",
    "            0.05,\n",
    "            sharedy,\n",
    "            \"shared: \" + str(shared_perc) + \"%\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            fontweight=\"normal\",\n",
    "        )\n",
    "\n",
    "    if text_3 is not None:\n",
    "        ax.text(\n",
    "            text_3[0],\n",
    "            text_3[1],\n",
    "            text_3[2],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment=\"bottom\",\n",
    "            color=\"black\",\n",
    "            fontweight=\"normal\",\n",
    "        )\n",
    "\n",
    "    # Set labels\n",
    "    ax.set_xlabel(\"Sensitivity to Drier Cond.\")\n",
    "    ax.set_ylabel(\"Sensitivity to Wetter Cond.\")\n",
    "    # ax.set_title(\"KDE with Linear Regression\")\n",
    "\n",
    "    # Format xy ticks to 2f\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: \"{:.1f}\".format(x)))\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: \"{:.1f}\".format(x)))\n",
    "\n",
    "    # If no ax was provided, show the plot\n",
    "    if ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dens_make_plot(\n",
    "    df_species_all,\n",
    "    top_species_all,\n",
    "    xchange,\n",
    "    ychange,\n",
    "    corr_method,\n",
    "    subset_of_runs,\n",
    "    dir_treelevel,\n",
    "    ncountours=6,\n",
    "    species_label_size=12,\n",
    "    axis_label_size=12,\n",
    "    species_to_plot=[\"top9\", \"all\"],\n",
    "    xylimits=(0, 1),\n",
    "):\n",
    "\n",
    "    # ! Start loop over top9 and all species!\n",
    "    # for top9_or_all in [\"all\"]:\n",
    "    for top9_or_all in species_to_plot:\n",
    "        print(\n",
    "            f\"Working on {top9_or_all} - {subset_of_runs} - {corr_method} - {ncountours} - {xchange} - {ychange}\"\n",
    "        )\n",
    "        if top9_or_all == \"all\":\n",
    "            fig, axs = plt.subplots(\n",
    "                4,\n",
    "                10,\n",
    "                figsize=(24, 10),\n",
    "                sharex=True,\n",
    "                sharey=True,\n",
    "                constrained_layout=False,\n",
    "            )\n",
    "            species_list = df_species_all.species.unique().tolist()\n",
    "\n",
    "            corr_text_y = 0.4\n",
    "            splity = 0.09\n",
    "\n",
    "            cbar_width = 0.01\n",
    "\n",
    "            ylabel_x = 0.1\n",
    "            ylabel_y = 0.25\n",
    "\n",
    "            xlabel_x = 0.5\n",
    "            xlabel_y = 0.05\n",
    "\n",
    "            x_text = f\"Sensitivity to {xchange} conditions (normalized SHAP values)\"\n",
    "            y_text = f\"Sensitivity to {ychange} conditions (normalized SHAP values)\"\n",
    "\n",
    "            axis_label_size = 14\n",
    "\n",
    "        elif top9_or_all == \"top9\":\n",
    "            fig, axs = plt.subplots(\n",
    "                2,  # 3\n",
    "                4,  # 3\n",
    "                figsize=(10, 5),  # 9,9\n",
    "                sharex=True,\n",
    "                sharey=True,\n",
    "                constrained_layout=False,\n",
    "            )\n",
    "            species_list = top_species_all.head(9).species.tolist()\n",
    "\n",
    "            corr_text_y = 0.3\n",
    "            splity = 0.04\n",
    "\n",
    "            cbar_width = 0.02\n",
    "\n",
    "            ylabel_x = 0.05\n",
    "            ylabel_y = 0.22  # 0.5 + va = \"center\", not ha\n",
    "\n",
    "            xlabel_x = 0.5\n",
    "            xlabel_y = -0.04  # 0.04\n",
    "\n",
    "            x_text = f\"Sensitivity to {xchange} conditions\\n(normalized SHAP values)\"\n",
    "            y_text = f\"Sensitivity to {ychange} conditions\\n(normalized SHAP values)\"\n",
    "\n",
    "            axis_label_size = 12\n",
    "\n",
    "        if ncountours == 5:\n",
    "            cbar_labels = [\"0\", \"25\", \"50\", \"75\", \"100\"]\n",
    "        elif ncountours == 6:\n",
    "            cbar_labels = [\"0\", \"20\", \"40\", \"60\", \"80\", \"100\"]\n",
    "        elif ncountours == 11:\n",
    "            cbar_labels = [\"0\", \"\", \"20\", \"\", \"40\", \"\", \"60\", \"\", \"80\", \"\", \"100\"]\n",
    "\n",
    "        # Remove species in species_list that are not in df_species_all (those that have both - WW and WD models)\n",
    "        species_list = [x for x in species_list if x in df_species_all.species.unique()]\n",
    "        # Frangula alnus causes issues, removing it\n",
    "        species_list = [x for x in species_list if x != \"Frangula alnus\"]\n",
    "\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        for i, ispecies in enumerate(species_list):\n",
    "\n",
    "            print(f\" - Working on {ispecies} ({i+1}/{len(species_list)})\")\n",
    "            # Prevent index error if there are more species than subplots\n",
    "            if i >= len(axs):\n",
    "                break\n",
    "\n",
    "            df_lm = df_species_all.query(\"species == @ispecies\")\n",
    "\n",
    "            plot_kde_with_regression(\n",
    "                df_lm[xchange].sample(frac=0.1, random_state=42),\n",
    "                df_lm[ychange].sample(frac=0.1, random_state=42),\n",
    "                ax=axs[i],\n",
    "                corr_method=corr_method,\n",
    "                ncountours=ncountours,\n",
    "                shared_perc=df_lm[\"n_trees_shared_perc\"].unique()[0],\n",
    "                corr_text_y=corr_text_y,\n",
    "                text_3=None,\n",
    "                # text_3=(0.05, splity, df_lm[\"split_spei_dwo\"].unique()[0]),\n",
    "            )\n",
    "\n",
    "            # Remove colorbar\n",
    "            axs[i].collections[-1].colorbar.remove()\n",
    "\n",
    "            # Give title to each subplot\n",
    "            if top9_or_all == \"all\":\n",
    "                species_name = df_lm[\"species_short\"].unique()[0]\n",
    "            else:\n",
    "                species_name = df_lm[\"species\"].unique()[0]\n",
    "\n",
    "            axs[i].set_title(\n",
    "                species_name,\n",
    "                fontsize=species_label_size,\n",
    "                fontweight=\"normal\",\n",
    "            )\n",
    "            axs[i].set_xlabel(\"\")\n",
    "            axs[i].set_ylabel(\"\")\n",
    "\n",
    "            # Increase tick size\n",
    "            axs[i].tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "\n",
    "            # Show every second tick\n",
    "            axs[i].xaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "            axs[i].yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "\n",
    "            # Set x and y limits\n",
    "            axs[i].set_xlim(xylimits)\n",
    "            axs[i].set_ylim(xylimits)\n",
    "\n",
    "        # Remove empty subplots\n",
    "        for i in range(len(species_list), len(axs)):\n",
    "            axs[i].axis(\"off\")\n",
    "\n",
    "        # Add colorbar on the right\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, cbar_width, 0.7])\n",
    "        cbar = plt.colorbar(axs[0].collections[0], cax=cbar_ax)\n",
    "        cbar.set_label(\"Density (%)\", fontsize=axis_label_size)\n",
    "        cbar.set_ticklabels(cbar_labels)\n",
    "\n",
    "        # Add global axis labels\n",
    "        fig.text(\n",
    "            xlabel_x,\n",
    "            xlabel_y,\n",
    "            x_text,\n",
    "            ha=\"center\",\n",
    "            fontsize=axis_label_size,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        fig.text(\n",
    "            ylabel_x,\n",
    "            ylabel_y,\n",
    "            y_text,\n",
    "            ha=\"center\",\n",
    "            rotation=\"vertical\",\n",
    "            fontsize=axis_label_size,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "        # plt.tight_layout(w_pad=1, h_pad=1)\n",
    "        plt.savefig(\n",
    "            f\"{dir_treelevel}/{top9_or_all}-corr_{corr_method}-{ychange}_vs_{xchange}-subset_of_runs_{subset_of_runs}.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=300,\n",
    "        )\n",
    "\n",
    "        plt.savefig(\n",
    "            f\"{dir_treelevel}/{top9_or_all}-corr_{corr_method}-{ychange}_vs_{xchange}-subset_of_runs_{subset_of_runs}.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directory\n",
    "dir_treelevel = f\"{dir_patterns}/tree-level\"\n",
    "os.makedirs(dir_treelevel, exist_ok=True)\n",
    "\n",
    "# Get NFI Data\n",
    "nfi_raw = get_final_nfi_data_for_analysis(verbose=False)\n",
    "nfi_sub = nfi_raw[\n",
    "    [\"tree_id\", \"lat_fr\", \"lon_fr\", \"tree_state_change\", \"species_lat2\", \"campagne_2\"]\n",
    "]\n",
    "\n",
    "# Get species data\n",
    "species_in_final_anlysis = get_species_with_models(\"list\")\n",
    "species_in_final_anlysis = list(sorted(species_in_final_anlysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many species do not have both, warmer wetter and warmer drier models\n",
    "no_wd = sorted(df_plot_from_mulitplot.query(\"warmer_drier == 0\").species.tolist())\n",
    "no_ww = sorted(df_plot_from_mulitplot.query(\"warmer_wetter == 0\").species.tolist())\n",
    "unique_no_ww_wd = list(set(no_wd + no_ww))\n",
    "\n",
    "print(f\" {no_wd.__len__()} Species without warmer_drier pattern: {no_wd}\")\n",
    "print(f\" {no_ww.__len__()} Species without warmer_wetter pattern: {no_ww}\")\n",
    "print(\n",
    "    f\" {unique_no_ww_wd.__len__()} Species without neither pattern: {unique_no_ww_wd} (N species: {species_in_final_anlysis.__len__()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! Settings\n",
    "plot_shap = \"raw\"  # raw scaled norm\n",
    "sort_shap_asceding = True\n",
    "scale_shap_within_model = False\n",
    "dot_alpha_scaled = False\n",
    "dot_color = \"black\"\n",
    "dot_alpha_scaler = 1\n",
    "jitter_amount = 0\n",
    "dead_only = False\n",
    "ax_in = None  # Optional axis argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shap_per_tree(\n",
    "    ispecies=\"Fagus sylvatica\",\n",
    "    # ispecies=\"Abies alba\",\n",
    "    ipattern=\"warmer_wetter\",\n",
    "    ipattern_shap=\"warmer\",\n",
    "    subset_of_runs=\"all\",\n",
    "    add_anomaly_type_filter=None,\n",
    "    df_single_responses=df_responses.copy(),\n",
    "    df_both_patterns=df_both_patterns.copy(),\n",
    "    df_temp_patterns=df_temp_patterns.copy(),\n",
    "    df_spei_patterns=df_spei_patterns.copy(),\n",
    "    nfi_sub=nfi_sub.copy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for one species\n",
    "ispecies = \"Carpinus betulus\"\n",
    "ichange_x = \"drier\"\n",
    "ichange_y = \"wetter\"\n",
    "subset_of_runs = \"grouped_subset\"\n",
    "\n",
    "df_1 = get_shap_per_tree(\n",
    "    ispecies=ispecies,\n",
    "    ipattern=\"warmer_drier\",\n",
    "    ipattern_shap=ichange_x,\n",
    "    subset_of_runs=subset_of_runs,\n",
    "    df_single_responses=df_responses.copy(),\n",
    "    df_both_patterns=df_both_patterns.copy(),\n",
    "    df_temp_patterns=df_temp_patterns.copy(),\n",
    "    df_spei_patterns=df_spei_patterns.copy(),\n",
    "    add_anomaly_type_filter=None,\n",
    "    nfi_sub=nfi_sub.copy(),\n",
    ")\n",
    "\n",
    "df_2 = get_shap_per_tree(\n",
    "    ispecies=ispecies,\n",
    "    ipattern=\"warmer_wetter\",\n",
    "    ipattern_shap=ichange_y,\n",
    "    subset_of_runs=subset_of_runs,\n",
    "    df_single_responses=df_responses.copy(),\n",
    "    df_both_patterns=df_both_patterns.copy(),\n",
    "    df_temp_patterns=df_temp_patterns.copy(),\n",
    "    df_spei_patterns=df_spei_patterns.copy(),\n",
    "    add_anomaly_type_filter=None,\n",
    "    nfi_sub=nfi_sub.copy(),\n",
    ")\n",
    "\n",
    "# Merge the two dataframes\n",
    "df_lm = pd.merge(\n",
    "    df_1,\n",
    "    df_2,\n",
    "    on=\"tree_id\",\n",
    "    how=\"inner\",\n",
    ")[\n",
    "    [\"tree_id\", ichange_x, ichange_y]\n",
    "].sort_values(ichange_x)\n",
    "\n",
    "# clear_output()\n",
    "plot_kde_with_regression(\n",
    "    df_lm[ichange_x],\n",
    "    df_lm[ichange_y],\n",
    "    text_3=(0.05, 0.6, ispecies),\n",
    "    corr_method=\"r\",\n",
    "    ncountours=6,\n",
    "    shared_perc=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig. 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_of_runs = \"all\"\n",
    "# subset_of_runs = \"grouped\"\n",
    "subset_of_runs = \"grouped_subset\"\n",
    "\n",
    "# Define analysis type\n",
    "if which_analysis == \"default\":\n",
    "    species_to_plot = [\"top9\", \"all\"]\n",
    "else:\n",
    "    species_to_plot = [\"top9\"]\n",
    "\n",
    "if which_analysis == \"smotek_0\":\n",
    "    xylimits = (0, 0.6)\n",
    "else:\n",
    "    xylimits = (0, 1)\n",
    "\n",
    "# Define anomaly subset if needed\n",
    "add_anomaly_type_filter = None\n",
    "\n",
    "# for subset_of_runs in [\"all\", \"grouped\", \"grouped_subset\"]:\n",
    "for subset_of_runs in [\"all\"]:\n",
    "    data_for_density_plots = dens_get_data(\n",
    "        subset_of_runs=subset_of_runs,\n",
    "        df_responses=df_responses,\n",
    "        df_both_patterns=df_both_patterns,\n",
    "        df_temp_patterns=df_temp_patterns,\n",
    "        df_spei_patterns=df_spei_patterns,\n",
    "        add_anomaly_type_filter=add_anomaly_type_filter,\n",
    "        # add_season_filter=\"spr\",\n",
    "        nfi_sub=nfi_sub,\n",
    "    )\n",
    "\n",
    "    data_for_density_plots = dens_add_stats(\n",
    "        data_for_density_plots,\n",
    "        nfi_sub=nfi_sub,\n",
    "        species_change_single=species_change_single,\n",
    "    )\n",
    "\n",
    "    # for icorr_method in [\"mk\", \"r\"]:\n",
    "    for icorr_method in [\"r\"]:\n",
    "\n",
    "        dens_make_plot(\n",
    "            df_species_all=data_for_density_plots,\n",
    "            top_species_all=top_species_all,\n",
    "            subset_of_runs=subset_of_runs,\n",
    "            xchange=\"drier\",\n",
    "            ychange=\"wetter\",\n",
    "            corr_method=icorr_method,\n",
    "            dir_treelevel=dir_treelevel,\n",
    "            ncountours=6,\n",
    "            species_label_size=12,\n",
    "            axis_label_size=12,\n",
    "            species_to_plot=species_to_plot,\n",
    "            xylimits=xylimits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from the figures above\n",
    "# Top9:\n",
    "r_t9 = [\n",
    "    0.29,\n",
    "    0.26,\n",
    "    0.36,\n",
    "    0.61,\n",
    "    0.19,\n",
    "    0.38,\n",
    "    0.52,\n",
    "    0.38,\n",
    "]\n",
    "\n",
    "# All:\n",
    "r_all = [\n",
    "    0.52,\n",
    "    0.20,\n",
    "    0.13,\n",
    "    0.17,\n",
    "    0.46,\n",
    "    0.40,\n",
    "    0.43,\n",
    "    0.33,\n",
    "    0.61,\n",
    "    0.19,\n",
    "    0.28,\n",
    "    0.29,\n",
    "    0.12,\n",
    "    0.41,\n",
    "    0.20,\n",
    "    0.38,\n",
    "    0.38,\n",
    "    0.16,\n",
    "    0.24,\n",
    "    0.06,\n",
    "    0.48,\n",
    "    0.15,\n",
    "    0.38,\n",
    "    0.08,\n",
    "    0.20,\n",
    "    0.38,\n",
    "    0.12,\n",
    "    0.12,\n",
    "    0.36,\n",
    "    0.18,\n",
    "    0.26,\n",
    "    0.18,\n",
    "    0.46,\n",
    "    0.26,\n",
    "    0.17,\n",
    "    0.07,\n",
    "    0.11,\n",
    "    0.32,\n",
    "]\n",
    "\n",
    "print(f\"Top9: {np.mean(r_t9):.2f} +- {np.std(r_t9):.2f}\")\n",
    "print(f\"All: {np.mean(r_all):.2f} +- {np.std(r_all):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal SPEI importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directory\n",
    "dir_treelevel = f\"{dir_patterns}/tree-level\"\n",
    "os.makedirs(dir_treelevel, exist_ok=True)\n",
    "\n",
    "# Get NFI Data\n",
    "nfi_raw = get_final_nfi_data_for_analysis()\n",
    "nfi_sub = nfi_raw[\n",
    "    [\"tree_id\", \"lat_fr\", \"lon_fr\", \"tree_state_change\", \"species_lat2\", \"campagne_2\"]\n",
    "]\n",
    "\n",
    "# Get species data\n",
    "species_in_final_anlysis = get_species_with_models(\"list\")\n",
    "species_in_final_anlysis = list(sorted(species_in_final_anlysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_with_models = get_species_with_models(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "subset_of_runs = \"all\"\n",
    "\n",
    "for ipattern in [\"drier\", \"wetter\"]:\n",
    "\n",
    "    # Start figure\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Get color palette for top9\n",
    "    cp_top9 = sns.color_palette(\"tab10\", n_colors=9)\n",
    "    # Make to dictionary\n",
    "    cp_top9 = dict(zip(top9, cp_top9))\n",
    "\n",
    "    # Loop over all anomalies\n",
    "    anomaly_order = (\n",
    "        [\"_mean\", \"_min\", \"_max\"] if ipattern == \"drier\" else [\"_min\", \"_mean\", \"_max\"]\n",
    "    )\n",
    "    for i, add_anomaly_type_filter in enumerate(anomaly_order):\n",
    "\n",
    "        df_list = []\n",
    "\n",
    "        # Get data per species\n",
    "        # for ispecies in species_with_models:\n",
    "        for ispecies in top9:\n",
    "\n",
    "            idf = get_shap_per_tree(\n",
    "                ispecies=ispecies,\n",
    "                ipattern=f\"warmer_{ipattern}\",\n",
    "                ipattern_shap=ipattern,\n",
    "                subset_of_runs=subset_of_runs,\n",
    "                df_single_responses=df_responses.copy(),\n",
    "                df_both_patterns=df_both_patterns.copy(),\n",
    "                df_temp_patterns=df_temp_patterns.copy(),\n",
    "                df_spei_patterns=df_spei_patterns.copy(),\n",
    "                nfi_sub=nfi_sub.copy(),\n",
    "                add_anomaly_type_filter=add_anomaly_type_filter,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            if idf.__len__() == 0:\n",
    "                continue\n",
    "\n",
    "            df_list.append(idf)\n",
    "\n",
    "        # Add plot layout in any case\n",
    "        # Get ylab based on add_anomaly_type_filter\n",
    "        # ! IMPORTANT: PERCENTAGE NUMBERS FROM PIE CHART PLOT, COPIED BY HAND!\n",
    "        if ipattern == \"drier\":\n",
    "            if add_anomaly_type_filter == \"_min\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\ndecreasing SPEI minimum anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Minimum SPEI (more intense droughts, 46% of models)\"\n",
    "                subtitle = \"Minimum SPEI (drier seasonal lows, 46% of models)\"\n",
    "            elif add_anomaly_type_filter == \"_mean\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\ndecreasing SPEI mean anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Mean SPEI (progressively drier conditions, 32% of models)\"\n",
    "                subtitle = \"Mean SPEI (drier seasonal average, 32% of models)\"\n",
    "            elif add_anomaly_type_filter == \"_max\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\ndecreasing SPEI maximum anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Maximum SPEI (less intense wet events, 22% of models)\"\n",
    "                subtitle = \"Maximum SPEI (drier seasonal peaks, 22% of models)\"\n",
    "\n",
    "        elif ipattern == \"wetter\":\n",
    "            if add_anomaly_type_filter == \"_min\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\nincreasing SPEI minimum anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Minimum SPEI (less intense droughts, 98% of models)\"\n",
    "                subtitle = \"Minimum SPEI (wetter seasonal lows, 98% of models)\"\n",
    "            elif add_anomaly_type_filter == \"_mean\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\nincreasing SPEI mean anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Mean SPEI (progressively wetter conditions 0% of models)\"\n",
    "                subtitle = \"Mean SPEI (wetter seasonal average 0% of models)\"\n",
    "            elif add_anomaly_type_filter == \"_max\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\nincreasing SPEI maximum anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Maximum SPEI (more intense wet events 2% of models)\"\n",
    "                subtitle = \"Maximum SPEI (wetter seasonal peaks 2% of models)\"\n",
    "\n",
    "        axs[i].set_xlabel(\"Year of second visit\")\n",
    "        axs[i].set_ylabel(ylab)\n",
    "        axs[i].set_title(subtitle, fontsize=10)\n",
    "        axs[i].set_ylim(0.3, 0.85)\n",
    "\n",
    "        # If no data was extracted, plot empty\n",
    "        if df_list.__len__() == 0:\n",
    "            # axs[i].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # Concat data\n",
    "        df_list = pd.concat(df_list)\n",
    "\n",
    "        df_top9 = df_list.query(\"species_lat2 in @top9\")\n",
    "        df_rest = df_list.query(\"species_lat2 not in @top9\")\n",
    "\n",
    "        # Reset species categories to what is present\n",
    "        df_list[\"species_lat2\"] = pd.Categorical(\n",
    "            df_list[\"species_lat2\"], categories=top9, ordered=True\n",
    "        )\n",
    "\n",
    "        # ! Plot\n",
    "        sns.lineplot(\n",
    "            data=df_list,\n",
    "            x=\"campagne_2\",\n",
    "            y=ipattern,\n",
    "            hue=\"species_lat2\",\n",
    "            ax=axs[i],\n",
    "            # color=cp_top9[ispecies],\n",
    "            # alpha=n_runs / 50,\n",
    "        )\n",
    "\n",
    "        # Remove legend\n",
    "        axs[i].get_legend().remove()\n",
    "\n",
    "    # Make custom legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=cp_top9[ispecies], lw=2, label=ispecies)\n",
    "        for ispecies in top9\n",
    "    ]\n",
    "\n",
    "    # Add legend\n",
    "    axs[2].legend(\n",
    "        handles=legend_elements,\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0.54),\n",
    "        title=\"Species\",\n",
    "    )\n",
    "\n",
    "    # Add main title\n",
    "    fig.suptitle(\n",
    "        f\"SPEI feature importance over time of warmer, {ipattern} models\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f\"{dir_treelevel}/temporal-spei-warmer_{ipattern}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "subset_of_runs = \"all\"\n",
    "\n",
    "for ipattern in [\"warmer\"]:\n",
    "\n",
    "    # Start figure\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Get color palette for top9\n",
    "    cp_top9 = sns.color_palette(\"tab10\", n_colors=9)\n",
    "    # Make to dictionary\n",
    "    cp_top9 = dict(zip(top9, cp_top9))\n",
    "\n",
    "    # Loop over all anomalies\n",
    "    anomaly_order = (\n",
    "        [\"_mean\", \"_min\", \"_max\"] if ipattern == \"drier\" else [\"_min\", \"_mean\", \"_max\"]\n",
    "    )\n",
    "    for i, add_anomaly_type_filter in enumerate(anomaly_order):\n",
    "\n",
    "        # Get ylab based on add_anomaly_type_filter\n",
    "        if ipattern == \"warmer\":\n",
    "            if add_anomaly_type_filter == \"_min\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\nincreasing temperature minimum anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Minimum Temp. anomaly \\n(less intense cold events 43% of models)\"\n",
    "                subtitle = (\n",
    "                    \"Minimum Temp. anomaly \\n(warmer seasonal low, 43% of models)\"\n",
    "                )\n",
    "            elif add_anomaly_type_filter == \"_mean\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\nincreasing temperature mean anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Mean of Temp. anomalies \\n(on average warmer conditions 57% of models)\"\n",
    "                subtitle = (\n",
    "                    \"Mean of Temp. anomalies \\n(warmer seasonal average, 57% of models)\"\n",
    "                )\n",
    "            elif add_anomaly_type_filter == \"_max\":\n",
    "                # ylab = \"Probability of tree mortality wrt. \\nincreasing temperature maximum anomaly (scaled SHAP values)\"\n",
    "                ylab = \"Importance (normalized SHAP values)\"\n",
    "                # subtitle = \"Maximum Temp. anomaly \\n(more intense hot event, 0% of models)\"\n",
    "                subtitle = (\n",
    "                    \"Maximum Temp. anomaly \\n(warmer seasonal peak, 0% of models)\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Pattern '{ipattern}' not found!\")\n",
    "\n",
    "        # Get data per species\n",
    "        df_list = []\n",
    "        # for ispecies in species_with_models:\n",
    "        for ispecies in top9:\n",
    "\n",
    "            idf = get_shap_per_tree(\n",
    "                ispecies=ispecies,\n",
    "                ipattern=f\"warmer\",\n",
    "                ipattern_shap=ipattern,\n",
    "                subset_of_runs=subset_of_runs,\n",
    "                df_single_responses=df_responses.copy(),\n",
    "                df_both_patterns=df_both_patterns.copy(),\n",
    "                df_temp_patterns=df_temp_patterns.copy(),\n",
    "                df_spei_patterns=df_spei_patterns.copy(),\n",
    "                nfi_sub=nfi_sub.copy(),\n",
    "                add_anomaly_type_filter=add_anomaly_type_filter,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            if idf.__len__() == 0:\n",
    "                continue\n",
    "\n",
    "            df_list.append(idf)\n",
    "\n",
    "        # Add plot layout in any case\n",
    "        axs[i].set_xlabel(\"Year of second visit\")\n",
    "        axs[i].set_ylabel(ylab)\n",
    "        axs[i].set_title(subtitle, fontsize=10)\n",
    "        axs[i].set_ylim(0.3, 0.85)\n",
    "\n",
    "        # If no data was extracted, skip\n",
    "        if df_list.__len__() == 0:\n",
    "            # axs[i].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # Concat data\n",
    "        df_list = pd.concat(df_list)\n",
    "\n",
    "        df_top9 = df_list.query(\"species_lat2 in @top9\")\n",
    "        df_rest = df_list.query(\"species_lat2 not in @top9\")\n",
    "\n",
    "        # Reset species categories to what is present\n",
    "        df_list[\"species_lat2\"] = pd.Categorical(\n",
    "            df_list[\"species_lat2\"], categories=top9, ordered=True\n",
    "        )\n",
    "\n",
    "        # ! Plot\n",
    "        sns.lineplot(\n",
    "            data=df_list,\n",
    "            x=\"campagne_2\",\n",
    "            y=ipattern,\n",
    "            hue=\"species_lat2\",\n",
    "            ax=axs[i],\n",
    "            # color=cp_top9[ispecies],\n",
    "            # alpha=n_runs / 50,\n",
    "        )\n",
    "\n",
    "        # Remove legend\n",
    "        axs[i].get_legend().remove()\n",
    "\n",
    "    # Make custom legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=cp_top9[ispecies], lw=2, label=ispecies)\n",
    "        for ispecies in top9\n",
    "    ]\n",
    "\n",
    "    # Add legend\n",
    "    axs[2].legend(\n",
    "        handles=legend_elements,\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0.54),\n",
    "        title=\"Species\",\n",
    "    )\n",
    "\n",
    "    # Add main title\n",
    "    fig.suptitle(\n",
    "        f\"Temperature anomaly feature importance over time of warmer models\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f\"{dir_treelevel}/temporal-temp-all.png\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate Sensitivity - Tree Height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all species\n",
    "\n",
    "myspatterns = [\"wetter\", \"warmer_wetter\", \"drier\", \"warmer_drier\"]\n",
    "\n",
    "for mypattern in myspatterns:\n",
    "\n",
    "    # Report on loop\n",
    "    print(f\"Working on pattern: {mypattern}\")\n",
    "\n",
    "    # Pick main mattern (wetter or drier models in general, subsetted to warmer_xxx)\n",
    "    main_pattern = \"wetter\" if \"wetter\" in mypattern else \"drier\"\n",
    "\n",
    "    # For drier models, only look at extreme drought events\n",
    "    # add_anomaly_type_filter = \"_min\" if main_pattern == \"drier\" else None\n",
    "    add_anomaly_type_filter = None\n",
    "    add_season_filter = None  # \"spr\"\n",
    "\n",
    "    df_results = []\n",
    "\n",
    "    for ispecies in species_with_models:\n",
    "        # Load df_shap\n",
    "        df_shap = get_shap_per_tree(\n",
    "            ispecies=ispecies,\n",
    "            ipattern=f\"{mypattern}\",\n",
    "            ipattern_shap=f\"{main_pattern}\",\n",
    "            subset_of_runs=\"all\",\n",
    "            df_single_responses=df_responses.copy(),\n",
    "            df_both_patterns=df_both_patterns.copy(),\n",
    "            df_temp_patterns=df_temp_patterns.copy(),\n",
    "            df_spei_patterns=df_spei_patterns.copy(),\n",
    "            add_anomaly_type_filter=add_anomaly_type_filter,\n",
    "            # add_season_filter=\"spr\",\n",
    "            nfi_sub=nfi_sub.copy(),\n",
    "            verbose=False,\n",
    "        )\n",
    "        if df_shap.__len__() == 0:\n",
    "            print(f\" - No data for {ispecies}, skipping\")\n",
    "            continue\n",
    "\n",
    "        nfi_shap = (\n",
    "            nfi_raw.copy()\n",
    "            .query(\n",
    "                \"tree_id in @df_shap.tree_id.unique() and tree_state_change == 'alive_dead'\"\n",
    "            )[[\"tree_id\", \"htot_final\"]]\n",
    "            .dropna()\n",
    "        )\n",
    "        df_shap = df_shap.merge(nfi_shap, on=\"tree_id\", how=\"left\").dropna(\n",
    "            subset=[\"htot_final\"]\n",
    "        )\n",
    "\n",
    "        # Calculate linear regression\n",
    "        model = LinearRegression()\n",
    "        X = df_shap[[\"htot_final\"]].values.reshape(-1, 1)\n",
    "        y = df_shap[f\"{main_pattern}\"].values\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        r_squared = model.score(X, y)\n",
    "        residuals = y - y_pred\n",
    "        sse = np.sum(residuals**2)\n",
    "        mse = sse / (len(y) - 2)\n",
    "        se_slope = np.sqrt(mse / np.sum((X - np.mean(X)) ** 2))\n",
    "        t_stat = model.coef_[0] / se_slope\n",
    "        p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=len(y) - 2))\n",
    "        coef = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        equation = (\n",
    "            f\"{intercept:.3f} + {coef:.3f}x | R² = {r_squared:.3f} | p = {p_value:.3f}\"\n",
    "        )\n",
    "\n",
    "        myicon = (\n",
    "            \"🟢\"\n",
    "            if p_value < 0.05 and coef > 0\n",
    "            else \"🔴\" if p_value < 0.05 and coef < 0 else \"⚪\"\n",
    "        )\n",
    "\n",
    "        # print(f\" {myicon} {ispecies:>20}: {equation}\")\n",
    "\n",
    "        idf = pd.DataFrame(\n",
    "            {\n",
    "                \"species\": ispecies,\n",
    "                \"coef\": coef.round(4),\n",
    "                \"intercept\": intercept.round(3),\n",
    "                \"r_squared\": r_squared.round(3),\n",
    "                \"p_value\": p_value.round(3),\n",
    "                \"icon\": myicon,\n",
    "                # \"equation\": equation,\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "        df_results.append(idf)\n",
    "        # print(f\"Species: {ispecies} | {equation}\")\n",
    "\n",
    "    df_results = pd.concat(df_results, ignore_index=True)\n",
    "    df_results = df_results.sort_values(\"species\")\n",
    "    add_anomaly_type_filter = (\n",
    "        \"_all\" if add_anomaly_type_filter == None else add_anomaly_type_filter\n",
    "    )\n",
    "\n",
    "    df_results.to_csv(\n",
    "        f\"{dir_treelevel}/lm-height_vs_{mypattern}-spei_filter{add_anomaly_type_filter}-season_temp_filter_{add_season_filter}.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the number of positive, negative and non-significant trends\n",
    "for mypattern in myspatterns:\n",
    "\n",
    "    add_anomaly_type_filter = None\n",
    "\n",
    "    df_lm = pd.read_csv(\n",
    "        f\"{dir_treelevel}/lm-height_vs_{mypattern}-spei_filter{add_anomaly_type_filter if add_anomaly_type_filter is not None else '_all'}-season_temp_filter_{add_season_filter}.csv\"\n",
    "    )\n",
    "\n",
    "    n_positive = df_lm.query(\"p_value < 0.05 and coef > 0\").shape[0]\n",
    "    n_negative = df_lm.query(\"p_value < 0.05 and coef < 0\").shape[0]\n",
    "    n_nonsig = df_lm.query(\"p_value >= 0.05\").shape[0]\n",
    "\n",
    "    species_positive = df_lm.query(\"p_value < 0.05 and coef > 0\").species.tolist()\n",
    "    species_negative = df_lm.query(\"p_value < 0.05 and coef < 0\").species.tolist()\n",
    "    species_nonsig = df_lm.query(\"p_value >= 0.05\").species.tolist()\n",
    "\n",
    "    print(\n",
    "        f\"\\nFor pattern '{mypattern}' with anomaly filter '{add_anomaly_type_filter if add_anomaly_type_filter is not None else '_all'}': \\n - {n_positive} positive: {species_positive} \\n - {n_negative} negative: {species_negative} \\n - {n_nonsig} non-significant trends: (N species: {df_lm.shape[0]})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# User input\n",
    "fig_imp = f\"{dir_patterns}/fig_dataset_pattern/top9.png\"\n",
    "fig_temp = f\"{dir_patterns}/pie_charts/pie-temp-double_change-top9_True-scaled_by_100perc_True-scale_by_nmodels_False-add_boxes_False.png\"\n",
    "fig_spei = f\"{dir_patterns}/pie_charts/pie-spei-double_change-top9_True-scaled_by_100perc_True-scale_by_nmodels_False-add_boxes_False.png\"\n",
    "fig_tree = (\n",
    "    f\"{dir_patterns}/tree-level/top9-corr_r-wetter_vs_drier-subset_of_runs_all.png\"\n",
    ")\n",
    "\n",
    "image_files = [\n",
    "    fig_imp,\n",
    "    fig_temp,\n",
    "    fig_spei,\n",
    "    fig_tree,\n",
    "]\n",
    "\n",
    "output_file = f\"{dir_patterns}/si_summary.png\"\n",
    "\n",
    "# Manually set label positions (in figure coordinates)\n",
    "label_y_coords = [0.983, 0.8, 0.55, 0.225]  # Top-to-bottom\n",
    "label_x_coords = [0.125, 0.125, 0.125, 0.125]  # Left-to-right\n",
    "label_letters = [\"A\", \"C\", \"D\", \"E\"]\n",
    "\n",
    "# A4 portrait size in inches (scaled down)\n",
    "a4_width, a4_height = 8.27 * 0.8, 11.7 * 0.8\n",
    "\n",
    "# Custom row height ratios\n",
    "height_ratios = [0.75, 1.0, 1.3, 1.0]\n",
    "\n",
    "fig = plt.figure(figsize=(a4_width, a4_height))\n",
    "gs = GridSpec(nrows=4, ncols=1, height_ratios=height_ratios, figure=fig)\n",
    "axs = [fig.add_subplot(gs[i, 0]) for i in range(4)]\n",
    "\n",
    "# Plot images\n",
    "for ax, img_path in zip(axs, image_files):\n",
    "    img = mpimg.imread(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Add panel labels using user-defined (x, y) coordinates\n",
    "for label, x, y in zip(label_letters, label_x_coords, label_y_coords):\n",
    "    fig.text(\n",
    "        x,\n",
    "        y,\n",
    "        label,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"none\", pad=2),\n",
    "    )\n",
    "\n",
    "# Optional: white-out dummy labels on row 0\n",
    "axs[0].text(\n",
    "    0.52,\n",
    "    0.98,\n",
    "    \"B\",\n",
    "    transform=axs[0].transAxes,\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    bbox=dict(facecolor=\"white\", edgecolor=\"none\", pad=0.5),\n",
    ")\n",
    "axs[0].text(\n",
    "    0.01,\n",
    "    0.95,\n",
    "    \"T\",\n",
    "    color=\"white\",\n",
    "    transform=axs[0].transAxes,\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    bbox=dict(facecolor=\"white\", edgecolor=\"none\", pad=0.5),\n",
    ")\n",
    "\n",
    "# Save or show\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_file, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Figure saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv tables with different sortings of all species\n",
    "list_of_sorts = {\n",
    "    \"1\": ([\"species\", \"nruns_rel\"], [True, False]),\n",
    "    \"2\": ([\"n_trees_total\", \"nruns_rel\"], [False, False]),\n",
    "    \"3\": ([\"change\", \"species\"], [False, False]),\n",
    "    \"4\": ([\"change\", \"nruns_rel\"], [False, False]),\n",
    "    \"5\": ([\"change\", \"n_trees_total\"], [False, False]),\n",
    "    \"6\": ([\"species\", \"change\"], [True, False]),\n",
    "}\n",
    "\n",
    "for subset in [\"top-all\", \"top-5\", \"top-10\", \"top-20\"]:\n",
    "\n",
    "    # os.makedirs(f\"{dir_patterns}/lookup-{subset}\", exist_ok=True)\n",
    "    df_subset = df_occ_red.copy()\n",
    "\n",
    "    if subset == \"top-all\":\n",
    "        pass\n",
    "    else:\n",
    "        tmp_n = subset.replace(\"top\", \"\")\n",
    "        tmp_n = int(tmp_n)\n",
    "        tmp_species = top_species_all.head(tmp_n).species.tolist()\n",
    "        df_subset = df_subset.query(\"species in @tmp_species\")\n",
    "\n",
    "    for key, (sort_list, tf_list) in list_of_sorts.items():\n",
    "        suffix = str.join(\"\", [f\"_{c}\" for c in sort_list])\n",
    "        df_file = (\n",
    "            move_vars_to_front(df_subset, sort_list)\n",
    "            .sort_values(sort_list, ascending=tf_list)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        # df_file.to_csv(\n",
    "        #     f\"{dir_patterns}/lookup-{subset}/sorted_by{suffix}.csv\", index=False\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted by Species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_occ_red.sort_values(\n",
    "    [\"species\", \"nruns_rel\"], ascending=[True, False]\n",
    ").reset_index(drop=True)[\n",
    "    [\n",
    "        \"species\",\n",
    "        \"change\",\n",
    "        \"nruns_val_rel\",\n",
    "        \"nruns_val\",\n",
    "        # \"feature_spei\",\n",
    "        \"spei_simple\",\n",
    "        # \"spei_simpler\",\n",
    "        \"temp_season\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Subset\n",
    "df_tmp.head(50)\n",
    "\n",
    "# Top 20\n",
    "df_tmp.query(\"species in @top9\").reset_index(drop=True).to_clipboard(index=False)\n",
    "df_tmp.query(\"species in @top9\").reset_index(drop=True).sort_values(\n",
    "    [\"change\", \"species\"], ascending=[True, True]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! Settings\n",
    "species_subset = (\n",
    "    top9\n",
    "    + [\"Quercus ilex\"]\n",
    "    + species_family.query(\"family_lat == 'Pinaceae'\")[\"species_lat2\"].tolist()\n",
    ")\n",
    "\n",
    "# Get temporary df\n",
    "df_occ_tmp = df_occ_red.copy()\n",
    "\n",
    "# Subset of columns\n",
    "cols = [\n",
    "    \"species\",\n",
    "    \"change_perc\",\n",
    "    # \"change\",\n",
    "    # ! Temp\n",
    "    # \"temp_metric\",\n",
    "    # \"temp_anom\",\n",
    "    # \"temp_season\",\n",
    "    \"feature_temp\",\n",
    "    # ! SPEI\n",
    "    # \"best_feature_spei\",\n",
    "    # \"spei_duration_simple\",\n",
    "    # \"spei_anom\",\n",
    "    # \"spei_season\",\n",
    "    # \"best_spei_simpler\",\n",
    "    # \"spei_simpler\",\n",
    "    # \"best_feature_spei\",\n",
    "    \"feature_spei\",\n",
    "]\n",
    "\n",
    "# Adjust sorting\n",
    "# df_occ_tmp = df_occ_tmp.sort_values(\"n_trees_total\", ascending=False)\n",
    "# df_occ_tmp = df_occ_tmp.sort_values(\"n_changes\", ascending=True)\n",
    "# df_occ_tmp = df_occ_tmp.sort_values(\"family_lat\", ascending=True)\n",
    "# df_occ_tmp = df_occ_tmp.sort_values(\"genus_lat\", ascending=True)\n",
    "# df_occ_tmp = df_occ_tmp.sort_values(\"nruns_rel\", ascending=False)\n",
    "df_occ_tmp = df_occ_tmp.sort_values(\"species\", ascending=True)\n",
    "\n",
    "# Add filter\n",
    "species_subset_1 = (\n",
    "    df_nchanges_per_species_2patterns.query(\"n_changes <= 2\")[\"species\"]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Fix sort order for climatic conditions\n",
    "tmp_order = [\n",
    "    \"warmer_drier\",\n",
    "    \"warmer_wetter\",\n",
    "    \"cooler_drier\",\n",
    "    \"cooler_wetter\",\n",
    "    \"other\",\n",
    "]\n",
    "\n",
    "df_occ_tmp = df_occ_tmp.assign(\n",
    "    change=pd.Categorical(df_occ_tmp[\"change\"], categories=tmp_order, ordered=True)\n",
    ").sort_values([\"species\", \"change\"], ascending=[True, True])\n",
    "\n",
    "\n",
    "# Show\n",
    "for ichange in tmp_order:\n",
    "    # display(ichange)\n",
    "    # display(df_occ_tmp[cols].query(\"change == @ichange and species in @species_subset\"))\n",
    "\n",
    "    print(\"\")  # Just to be able to outcomment all other stuff\n",
    "    # display(df_occ_tmp.query(\"change == @ichange and species in @species_subset_1\"))\n",
    "    # display(df_occ_tmp.query(\"change == @ichange\"))\n",
    "\n",
    "df_occ_tmp[cols].to_clipboard(index=False)\n",
    "df_occ_tmp[cols].to_csv(f\"{dir_tables}/climate_features_all_species.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Species Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if which_analysis != \"default\":\n",
    "    chime.error()\n",
    "    raise ValueError(\n",
    "        f\"🟥 Analysis '{which_analysis}' not implemented for remainder of this notebook, because not all species were run for sensitivity analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortality Trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mort_trend_data():\n",
    "    # path = \"../01_process_nfi_data/specific_runs/run_to_calculate_mortality_trends_in_analysis - 2024-11-25/data_of_change/direct_bs/species_lat/centered_False-normalized_False/subset-top-None_bs-100.csv\"\n",
    "\n",
    "    # Get df extra created for this analysis\n",
    "    df_mort_trends_raw = pd.read_feather(\n",
    "        \"../../data/final/mortality_trends/df_species52_temporal_data.feather\"\n",
    "    )\n",
    "\n",
    "    # Clean df\n",
    "    df_mort_trends_raw[\"species\"] = (\n",
    "        df_mort_trends_raw[\"group_year\"].str.split(\"_\").str[0]\n",
    "    )\n",
    "    df_mort_trends_raw[\"year\"] = (\n",
    "        df_mort_trends_raw[\"group_year\"].str.split(\"_\").str[1].astype(int)\n",
    "    )\n",
    "    df_mort_trends_raw = df_mort_trends_raw.drop(\n",
    "        [\"group_year\", \"region\"], axis=1, errors=\"ignore\"\n",
    "    )\n",
    "    df_mort_trends_raw = df_mort_trends_raw.rename({\"mean\": \"mort\"}, axis=1)\n",
    "\n",
    "    # Remove NA in mortality\n",
    "    # df_mort_trends_raw_na = df_mort_trends_raw.query(\"mort.isna()\").copy()\n",
    "    # df_mort_trends_raw = df_mort_trends_raw.dropna(subset=[\"mort\"])\n",
    "    # print(\n",
    "    # f\"Removed {df_mort_trends_raw_na.species.nunique()} species with NA in mortality.\"\n",
    "    # )\n",
    "\n",
    "    # Reorder columns\n",
    "    df_mort_trends_raw = move_vars_to_front(\n",
    "        df_mort_trends_raw, [\"species\", \"year\", \"mort\"]\n",
    "    )\n",
    "\n",
    "    return df_mort_trends_raw\n",
    "\n",
    "\n",
    "def get_mort_trend(df_in):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # For every species group, calculate the trend as the slope of the linear regression\n",
    "    # Initiate list\n",
    "    df_mort_trends = []\n",
    "    for ispecies in df_in.species.unique():\n",
    "        # Get subset\n",
    "        df_i = df_in.query(\"species == @ispecies\").copy().dropna()\n",
    "        # Get linear model\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(df_i[[\"year\"]], df_i[\"mort\"])\n",
    "        # Get slope\n",
    "        slope = lm.coef_[0]\n",
    "        # Get intercept\n",
    "        intercept = lm.intercept_\n",
    "        # Get r2\n",
    "        r2 = lm.score(df_i[[\"year\"]], df_i[\"mort\"])\n",
    "        # Get p-value\n",
    "        p_value = stats.linregress(df_i[\"year\"], df_i[\"mort\"]).pvalue\n",
    "        # Make plot\n",
    "        # fig, ax = plt.subplots()\n",
    "        # sns.regplot(data=df_i, x=\"year\", y=\"mort\", ax=ax)\n",
    "        # ax.set_title(f\"{ispecies} - Slope: {slope:.2f} - R2: {r2:.2f}\")\n",
    "        # plt.show()\n",
    "\n",
    "        # Get MK test\n",
    "        mktest = get_mk_test_raw(df_i[\"year\"], df_i[\"mort\"])\n",
    "\n",
    "        # Append to list\n",
    "        df_mort_trends.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"species\": ispecies,\n",
    "                    \"slope\": slope,\n",
    "                    \"intercept\": intercept,\n",
    "                    \"r2\": r2,\n",
    "                    \"pvalue\": p_value,\n",
    "                    \"mk_pvalue\": mktest[\"p\"],\n",
    "                    \"mk_trend\": mktest[\"trend\"],\n",
    "                },\n",
    "                index=[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    df_mort_trends = pd.concat(df_mort_trends, axis=0).reset_index(drop=True)\n",
    "    return df_mort_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute rates\n",
    "df_mort_abs = get_mort_trend_data()\n",
    "\n",
    "# Rates normalized to first year\n",
    "df_mort_rel = df_mort_abs.copy()\n",
    "\n",
    "for species in df_mort_rel.species.unique():\n",
    "    # Get species\n",
    "    df_species = df_mort_rel.query(\"species == @species\").copy()\n",
    "    # Remove 0 mortality (no mortality could be calculated for that year)\n",
    "    df_species = df_species.query(\"mort > 0\").copy()\n",
    "    # Normalize to first year\n",
    "    df_species[\"mort\"] = df_species[\"mort\"] / df_species[\"mort\"].iloc[0]\n",
    "    # Replace in df\n",
    "    df_mort_rel.loc[df_mort_rel.species == species, \"mort\"] = df_species[\"mort\"]\n",
    "\n",
    "df_mort_rel_trends = get_mort_trend(df_mort_rel)\n",
    "df_mort_abs_trends = get_mort_trend(df_mort_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !Table for supplementary\n",
    "df_mort_supp = df_mort_abs_trends.copy()[[\"species\"]]\n",
    "\n",
    "# Add absolute rates\n",
    "df_mort_supp[\"Absolute increase [%/yr^2]\"] = df_mort_abs_trends[\"slope\"].round(2)\n",
    "df_mort_supp[\"Relative increase [%/yr]\"] = (df_mort_rel_trends[\"slope\"] * 100).round(2)\n",
    "\n",
    "# Attache change in rate over the years\n",
    "dict_rates = {}\n",
    "last_year = df_mort_rel.year.max()\n",
    "for ispecies in df_mort_supp.species.unique():\n",
    "    irate = (\n",
    "        df_mort_rel.query(\"species == @ispecies and year == @last_year\")\n",
    "        .copy()[\"mort\"]\n",
    "        .values[0]\n",
    "    )\n",
    "\n",
    "    dict_rates[ispecies] = irate\n",
    "\n",
    "df_mort_supp[\"x-fold increase\"] = df_mort_supp[\"species\"].map(dict_rates).round(2)\n",
    "\n",
    "# Attach significance\n",
    "df_mort_supp[\"pval\"] = df_mort_rel_trends[\"pvalue\"].round(3)\n",
    "\n",
    "# Attach species info\n",
    "df_mort_supp = df_mort_supp.merge(\n",
    "    df_occ_red[\n",
    "        [\"species\", \"n_trees_total\", \"family_lat\", \"genus_lat\"]\n",
    "    ].drop_duplicates(),\n",
    "    on=\"species\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Add short names\n",
    "# Shorten species name and set as index\n",
    "df_mort_supp_clean = df_mort_supp.copy()\n",
    "df_mort_supp_clean[\"species\"] = df_mort_supp_clean[\"species\"].replace(\n",
    "    {\"Populus\": \"Populus spp\"}\n",
    ")\n",
    "df_mort_supp_clean[\"species\"] = (\n",
    "    df_mort_supp_clean[\"species\"]\n",
    "    + \" (\"\n",
    "    + df_mort_supp_clean[\"species\"].str.split(\" \").str[0].str[:2].str.title()\n",
    "    + df_mort_supp_clean[\"species\"].str.split(\" \").str[1].str[:2].str.title()\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "\n",
    "df_mort_supp_clean = move_vars_to_front(\n",
    "    df_mort_supp_clean, [\"species\", \"family_lat\", \"genus_lat\", \"n_trees_total\"]\n",
    ")\n",
    "\n",
    "df_mort_supp_clean.rename(\n",
    "    {\n",
    "        \"species\": \"Species (abbreviation)\",\n",
    "        \"family_lat\": \"Family\",\n",
    "        \"genus_lat\": \"Genus\",\n",
    "        \"n_trees_total\": \"N Trees\",\n",
    "    },\n",
    "    axis=1,\n",
    ").to_csv(f\"{dir_tables}/mortality_trends.csv\", index=False)\n",
    "df_mort_supp_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_mort_rel_trends.head(3))\n",
    "trends_sign = df_mort_rel_trends.query(\"pvalue < 0.05\").species.nunique()\n",
    "trends_total = df_mort_rel_trends.species.nunique()\n",
    "print(\n",
    "    f\"Significant trends: {trends_sign} out of {trends_total} species = {trends_sign/trends_total*100:.0f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! Data for plotting\n",
    "# Attach n_trees\n",
    "df_mort_trends = df_mort_rel_trends.merge(\n",
    "    df_occ_red[\n",
    "        [\"species\", \"n_trees_total\", \"family_lat\", \"genus_lat\"]\n",
    "    ].drop_duplicates(),\n",
    "    on=\"species\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "df_mort_trends[\"n_trees_rel\"] = (\n",
    "    df_mort_trends[\"n_trees_total\"] / df_mort_trends[\"n_trees_total\"].sum() * 100\n",
    ")\n",
    "df_mort_trends[\"n_trees_rel\"] = df_mort_trends[\"n_trees_rel\"].round(2)\n",
    "\n",
    "# Attach whether significant\n",
    "df_mort_trends[\"significant\"] = df_mort_trends[\"pvalue\"] < 0.05\n",
    "\n",
    "# Attach columns from supplementary table\n",
    "df_mort_trends = df_mort_trends.merge(\n",
    "    df_mort_supp[[\"species\", \"Absolute increase [%/yr^2]\", \"Relative increase [%/yr]\"]],\n",
    "    on=\"species\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Display\n",
    "display(df_mort_trends.head(5))\n",
    "df_mort_trends.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Buxus sempervirens because of extreme mortality increase\n",
    "df_mort_trends.query(\"species == 'Buxus sempervirens'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize for lms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach tolerances to responses df\n",
    "df_tol = pd.read_csv(\n",
    "    \"../../data/final/tables/species_tolerance_and_habita_data.csv\"\n",
    ").rename(\n",
    "    columns={\n",
    "        \"species_lat2\": \"species\",\n",
    "        \"ell_soil reaction\": \"ell_soil_reaction\",\n",
    "        \"ell_soil moisture\": \"ell_soil_moisture\",\n",
    "    }\n",
    ")\n",
    "print(f\"Tolerance indeces: {df_tol.columns.tolist()}\")\n",
    "# Add temp-spei patterns\n",
    "df_lrs = (\n",
    "    df_plot_from_mulitplot.copy()\n",
    "    .drop(columns={\"runs\"})\n",
    "    .merge(df_tol, on=\"species\", how=\"left\")\n",
    ")\n",
    "# Add single patterns\n",
    "df_lrs = df_lrs.merge(species_change_single, on=\"species\", how=\"left\")\n",
    "\n",
    "# Add phylogeny\n",
    "df_lrs = df_lrs.merge(\n",
    "    df_occ[[\"species\", \"genus_lat\", \"family_lat\"]].drop_duplicates(),\n",
    "    on=\"species\",\n",
    "    how=\"left\",\n",
    ")\n",
    "# Add mortality data\n",
    "df_lrs = df_lrs.merge(\n",
    "    df_mort_trends,\n",
    "    on=[\"species\", \"family_lat\", \"genus_lat\", \"n_trees_total\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Add temp/spei ratio\n",
    "df_lrs[\"temp_spei_ratio\"] = df_lrs[\"mean_temp\"] / df_lrs[\"mean_spei\"]\n",
    "\n",
    "# Get dummy of mean spei duration and attach it\n",
    "dmy = (\n",
    "    df_both_patterns.copy()\n",
    "    .query(\"change == 'warmer_drier'\")\n",
    "    .groupby(\"species\")\n",
    "    .agg({\"spei_duration\": \"mean\"})\n",
    "    .rename(columns={\"spei_duration\": \"mean_spei_duration\"})\n",
    "    .reset_index()\n",
    ")\n",
    "df_lrs = df_lrs.merge(dmy, on=\"species\", how=\"left\")\n",
    "\n",
    "# Shorten species names\n",
    "df_lrs = df_lrs.replace({\"Populus\": \"Populus spp\"})\n",
    "df_lrs[\"species_long\"] = df_lrs[\"species\"].copy()\n",
    "df_lrs[\"species\"] = (\n",
    "    df_lrs[\"species\"].str.split(\" \").str[0].str[:2].str.title()\n",
    "    # + \"_\"\n",
    "    + df_lrs[\"species\"].str.split(\" \").str[1].str[:2].str.title()\n",
    "    # + \".\"\n",
    ")\n",
    "\n",
    "# Show\n",
    "print(f\"\\nColumns: {sorted(df_lrs.columns.tolist())}\")\n",
    "df_lrs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get color palettes\n",
    "color_order = [\n",
    "    # Purples\n",
    "    \"#5a4e60\",\n",
    "    \"#907e99\",\n",
    "    \"#cab2d7\",\n",
    "    # Blues\n",
    "    \"#485766\",\n",
    "    \"#758ba2\",\n",
    "    \"#a6c4e3\",\n",
    "    # Greens\n",
    "    \"#4e643b\",\n",
    "    \"#7e9f62\",\n",
    "    \"#b2df8b\",\n",
    "    # Yellows\n",
    "    \"#72542d\",\n",
    "    \"#b5884c\",\n",
    "    \"#fdbf6d\",\n",
    "    # Reds\n",
    "    \"#4f230c\",\n",
    "    \"#7f3d1a\",\n",
    "    \"#b35929\",\n",
    "    # Black\n",
    "    \"black\",\n",
    "]\n",
    "\n",
    "family_lat_order = [\n",
    "    \"Aquifoliaceae\",\n",
    "    \"Betulaceae\",\n",
    "    \"Buxaceae\",\n",
    "    \"Cupressaceae\",\n",
    "    \"Ericaceae\",\n",
    "    \"Fabaceae\",\n",
    "    \"Fagaceae\",\n",
    "    \"Malvaceae\",\n",
    "    \"Oleaceae\",\n",
    "    \"Pinaceae\",\n",
    "    \"Rhamnaceae\",\n",
    "    \"Rosaceae\",\n",
    "    \"Salicaceae\",\n",
    "    \"Sapindaceae\",\n",
    "    \"Ulmaceae\",\n",
    "    \"Viburnaceae\",\n",
    "]\n",
    "\n",
    "family_en_order = [\n",
    "    \"Beech\",\n",
    "    \"Birch\",\n",
    "    \"Boxwood\",\n",
    "    \"Buckthorn\",\n",
    "    \"Cypress\",\n",
    "    \"Elm\",\n",
    "    \"Heath\",\n",
    "    \"Holly\",\n",
    "    \"Legume\",\n",
    "    \"Mallow\",\n",
    "    \"Olive\",\n",
    "    \"Pine\",\n",
    "    \"Rose\",\n",
    "    \"Soapberry\",\n",
    "    \"Viburnum\",\n",
    "    \"Willow\",\n",
    "]\n",
    "\n",
    "family_both_order = [\n",
    "    \"Aquifoliaceae (Holly)\",\n",
    "    \"Betulaceae (Birch)\",\n",
    "    \"Buxaceae (Boxwood)\",\n",
    "    \"Cupressaceae (Cypress)\",\n",
    "    \"Ericaceae (Heath)\",\n",
    "    \"Fabaceae (Legume)\",\n",
    "    \"Fagaceae (Beech)\",\n",
    "    \"Malvaceae (Mallow)\",\n",
    "    \"Oleaceae (Olive)\",\n",
    "    \"Pinaceae (Pine)\",\n",
    "    \"Rhamnaceae (Buckthorn)\",\n",
    "    \"Rosaceae (Rose)\",\n",
    "    \"Salicaceae (Willow)\",\n",
    "    \"Sapindaceae (Soapberry)\",\n",
    "    \"Ulmaceae (Elm)\",\n",
    "    \"Viburnaceae (Viburnum)\",\n",
    "]\n",
    "\n",
    "dict_family_en = {\n",
    "    \"Aquifoliaceae\": \"Holly\",\n",
    "    \"Betulaceae\": \"Birch\",\n",
    "    \"Buxaceae\": \"Boxwood\",\n",
    "    \"Cupressaceae\": \"Cypress\",\n",
    "    \"Ericaceae\": \"Heath\",\n",
    "    \"Fabaceae\": \"Legume\",\n",
    "    \"Fagaceae\": \"Beech\",\n",
    "    \"Malvaceae\": \"Mallow\",\n",
    "    \"Oleaceae\": \"Olive\",\n",
    "    \"Pinaceae\": \"Pine\",\n",
    "    \"Rhamnaceae\": \"Buckthorn\",\n",
    "    \"Rosaceae\": \"Rose\",\n",
    "    \"Salicaceae\": \"Willow\",\n",
    "    \"Sapindaceae\": \"Soapberry\",\n",
    "    \"Ulmaceae\": \"Elm\",\n",
    "    \"Viburnaceae\": \"Viburnum\",\n",
    "}\n",
    "\n",
    "dict_family_both = {\n",
    "    \"Aquifoliaceae\": \"Aquifoliaceae (Holly)\",\n",
    "    \"Betulaceae\": \"Betulaceae (Birch)\",\n",
    "    \"Buxaceae\": \"Buxaceae (Boxwood)\",\n",
    "    \"Cupressaceae\": \"Cupressaceae (Cypress)\",\n",
    "    \"Ericaceae\": \"Ericaceae (Heath)\",\n",
    "    \"Fabaceae\": \"Fabaceae (Legume)\",\n",
    "    \"Fagaceae\": \"Fagaceae (Beech)\",\n",
    "    \"Malvaceae\": \"Malvaceae (Mallow)\",\n",
    "    \"Oleaceae\": \"Oleaceae (Olive)\",\n",
    "    \"Pinaceae\": \"Pinaceae (Pine)\",\n",
    "    \"Rhamnaceae\": \"Rhamnaceae (Buckthorn)\",\n",
    "    \"Rosaceae\": \"Rosaceae (Rose)\",\n",
    "    \"Salicaceae\": \"Salicaceae (Willow)\",\n",
    "    \"Sapindaceae\": \"Sapindaceae (Soapberry)\",\n",
    "    \"Ulmaceae\": \"Ulmaceae (Elm)\",\n",
    "    \"Viburnaceae\": \"Viburnaceae (Viburnum)\",\n",
    "}\n",
    "\n",
    "dict_top9_lat_mixed = {\n",
    "    \"Abies alba\": \"Abies alba (Silver Fir)\",\n",
    "    \"Carpinus betulus\": \"Carpinus betulus (European Hornbeam)\",\n",
    "    \"Castanea sativa\": \"Castanea sativa (Sweet Chestnut)\",\n",
    "    \"Fagus sylvatica\": \"Fagus sylvatica (Beech)\",\n",
    "    \"Picea abies\": \"Picea abies (Norway Spruce)\",\n",
    "    \"Pinus sylvestris\": \"Pinus sylvestris (Scots Pine)\",\n",
    "    \"Quercus petraea\": \"Quercus petraea (Sessile Oak)\",\n",
    "    \"Quercus pubescens\": \"Quercus pubescens (Downy Oak)\",\n",
    "    \"Quercus robur\": \"Quercus robur (Pedunculate Oak)\",\n",
    "    \"Quercus ilex\": \"Quercus ilex (Holm Oak)\",\n",
    "}\n",
    "\n",
    "# For all non-relevant species, use \"Other\"\n",
    "for ispecies in df_lrs.species_long.unique():\n",
    "    if ispecies not in dict_top9_lat_mixed.keys():\n",
    "        dict_top9_lat_mixed[ispecies] = \"Other\"\n",
    "\n",
    "\n",
    "dict_top9_mixed_color = {\n",
    "    # Pinetrees\n",
    "    \"Abies alba (Silver Fir)\": \"#fdbf6d\",\n",
    "    \"Picea abies (Norway Spruce)\": \"#b5884c\",\n",
    "    \"Pinus sylvestris (Scots Pine)\": \"#72542d\",\n",
    "    # Oaks\n",
    "    \"Quercus petraea (Sessile Oak)\": \"#45BF55\",\n",
    "    \"Quercus pubescens (Downy Oak)\": \"#168039\",\n",
    "    \"Quercus robur (Pedunculate Oak)\": \"#044D29\",\n",
    "    \"Quercus ilex (Holm Oak)\": \"#00261C\",\n",
    "    # Others\n",
    "    \"Fagus sylvatica (Beech)\": \"#cab2d7\",\n",
    "    \"Carpinus betulus (European Hornbeam)\": \"#7B52AB\",\n",
    "    \"Castanea sativa (Sweet Chestnut)\": \"#553285\",\n",
    "    \"Other\": \"darkgrey\",\n",
    "}\n",
    "\n",
    "dict_species_short_lat = dict(zip(df_lrs.species, df_lrs.species_long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patternshare_lm_ax(\n",
    "    ax,\n",
    "    tmp,\n",
    "    x,\n",
    "    y,\n",
    "    color_palette,\n",
    "    color_method,\n",
    "    dict_family,\n",
    "    long_labels=True,\n",
    "    bold_top_species=[],\n",
    "    dot_size=0,\n",
    "    add_labels=None,\n",
    "    pval_threshold=0.01,\n",
    "    add_title=True,\n",
    "    show_major_only=False,\n",
    "    font_base=10,\n",
    "    dirsave=None,\n",
    "):\n",
    "\n",
    "    # Input\n",
    "    # if add_labels and not show_major_only:\n",
    "    #     dot_size = 0\n",
    "\n",
    "    if add_labels not in [\"none\", \"top9\", \"top9+\", \"all\"]:\n",
    "        raise ValueError(\"Please select a valid option for add_labels.\")\n",
    "\n",
    "    if add_labels in [\"none\"]:\n",
    "        adjust_labels = False\n",
    "    else:\n",
    "        adjust_labels = True\n",
    "\n",
    "    # Create lists to hold the text objects for adjustment\n",
    "    species_texts = []\n",
    "\n",
    "    # Remove na values\n",
    "    tmp = tmp.dropna(subset=[x, y])\n",
    "\n",
    "    # Remove probelmatic species (BuSe has massive mortality increase, distorting the plot)\n",
    "    if x in [\"mort_increase_rel\", \"mort_increase_abs\", \"slope\"]:\n",
    "        tmp = tmp.query(\"species != 'BuSe'\").copy()\n",
    "\n",
    "    # Variable fixes\n",
    "    if x == \"count\":\n",
    "        tmp[x] = tmp[x] / 10000\n",
    "\n",
    "    # Get ylabel\n",
    "    if y == \"warmer_drier\":\n",
    "        ylab = \"warmer and drier\"\n",
    "    elif y == \"warmer_wetter\":\n",
    "        ylab = \"warmer and wetter\"\n",
    "    elif y == \"other\":\n",
    "        ylab = \"other\"\n",
    "    elif y == \"cooler_drier\":\n",
    "        ylab = \"cooler and drier\"\n",
    "    elif y == \"cooler_wetter\":\n",
    "        ylab = \"cooler and wetter\"\n",
    "    elif y == \"warmer\":\n",
    "        ylab = \"warmer\"\n",
    "    elif y == \"cooler\":\n",
    "        ylab = \"cooler\"\n",
    "    elif y == \"response_temp_unclear\":\n",
    "        ylab = \"unclear Temp. response\"\n",
    "    elif y == \"drier\":\n",
    "        ylab = \"drier\"\n",
    "    elif y == \"wetter\":\n",
    "        ylab = \"wetter\"\n",
    "    elif y == \"response_spei_unclear\":\n",
    "        ylab = \"unclear SPEI response\"\n",
    "    else:\n",
    "        ylab = y\n",
    "\n",
    "    # ylab = f\"Vulnerability to {ylab} (% valid models)\"\n",
    "    # ylab = f\"Vulnerability to {ylab}\\nconditions (% of models)\"\n",
    "    ylab = f\"% of models indicating {ylab}\\n conditions increase mortality risk\"\n",
    "    # ylab = f\"Increasing Mortality with {ylab} conditions (% of valid models)\"\n",
    "\n",
    "    # Get xlabel\n",
    "    if x == \"species\":\n",
    "        xlab = \"Species\"\n",
    "    elif x == \"tree_class\":\n",
    "        xlab = \"Tree Class\"\n",
    "    elif x == \"count\":\n",
    "        xlab = \"Number of Trees (x 10'000)\"\n",
    "    elif x == \"htot_mean\":\n",
    "        xlab = \"Mean Height (m)\"\n",
    "    elif x == \"succession\":\n",
    "        xlab = \"Succession Stage\"\n",
    "    elif x == \"ell_light\":\n",
    "        xlab = \"Ellenberg Light Indicator\"\n",
    "    elif x == \"ell_temperature\":\n",
    "        xlab = \"Ellenberg Temperature Indicator\"\n",
    "    elif x == \"ell_soil_reaction\":\n",
    "        xlab = \"Ellenberg Soil Reaction Indicator\"\n",
    "    elif x == \"ell_soil_moisture\":\n",
    "        xlab = \"Ellenberg Soil Moisture Indicator\"\n",
    "    elif x == \"ell_nitrogen\":\n",
    "        xlab = \"Ellenberg Soil Nitrogen Indicator\"\n",
    "    elif x == \"ell_salt\":\n",
    "        xlab = \"Ellenberg Salt Indicator\"\n",
    "    elif x == \"nii_shade\":\n",
    "        xlab = \"Shade Tolerance\"\n",
    "    elif x == \"nii_drought\":\n",
    "        xlab = \"Drought Tolerance\"\n",
    "    elif x == \"nii_waterlog\":\n",
    "        xlab = \"Waterlogging Tolerance\"\n",
    "    elif x == \"slope\":\n",
    "        xlab = \"Relative change in mortality rate (%)\"\n",
    "    else:\n",
    "        xlab = x\n",
    "\n",
    "    xlab = xlab.capitalize()\n",
    "    ylab = ylab.capitalize()\n",
    "\n",
    "    # ! Run lm\n",
    "    lm = smf.ols(f\"{y} ~ {x}\", data=tmp).fit()\n",
    "\n",
    "    # Get coefficients\n",
    "    r2 = lm.rsquared\n",
    "    # Ignore warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    slope = lm.params[1]\n",
    "    intercept = lm.params[0]\n",
    "    pval = lm.pvalues[1]  # .round(5)\n",
    "    # pval_sign = f\"= {pval:.2f}\" if pval > pval_threshold else f\"< {pval_threshold}\"\n",
    "    pval_sign = f\"= {pval:.2f}\"\n",
    "    slope_linetype = \"dashed\" if pval > pval_threshold else \"solid\"\n",
    "    slope_sign = \"+\" if slope > 0 else \"-\"\n",
    "\n",
    "    line_eq = f\"y = {intercept:.2f} {slope_sign} {np.abs(slope):.2f}x | R² = {r2:.2f} | p {pval_sign}\"\n",
    "\n",
    "    # ! Add color column\n",
    "    if color_method == \"species_mixed\":\n",
    "        color_var = \"species_long\"\n",
    "    else:\n",
    "        color_var = \"family_lat\"\n",
    "\n",
    "    # Create a color column\n",
    "    tmp[\"color\"] = tmp[color_var].map(dict_family).map(color_palette)\n",
    "\n",
    "    # ! Create regplot\n",
    "    lm_color = \"darkgrey\"\n",
    "    lm_plot = sns.regplot(\n",
    "        data=tmp,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        fit_reg=True,\n",
    "        ci=95,\n",
    "        scatter=True,\n",
    "        ax=ax,\n",
    "        scatter_kws={\"s\": 0},\n",
    "        # scatter_kws={\"s\": dot_size, \"color\": tmp[\"color\"]},\n",
    "        line_kws={\"color\": lm_color, \"linestyle\": slope_linetype, \"alpha\": 0.5},\n",
    "        truncate=False,\n",
    "    )\n",
    "\n",
    "    # Add equation as text\n",
    "    ax.text(\n",
    "        0.98,\n",
    "        0.96,\n",
    "        line_eq,\n",
    "        fontsize=font_base * 1,\n",
    "        fontstyle=\"italic\",\n",
    "        ha=\"right\",\n",
    "        # color=lm_color,\n",
    "        color=\"black\",\n",
    "        transform=ax.transAxes,\n",
    "        bbox=dict(\n",
    "            facecolor=\"white\", edgecolor=\"white\", boxstyle=\"round,pad=0.3\", alpha=0.75\n",
    "        ),\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    # Add scatterplot\n",
    "    # sns.scatterplot(\n",
    "    #     data=tmp,\n",
    "    #     x=x,\n",
    "    #     y=y,\n",
    "    #     ax=ax,\n",
    "    #     s=dot_size,\n",
    "    #     marker=\"o\",\n",
    "    #     edgecolor=\"white\",\n",
    "    #     linewidth=2,\n",
    "    #     color=tmp[color_var].map(color_palette),\n",
    "    # )\n",
    "\n",
    "    # ! Add scatter for each species\n",
    "    for _, row in tmp.iterrows():\n",
    "        ispecies = row[\"species\"]\n",
    "\n",
    "        # Add point if not displaying all species\n",
    "        if add_labels != \"all\":\n",
    "            ax.scatter(\n",
    "                row[x],\n",
    "                row[y],\n",
    "                s=dot_size,\n",
    "                color=row[\"color\"],  # Fill color\n",
    "                edgecolors=\"black\",  # Black edges\n",
    "                linewidths=0.5,  # Thickness of the edge lines\n",
    "                marker=\"o\",  # Circle marker (default)\n",
    "            )\n",
    "\n",
    "        # ! Add labels if needed\n",
    "\n",
    "        if add_labels == \"top9\":\n",
    "            bald_top_species_ = [\n",
    "                \"AbAl\",\n",
    "                \"CaBe\",\n",
    "                \"CaSa\",\n",
    "                \"FaSy\",\n",
    "                \"PiAb\",\n",
    "                \"PiSy\",\n",
    "                \"QuPe\",\n",
    "                \"QuPu\",\n",
    "                \"QuRo\",\n",
    "            ]\n",
    "\n",
    "        if add_labels == \"top9+\":\n",
    "            bald_top_species_ = [\n",
    "                \"AbAl\",\n",
    "                \"CaBe\",\n",
    "                \"CaSa\",\n",
    "                \"FaSy\",\n",
    "                \"PiAb\",\n",
    "                \"PiSy\",\n",
    "                \"QuPe\",\n",
    "                \"QuPu\",\n",
    "                \"QuRo\",\n",
    "                \"QuIl\",\n",
    "            ]\n",
    "\n",
    "        if add_labels == \"all\":\n",
    "            bald_top_species_ = tmp.species.unique()\n",
    "            ifw = \"normal\"\n",
    "            ifs = \"normal\"\n",
    "        else:\n",
    "            ifw = \"normal\"\n",
    "            ifs = \"normal\"\n",
    "\n",
    "        if add_labels != \"none\":\n",
    "            if ispecies in bald_top_species_:\n",
    "\n",
    "                if long_labels:\n",
    "                    ispecies = dict_species_short_lat[ispecies]\n",
    "\n",
    "                # Create the species text label\n",
    "                species_text = ax.text(\n",
    "                    row[x],\n",
    "                    row[y],\n",
    "                    ispecies,\n",
    "                    fontsize=font_base * 1.1,\n",
    "                    fontweight=ifw,\n",
    "                    fontstyle=ifs,\n",
    "                    color=row[\"color\"],\n",
    "                    zorder=3,\n",
    "                    bbox=dict(\n",
    "                        facecolor=\"white\",\n",
    "                        # alpha=0.8,\n",
    "                        alpha=0.85,\n",
    "                        edgecolor=\"white\",\n",
    "                        boxstyle=\"round,pad=0.05\",\n",
    "                    ),\n",
    "                )\n",
    "                species_texts.append(species_text)\n",
    "\n",
    "    # ! Adjust labels if needed\n",
    "    if adjust_labels:\n",
    "        if add_labels == \"all\":\n",
    "            adjust_text(\n",
    "                species_texts,\n",
    "                expand_axes=False,\n",
    "                ax=ax,\n",
    "                min_arrow_len=5,\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=\"-\",\n",
    "                    lw=0,\n",
    "                    alpha=0,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            adjust_text(\n",
    "                species_texts,\n",
    "                expand_axes=False,\n",
    "                expand=(1.5, 1.5),\n",
    "                ax=ax,\n",
    "                min_arrow_len=5,\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=\"-\",\n",
    "                    lw=0.75,\n",
    "                    alpha=1,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Axis layout\n",
    "    ax.set(ylim=(-15, 115))\n",
    "    ax.set_xlabel(xlab, weight=\"bold\", fontsize=font_base * 1.1)\n",
    "    ax.set_ylabel(ylab, weight=\"bold\", fontsize=font_base * 1.1)\n",
    "\n",
    "    # Increase tick size\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=font_base * 1)\n",
    "\n",
    "    # Add title if needed\n",
    "    if add_title:\n",
    "        ax.set_title(f\"{y} ~ {x}\", weight=\"bold\")\n",
    "\n",
    "    # Remove legend if labels are added\n",
    "    if adjust_labels:\n",
    "        ax.legend().remove()\n",
    "\n",
    "    # Remove top and right axis\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # Tight layout\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# plot_patternshare_lm(\n",
    "#     tmp, \"htot_mean\", \"warmer_wetter\", dot_size=0, add_labels=True, dirsave=dir_patterns\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Plot\n",
    "def plot_pca_ax(\n",
    "    df_plot,\n",
    "    color_palette,\n",
    "    color_method,\n",
    "    dict_family,\n",
    "    long_labels=True,\n",
    "    ax=ax,\n",
    "    add_labels=\"none\",\n",
    "    dot_size=150,\n",
    "    bold_top_species=[],\n",
    "    my_palette=\"Paired\",\n",
    "    color_pca=\"black\",\n",
    "    pca_arr_exp=1.1,\n",
    "    pca_txt_exp=2,\n",
    "    font_base=14,\n",
    "    figx=12,\n",
    "    figy=6,\n",
    "    subset=[\n",
    "        \"species\",\n",
    "        # \"mean_temp\",\n",
    "        # \"mean_spei\",\n",
    "        # \"spei+temp\",\n",
    "        \"warmer_drier\",\n",
    "        \"cooler_drier\",\n",
    "        \"other\",\n",
    "        \"cooler_wetter\",\n",
    "        \"warmer_wetter\",\n",
    "    ],\n",
    "):\n",
    "    # ! PCA ------------------------------------------------\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    replacement_pattern = \"&\"\n",
    "\n",
    "    # Get df for PCA\n",
    "    df_pca = df_plot.copy()[subset].set_index(\"species\")\n",
    "\n",
    "    # Rename change variables\n",
    "    df_pca.columns = df_pca.columns.str.replace(\"r_\", f\"r {replacement_pattern} \")\n",
    "\n",
    "    # Get numerical columns\n",
    "    numeric_cols = df_pca.columns\n",
    "\n",
    "    # Rename \"other\" to \"non sign.\"\n",
    "    # numeric_cols = [c.replace(\"other\", \"non sign.\") for c in numeric_cols]\n",
    "\n",
    "    # Standardize data\n",
    "    df_pca = (df_pca - df_pca.mean()) / df_pca.std()\n",
    "\n",
    "    # Do PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(df_pca)\n",
    "    df_pca[\"PC1\"] = pca_result[:, 0]\n",
    "    df_pca[\"PC2\"] = pca_result[:, 1]\n",
    "\n",
    "    # Attach genus information\n",
    "    # Todo: Use local instead of global df_occ variable here!\n",
    "    df_pca = df_pca.reset_index()\n",
    "    df_pca = df_pca.merge(\n",
    "        df_occ[[\"species\", \"genus_lat\", \"family_lat\"]].drop_duplicates(),\n",
    "        on=\"species\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Shorten species name and set as index\n",
    "    df_pca[\"species\"] = df_pca[\"species\"].replace({\"Populus\": \"Populus spp\"})\n",
    "    df_pca[\"species_long\"] = df_pca[\"species\"].copy()\n",
    "    df_pca[\"species\"] = (\n",
    "        df_pca[\"species\"].str.split(\" \").str[0].str[:2].str.title()\n",
    "        # + \"_\"\n",
    "        + df_pca[\"species\"].str.split(\" \").str[1].str[:2].str.title()\n",
    "        # + \".\"\n",
    "    )\n",
    "\n",
    "    # Get explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    # ! PLOT ------------------------------------------------\n",
    "\n",
    "    # ! Add color column\n",
    "    if color_method == \"species_mixed\":\n",
    "        color_var = \"species_long\"\n",
    "    else:\n",
    "        color_var = \"family_lat\"\n",
    "\n",
    "    # Create a color column\n",
    "    df_pca[\"color\"] = df_pca[color_var].map(dict_family).map(color_palette)\n",
    "\n",
    "    # Add 0 lines\n",
    "    ax.axhline(0, color=\"grey\", linewidth=0.5, linestyle=\"dotted\")\n",
    "    ax.axvline(0, color=\"grey\", linewidth=0.5, linestyle=\"dotted\")\n",
    "\n",
    "    if add_labels == \"all\":\n",
    "        dot_size = 0\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=df_pca,\n",
    "        x=\"PC1\",\n",
    "        y=\"PC2\",\n",
    "        ax=ax,\n",
    "        s=dot_size,\n",
    "        marker=\"o\",\n",
    "        color=df_pca[\"color\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    # ! Add species labels\n",
    "    if add_labels not in [\"none\", \"top9\", \"top9+\", \"all\"]:\n",
    "        raise ValueError(\"Please select a valid option for add_labels.\")\n",
    "\n",
    "    if add_labels == \"none\":\n",
    "        adjust_labels = False\n",
    "    else:\n",
    "        adjust_labels = True\n",
    "\n",
    "    if add_labels == \"top9\":\n",
    "        bald_top_species_ = [\n",
    "            \"AbAl\",\n",
    "            \"CaBe\",\n",
    "            \"CaSa\",\n",
    "            \"FaSy\",\n",
    "            \"PiAb\",\n",
    "            \"PiSy\",\n",
    "            \"QuPe\",\n",
    "            \"QuPu\",\n",
    "            \"QuRo\",\n",
    "        ]\n",
    "\n",
    "    if add_labels == \"top9+\":\n",
    "        bald_top_species_ = [\n",
    "            \"AbAl\",\n",
    "            \"CaBe\",\n",
    "            \"CaSa\",\n",
    "            \"FaSy\",\n",
    "            \"PiAb\",\n",
    "            \"PiSy\",\n",
    "            \"QuPe\",\n",
    "            \"QuPu\",\n",
    "            \"QuRo\",\n",
    "            \"QuIl\",\n",
    "        ]\n",
    "\n",
    "    if add_labels == \"all\":\n",
    "        bald_top_species_ = df_pca.species.unique()\n",
    "        ifw = \"normal\"\n",
    "        ifs = \"normal\"\n",
    "    else:\n",
    "        ifw = \"normal\"\n",
    "        ifs = \"normal\"\n",
    "\n",
    "    txt_species = []\n",
    "    if add_labels != \"none\":\n",
    "        for i, ispecies in enumerate(df_pca.species):\n",
    "\n",
    "            if ispecies in bald_top_species_:\n",
    "\n",
    "                # Create the species text label\n",
    "                if long_labels:\n",
    "                    ispecies = dict_species_short_lat[ispecies]\n",
    "\n",
    "                species_text = ax.text(\n",
    "                    df_pca[\"PC1\"].iloc[i],\n",
    "                    df_pca[\"PC2\"].iloc[i],\n",
    "                    ispecies,\n",
    "                    color=df_pca[\"color\"].iloc[i],\n",
    "                    fontsize=10,\n",
    "                    fontweight=ifw,\n",
    "                    fontstyle=ifs,\n",
    "                    zorder=3,\n",
    "                    # ha=\"center\",\n",
    "                    # va=\"center\",\n",
    "                    bbox=dict(\n",
    "                        facecolor=\"white\",\n",
    "                        # alpha=0.8,\n",
    "                        alpha=0.85,\n",
    "                        edgecolor=\"white\",\n",
    "                        boxstyle=\"round,pad=0.05\",\n",
    "                    ),\n",
    "                )\n",
    "                txt_species.append(species_text)\n",
    "\n",
    "    # ! PCA ARROWS ------------------------------------------------------------------------------------\n",
    "    txt_pca = []\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    for i, feature in enumerate(numeric_cols):\n",
    "\n",
    "        arrow_x = loadings[i, 0] * pca_arr_exp\n",
    "        arrow_y = loadings[i, 1] * pca_arr_exp\n",
    "        text_x = loadings[i, 0] * pca_txt_exp\n",
    "        text_y = loadings[i, 1] * pca_txt_exp\n",
    "\n",
    "        if feature == f\"warmer {replacement_pattern} drier\":\n",
    "            text_x = -1.5\n",
    "            text_y = -2.2\n",
    "        elif feature == f\"warmer {replacement_pattern} wetter\":\n",
    "            text_x = -1.4\n",
    "            text_y = 2.2\n",
    "        elif feature == f\"cooler {replacement_pattern} drier\":\n",
    "            text_x = 1.3\n",
    "            text_y = -1.5\n",
    "        elif feature == f\"cooler {replacement_pattern} wetter\":\n",
    "            text_x = 1.2\n",
    "            text_y = 1\n",
    "        elif feature == \"other\":\n",
    "            text_x = 1.3\n",
    "            text_y = -0.5\n",
    "        else:\n",
    "            raise ValueError(f\"feature not correctly matched: {feature}\")\n",
    "\n",
    "        ax.arrow(\n",
    "            0,\n",
    "            0,\n",
    "            arrow_x,\n",
    "            arrow_y,\n",
    "            color=color_pca,\n",
    "            alpha=1,\n",
    "            linewidth=1.25,\n",
    "            head_width=0.1,\n",
    "            head_length=0.1,\n",
    "        )\n",
    "\n",
    "        txt_pca.append(\n",
    "            ax.text(\n",
    "                text_x,\n",
    "                text_y,\n",
    "                feature,\n",
    "                color=color_pca,\n",
    "                ha=\"center\",\n",
    "                # va=\"center\",\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=font_base * 1,\n",
    "                bbox=dict(\n",
    "                    facecolor=\"white\",\n",
    "                    alpha=0.8,\n",
    "                    # alpha=0,\n",
    "                    edgecolor=color_pca,\n",
    "                    boxstyle=\"round,pad=0.2\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ! Adjust text labels ---------------------------------------------------------------\n",
    "    if adjust_labels:\n",
    "        # Append \"warmer - drier\" and \"warmer - wetter\" to avoid overlaps\n",
    "        display(txt_pca)\n",
    "        if long_labels:\n",
    "            # txt_species.append(txt_pca[0])\n",
    "            txt_species.append(txt_pca[1])\n",
    "            # txt_species.append(txt_pca[2])\n",
    "            # txt_species.append(txt_pca[3])\n",
    "            # txt_species.append(txt_pca[4])\n",
    "\n",
    "        if add_labels == \"all\":\n",
    "            adjust_text(\n",
    "                txt_species,\n",
    "                expand_axes=False,\n",
    "                expand=(1, 1),\n",
    "                ax=ax,\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=\"-\",\n",
    "                    lw=0,\n",
    "                    alpha=0,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            adjust_text(\n",
    "                txt_species,\n",
    "                expand_axes=False,\n",
    "                expand=(1.5, 1.5),\n",
    "                ax=ax,\n",
    "                min_arrow_len=5,\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=\"-\",\n",
    "                    lw=0.75,\n",
    "                    alpha=1,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Add legend for color\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker=\"o\", color=\"w\", label=key, markerfacecolor=value)\n",
    "        for key, value in color_palette.items()\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=handles,\n",
    "        title=color_var,\n",
    "        fontsize=font_base * 1,\n",
    "        loc=\"center right\",\n",
    "        markerscale=2,\n",
    "        # va=\"center\",\n",
    "        bbox_to_anchor=(1.25, 0.5),\n",
    "        frameon=False,\n",
    "    ).set_title(\"\", prop={\"weight\": \"bold\", \"size\": font_base * 1.1})\n",
    "\n",
    "    # Add explained variance to axis\n",
    "    ax.set_xlabel(\n",
    "        f\"Principal component 1 ({explained_variance[0] * 100:.0f}%)\",\n",
    "        weight=\"bold\",\n",
    "        size=font_base * 1.1,\n",
    "    )\n",
    "    ax.set_ylabel(\n",
    "        f\"Principal component 2 ({explained_variance[1] * 100:.0f}%)\",\n",
    "        weight=\"bold\",\n",
    "        size=font_base * 1.1,\n",
    "    )\n",
    "\n",
    "    # Remove top and right axis\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # Increase tick size\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=font_base * 0.9)\n",
    "\n",
    "    # Set same limits\n",
    "    # Limit fix\n",
    "    ymax = 2.75 if add_labels else 2.825\n",
    "    ax.set_xlim(-2.6, 3.2)\n",
    "    ax.set_ylim(-ymax, ymax)\n",
    "\n",
    "    # Return ax\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pca_lm_fig(mychange, add_labels, color_method, long_labels):\n",
    "\n",
    "    # ! Input check\n",
    "    if mychange not in [\"warmer_wetter\", \"wetter\", \"mixed\"]:\n",
    "        raise ValueError(\"mychange must be 'warmer_wetter' or 'wetter'.\")\n",
    "\n",
    "    if mychange == \"warmer_wetter\":\n",
    "        list_change = [\"warmer_wetter\"] * 3\n",
    "    elif mychange == \"wetter\":\n",
    "        list_change = [\"wetter\"] * 3\n",
    "    elif mychange == \"mixed\":\n",
    "        list_change = [\"wetter\", \"wetter\", \"warmer_wetter\"]\n",
    "\n",
    "    # ! Select color palette\n",
    "    legend_title = \"Family\"\n",
    "    legend_nrow = 6\n",
    "    if color_method == \"latin\":\n",
    "        color_palette = dict(zip(family_lat_order, color_order))\n",
    "    elif color_method == \"common\":\n",
    "        color_palette = dict(zip(family_en_order, color_order))\n",
    "        dict_family = dict_family_en\n",
    "    elif color_method == \"mixed\":\n",
    "        color_palette = dict(zip(family_both_order, color_order))\n",
    "        dict_family = dict_family_both\n",
    "    elif color_method == \"species_mixed\":\n",
    "        dict_family = dict_top9_lat_mixed\n",
    "        color_palette = dict_top9_mixed_color\n",
    "        legend_title = \"Species\"\n",
    "        legend_nrow = 4\n",
    "    else:\n",
    "        chime.error()\n",
    "        raise ValueError(\"Please select a valid color palette (color_method).\")\n",
    "\n",
    "    # Figure settings\n",
    "    add_title = False\n",
    "    dot_size = 60\n",
    "\n",
    "    # Create a figure and a grid of subplots\n",
    "    fig, axs = plt.subplots(\n",
    "        2,\n",
    "        2,\n",
    "        figsize=(18, 10),\n",
    "        # figsize=(40, 7),  # For 4x1 plot\n",
    "    )\n",
    "    # plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # ! PCA Plot\n",
    "    plot_pca_ax(\n",
    "        df_plot,\n",
    "        color_palette=color_palette,\n",
    "        color_method=color_method,\n",
    "        dict_family=dict_family,\n",
    "        long_labels=long_labels,\n",
    "        ax=axs[0],\n",
    "        font_base=10,\n",
    "        dot_size=dot_size,\n",
    "        add_labels=add_labels,\n",
    "        # bold_top_species=bold_species_short,\n",
    "    )\n",
    "\n",
    "    # Remove legend from PCA plot\n",
    "    axs[0].get_legend().remove()\n",
    "\n",
    "    # ! LM Plots\n",
    "    plot_patternshare_lm_ax(\n",
    "        axs[1],\n",
    "        tmp2,\n",
    "        \"nii_drought\",\n",
    "        list_change[0],\n",
    "        color_palette=color_palette,\n",
    "        color_method=color_method,\n",
    "        dict_family=dict_family,\n",
    "        long_labels=long_labels,\n",
    "        dot_size=dot_size,\n",
    "        # bold_top_species=bold_species_short,\n",
    "        add_labels=add_labels,\n",
    "        pval_threshold=pval_threshold,\n",
    "        add_title=add_title,\n",
    "        dirsave=dir_patterns,\n",
    "    )\n",
    "\n",
    "    plot_patternshare_lm_ax(\n",
    "        axs[2],\n",
    "        tmp2,\n",
    "        \"htot_mean\",\n",
    "        list_change[1],\n",
    "        color_palette=color_palette,\n",
    "        color_method=color_method,\n",
    "        dict_family=dict_family,\n",
    "        long_labels=long_labels,\n",
    "        dot_size=dot_size,\n",
    "        # bold_top_species=bold_species_short,\n",
    "        add_labels=add_labels,\n",
    "        pval_threshold=pval_threshold,\n",
    "        add_title=add_title,\n",
    "        dirsave=dir_patterns,\n",
    "    )\n",
    "\n",
    "    plot_patternshare_lm_ax(\n",
    "        axs[3],\n",
    "        tmp2,\n",
    "        \"nii_shade\",\n",
    "        list_change[2],\n",
    "        color_palette=color_palette,\n",
    "        color_method=color_method,\n",
    "        dict_family=dict_family,\n",
    "        long_labels=long_labels,\n",
    "        dot_size=dot_size,\n",
    "        # bold_top_species=bold_species_short,\n",
    "        add_labels=add_labels,\n",
    "        pval_threshold=pval_threshold,\n",
    "        add_title=add_title,\n",
    "        dirsave=dir_patterns,\n",
    "    )\n",
    "\n",
    "    # Add letters to each subplot\n",
    "    letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.text(\n",
    "            0.02,\n",
    "            0.98,\n",
    "            letters[i],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "            # fontstyle=\"italic\",\n",
    "        )\n",
    "\n",
    "    # Fix axes\n",
    "    axs[0].set_ylim(-3.2, 3.9)\n",
    "    axs[0].set_xlim(-2.6, 3.9)\n",
    "    axs[1].set_ylim(-5, 55)\n",
    "    axs[2].set_ylim(-5, 55)\n",
    "    axs[3].set_ylim(-5, 55)\n",
    "\n",
    "    # Set all axes labels to same size\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n",
    "        ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "\n",
    "    # Create new legend using color_palette with circular markers and black edges\n",
    "    legend_patches = [\n",
    "        mlines.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",  # Circle marker\n",
    "            color=\"black\",  # Edge color (black)\n",
    "            markerfacecolor=color,  # Fill color from color_palette\n",
    "            markeredgewidth=1,  # Edge line width\n",
    "            markersize=10,  # Circle size\n",
    "            linestyle=\"None\",  # No connecting line\n",
    "            label=label,  # Label for legend\n",
    "        )\n",
    "        for label, color in color_palette.items()\n",
    "    ]\n",
    "\n",
    "    # Plotting the legend\n",
    "    plt.legend(\n",
    "        handles=legend_patches,\n",
    "        loc=\"lower center\",\n",
    "        ncol=legend_nrow,\n",
    "        bbox_to_anchor=(-0.11, -0.5),\n",
    "        # bbox_to_anchor=(-1.3, -0.3), # For 4x1 plot\n",
    "        frameon=False,\n",
    "        fontsize=12,\n",
    "        title=legend_title,\n",
    "        handlelength=0.7,\n",
    "    )\n",
    "\n",
    "    # Adjust legend title formatting\n",
    "    plt.setp(\n",
    "        plt.gca().get_legend().get_title(),\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(w_pad=200, h_pad=2)\n",
    "    os.makedirs(f\"{dir_patterns}/pca+lms\", exist_ok=True)\n",
    "    plt.savefig(\n",
    "        f\"{dir_patterns}/pca+lms/{mychange}-labels_{add_labels}_long_{long_labels}-colored_{color_method}.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    # plt.savefig(\n",
    "    #     f\"{dir_patterns}/pca+lms/{mychange}-labels_{add_labels}_long_{long_labels}-colored_{color_method}.pdf\",\n",
    "    # )\n",
    "    clear_output()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triangle_ax(\n",
    "    ax,\n",
    "    df_in,\n",
    "    spei_or_temp,\n",
    "    color_palette,\n",
    "    bold_top_species=[],\n",
    "    show_major_only=False,\n",
    "    dot_size=0,\n",
    "    add_labels=True,\n",
    "    pval_threshold=0.01,\n",
    "    add_title=True,\n",
    "    font_base=10,\n",
    "    dirsave=None,\n",
    "):\n",
    "\n",
    "    # Libs\n",
    "    import mpltern\n",
    "\n",
    "    # Input check\n",
    "    if add_labels and dot_size != 0:\n",
    "        print(f\" To remove dots, set dot_size to 0.\")\n",
    "\n",
    "    # Create lists to hold the text objects for adjustment\n",
    "    species_texts = []\n",
    "\n",
    "    # Extract columns\n",
    "    if spei_or_temp == \"spei\":\n",
    "        cols = [\"drier\", \"wetter\", \"spei_unclear\"]\n",
    "    elif spei_or_temp == \"temp\":\n",
    "        cols = [\"cooler\", \"warmer\", \"temp_unclear\"]\n",
    "\n",
    "    tmp = df_lrs[cols + [\"species\", \"family_lat\", \"genus_lat\"]].copy()\n",
    "\n",
    "    # Remove \"_unclear\" to have clean columns names\n",
    "    corner_labels = tmp.columns.str.replace(f\"{spei_or_temp}_\", \"\").str.capitalize()\n",
    "\n",
    "    # Normalize cols for ternary plot\n",
    "    tmp[cols] = tmp[cols].div(tmp[cols].sum(axis=1), axis=0)\n",
    "\n",
    "    # Rename family column if not in latin\n",
    "    if \"Beech\" or \"Fagaceae (Beech)\" in color_palette.keys():\n",
    "        # todo: should not use global dict_family\n",
    "        tmp[color_var] = tmp[color_var].map(dict_family)\n",
    "\n",
    "    # Create a color column\n",
    "    tmp[\"color\"] = tmp[color_var].map(color_palette)\n",
    "\n",
    "    # Plot species labels\n",
    "    for _, row in tmp.iterrows():\n",
    "        # Get coordinates\n",
    "        ax.scatter(\n",
    "            row[cols[0]],\n",
    "            row[cols[1]],\n",
    "            row[cols[2]],\n",
    "            s=dot_size,\n",
    "            color=color_palette[row[color_var]],\n",
    "        )\n",
    "\n",
    "        if add_labels:\n",
    "\n",
    "            # Define fontweight\n",
    "            ifw = \"normal\"\n",
    "            ifs = \"normal\"\n",
    "            if row[\"species\"] in bold_top_species:\n",
    "                ifw = \"bold\"\n",
    "                ifs = \"italic\"\n",
    "            if show_major_only and row[\"species\"] not in bold_top_species:\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                # Create the species text label\n",
    "                species_text = ax.text(\n",
    "                    row[cols[0]],\n",
    "                    row[cols[1]],\n",
    "                    row[cols[2]],\n",
    "                    row[\"species\"],\n",
    "                    fontsize=font_base * 1,\n",
    "                    fontweight=ifw,\n",
    "                    fontstyle=ifs,\n",
    "                    color=color_palette[row[color_var]],\n",
    "                    zorder=3,\n",
    "                )\n",
    "                species_texts.append(species_text)\n",
    "\n",
    "    if add_labels:\n",
    "        # Adjust text positions to avoid overlap with arrows\n",
    "        adjust_text(\n",
    "            species_texts,\n",
    "            expand_axes=False,\n",
    "            ax=ax,\n",
    "            # arrowprops=dict(\n",
    "            #     arrowstyle=\"->\",\n",
    "            #     lw=1,\n",
    "            #     alpha=0.75,\n",
    "            # ),\n",
    "        )\n",
    "\n",
    "    # Add ticks and labels for the corners\n",
    "    ax.set_tlabel(corner_labels[0], fontsize=font_base * 1.1)  # , fontweight=\"bold\")\n",
    "    ax.set_llabel(corner_labels[1], fontsize=font_base * 1.1)  # , fontweight=\"bold\")\n",
    "    ax.set_rlabel(corner_labels[2], fontsize=font_base * 1.1)  # , fontweight=\"bold\")\n",
    "\n",
    "    # Customize tick positions and labels\n",
    "    ax.taxis.set_ticks([0.2, 0.4, 0.6, 0.8])\n",
    "    ax.laxis.set_ticks([0.2, 0.4, 0.6, 0.8])\n",
    "    ax.raxis.set_ticks([0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Settings\n",
    "withFrEx = True\n",
    "pval_threshold = 0.05\n",
    "\n",
    "# Figure Settings\n",
    "save_fig = False\n",
    "sign_only = False\n",
    "add_labels = True\n",
    "add_title = False\n",
    "dot_size = 100\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Suppress matplotlib warnings\n",
    "# Small fix because '+' does not work in variable name\n",
    "df_lrs.rename(columns={\"spei+temp\": \"spei_temp\"}, inplace=True)\n",
    "\n",
    "# Get data\n",
    "tmp2 = df_lrs.copy()\n",
    "# tmp_dir = f\"{dir_patterns}/pattern_lms-pval_{pval_threshold}\"\n",
    "tmp_dir = f\"{dir_patterns}/pca+lms/\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "# Rename columns\n",
    "tmp2 = tmp2.rename(\n",
    "    columns={\n",
    "        \"Absolute increase [%/yr^2]\": \"mort_increase_abs\",\n",
    "        \"Relative increase [%/yr]\": \"mort_increase_rel\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add coloring column\n",
    "# tmp2[\"family_col\"] = tmp2[\"family_lat\"].map(dict_family)\n",
    "\n",
    "# Show df\n",
    "display(tmp2.head(2))\n",
    "\n",
    "# Make dir\n",
    "# os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "tmp_list = []\n",
    "\n",
    "if add_labels:\n",
    "    dot_size = 0\n",
    "\n",
    "xs = [\n",
    "    # \"n_trees_total\",\n",
    "    # \"count\",\n",
    "    # \"slope\",\n",
    "    # \"ell_light\",\n",
    "    # \"ell_temperature\",\n",
    "    # \"ell_soil_reaction\",\n",
    "    # \"ell_soil_moisture\",\n",
    "    # \"ell_nitrogen\",\n",
    "    # \"ell_salt\",\n",
    "    \"nii_drought\",\n",
    "    \"nii_shade\",\n",
    "    \"htot_mean\",\n",
    "    # \"nii_waterlog\",\n",
    "    # \"mort_increase_abs\",\n",
    "    # \"mort_increase_rel\",\n",
    "]\n",
    "\n",
    "ys = [\n",
    "    # \"warmer\",\n",
    "    \"warmer_drier\",\n",
    "    \"warmer_wetter\",\n",
    "    # \"cooler\",\n",
    "    # \"cooler_drier\",\n",
    "    # \"cooler_wetter\",\n",
    "    \"drier\",\n",
    "    \"wetter\",\n",
    "    # \"other\",\n",
    "    # \"response_temp_unclear\",\n",
    "    # \"response_spei_unclear\",\n",
    "    # \"mean_spei\",\n",
    "    # \"mean_temp\",\n",
    "    # \"mean_roc\",\n",
    "    # \"spei_temp\",\n",
    "    # \"temp_spei_ratio\",\n",
    "]\n",
    "\n",
    "for y in ys:\n",
    "    # display(f\"🟨 {x} -------------------------\")\n",
    "    for x in xs:\n",
    "\n",
    "        idf = tmp2.copy()\n",
    "        idf = idf.dropna(subset=[x, y])\n",
    "\n",
    "        # Note: Removal of species needs to be done here and in the plot_patternshare_lm function\n",
    "        if x in [\"mort_increase_rel\", \"mort_increase_abs\", \"slope\"]:\n",
    "            idf = idf.query(\n",
    "                \"species != 'BuSe'\"\n",
    "            ).copy()  # has extreme increase which distorts the regression\n",
    "            # idf = idf.query(\"species != 'FrEx'\").copy()\n",
    "\n",
    "        # Run lm\n",
    "        lm = smf.ols(f\"{y} ~ {x}\", data=idf).fit()\n",
    "\n",
    "        # Get coefficients\n",
    "        r2 = lm.rsquared\n",
    "        slope = lm.params[1]\n",
    "        intercept = lm.params[0]\n",
    "        pval = lm.pvalues[1]\n",
    "        pval_sign = f\"= {pval:.2f}\" if pval > pval_threshold else f\"< {pval_threshold}\"\n",
    "        slope_linetype = \"solid\" if pval < pval_threshold else \"dashed\"\n",
    "        slope_sign = \"+\" if slope > 0 else \"-\"\n",
    "        line_eq = f\"y = {intercept:.2f} {slope_sign} {np.abs(slope):.2f}x \\t| R² = {r2:.2f} | p {pval_sign} ({pval:.3f})\"\n",
    "        exclam = \"!!!\" if pval < pval_threshold else \"\"\n",
    "\n",
    "        # Save information in list\n",
    "        tmp_list.append(\n",
    "            {\n",
    "                \"exclam\": exclam,\n",
    "                \"y\": y,\n",
    "                \"x\": x,\n",
    "                \"slope\": slope,\n",
    "                \"intercept\": intercept,\n",
    "                \"r2\": r2.round(2),\n",
    "                \"pval\": pval.round(4),\n",
    "                \"pval_sign\": pval_sign,\n",
    "                \"slope_linetype\": slope_linetype,\n",
    "                \"slope_sign\": slope_sign,\n",
    "                \"line_eq\": line_eq,\n",
    "                \"n_species\": idf[\"species\"].nunique(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"{exclam}\\t {y:<25} ~ {x:<25} {line_eq} {exclam}\")\n",
    "        if save_fig:\n",
    "            plot_patternshare_lm(\n",
    "                idf,\n",
    "                x,\n",
    "                y,\n",
    "                color_palette=color_palette,\n",
    "                color_var=\"family_col\",\n",
    "                dot_size=dot_size,\n",
    "                add_labels=add_labels,\n",
    "                pval_threshold=pval_threshold,\n",
    "                add_title=add_title,\n",
    "                dirsave=tmp_dir,\n",
    "                save_fig=save_fig,\n",
    "            )\n",
    "\n",
    "# Save list as csv\n",
    "tmp_list = pd.DataFrame(tmp_list)\n",
    "tmp_list = tmp_list.sort_values([\"exclam\", \"y\", \"x\"])\n",
    "tmp_list.to_csv(f\"{tmp_dir}/lm-model_frequency-species_traits.csv\", index=False)\n",
    "\n",
    "# # Show\n",
    "clear_output(wait=True)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "display(\n",
    "    # tmp_list.sort_values([\"exclam\", \"y\", \"pval\"])[\n",
    "    tmp_list.sort_values([\"x\", \"y\"])[\n",
    "        [\n",
    "            \"x\",\n",
    "            \"y\",\n",
    "            \"pval\",\n",
    "            \"slope_sign\",\n",
    "            \"exclam\",\n",
    "            \"slope\",\n",
    "            \"intercept\",\n",
    "            \"r2\",\n",
    "            \"n_species\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "pd.set_option(\"display.max_rows\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most important species in bold\n",
    "bold_species_long = (\n",
    "    df_plot.sort_values(\"n_trees_total\", ascending=False).species.head(9).tolist()\n",
    ")\n",
    "bold_species_short = (\n",
    "    tmp2.sort_values(\"n_trees_total\", ascending=False).species.head(9).tolist()\n",
    ")\n",
    "print(bold_species_long)\n",
    "print(bold_species_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA + LMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig. 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pca_lm_fig(\"mixed\", \"top9+\", \"mixed\", long_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pca_lm_fig(\"wetter\", \"top9+\", \"mixed\", long_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pca_lm_fig(\"warmer_wetter\", \"top9+\", \"mixed\", long_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "- Rerun the part of this script above this cell by adjusting the `which_analysis` parameter in the settings cell in the beginning to see consistent results across different SMOTE and RFE settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Comparing performances of final models across different analyses\n",
    "# Load and compare final model performances\n",
    "files = glob.glob(\n",
    "    \"./extra_runs/smote*/_analysis/_top9_species/*/pattern_analysis/by_mk/roc_0.6-min_group_share_0.6/tables/model_performance_summary.csv\"\n",
    ")\n",
    "\n",
    "analysis_type = [s.split(\"/\")[2] for s in files]\n",
    "\n",
    "files = pd.DataFrame(\n",
    "    {\n",
    "        \"file\": files,\n",
    "        \"analysis_type\": analysis_type,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Keep only most recent file for each analysis type\n",
    "files = files.sort_values([\"analysis_type\", \"file\"])\n",
    "files = files.groupby(\"analysis_type\").last().reset_index()\n",
    "\n",
    "df = []\n",
    "for i, row in files.iterrows():\n",
    "    tmp = pd.read_csv(row[\"file\"])\n",
    "    tmp = tmp.query(\"Species in ['Mean', 'SD']\")\n",
    "    tmp[\"analysis_type\"] = row[\"analysis_type\"]\n",
    "    tmp = tmp.rename(\n",
    "        columns={\n",
    "            \"Train Roc Auc\": \"Train ROC-AUC\",\n",
    "            \"Test Roc Auc\": \"Test ROC-AUC\",\n",
    "            \"Train Pr Auc\": \"Train PR-AUC\",\n",
    "            \"Test Pr Auc\": \"Test PR-AUC\",\n",
    "        }\n",
    "    )\n",
    "    df.append(tmp)\n",
    "\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "# Filter only 'Mean' rows\n",
    "df_mean = df[df[\"Species\"] == \"Mean\"]\n",
    "\n",
    "# Metrics and styles\n",
    "metrics = [\"ROC-AUC\", \"PR-AUC\", \"Precision\", \"Recall\", \"F1\"]\n",
    "linestyles = {\"Train\": \"--\", \"Test\": \"-\"}\n",
    "colors = {\n",
    "    \"ROC-AUC\": \"tab:blue\",\n",
    "    \"PR-AUC\": \"tab:orange\",\n",
    "    \"Precision\": \"tab:green\",\n",
    "    \"Recall\": \"tab:red\",\n",
    "    \"F1\": \"tab:purple\",\n",
    "}\n",
    "markers = {\n",
    "    \"ROC-AUC\": \"o\",\n",
    "    \"PR-AUC\": \"s\",\n",
    "    \"Precision\": \"D\",\n",
    "    \"Recall\": \"^\",\n",
    "    \"F1\": \"v\",\n",
    "}\n",
    "\n",
    "# Melt to long format\n",
    "train_cols = [f\"Train {m}\" for m in metrics]\n",
    "train_cols = [f\"Train {m}\" for m in metrics]\n",
    "test_cols = [f\"Test {m}\" for m in metrics]\n",
    "\n",
    "df_train = df_mean[[\"analysis_type\"] + train_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"Score\"\n",
    ")\n",
    "df_train[\"Set\"] = \"Train\"\n",
    "df_train[\"Metric\"] = df_train[\"Metric\"].str.replace(\"Train \", \"\")\n",
    "\n",
    "df_test = df_mean[[\"analysis_type\"] + test_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"Score\"\n",
    ")\n",
    "df_test[\"Set\"] = \"Test\"\n",
    "df_test[\"Metric\"] = df_test[\"Metric\"].str.replace(\"Test \", \"\")\n",
    "\n",
    "df_long = pd.concat([df_train, df_test])\n",
    "\n",
    "# Melt SD data\n",
    "df_sd = df[df[\"Species\"] == \"SD\"]\n",
    "\n",
    "df_train_sd = df_sd[[\"analysis_type\"] + train_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"SD\"\n",
    ")\n",
    "df_train_sd[\"Set\"] = \"Train\"\n",
    "df_train_sd[\"Metric\"] = df_train_sd[\"Metric\"].str.replace(\"Train \", \"\")\n",
    "\n",
    "df_test_sd = df_sd[[\"analysis_type\"] + test_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"SD\"\n",
    ")\n",
    "df_test_sd[\"Set\"] = \"Test\"\n",
    "df_test_sd[\"Metric\"] = df_test_sd[\"Metric\"].str.replace(\"Test \", \"\")\n",
    "\n",
    "df_sd_long = pd.concat([df_train_sd, df_test_sd])\n",
    "\n",
    "# Merge SDs with main data\n",
    "df_long = pd.merge(\n",
    "    df_long, df_sd_long, on=[\"analysis_type\", \"Metric\", \"Set\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Optional: numeric x positions (if needed for mapping)\n",
    "analysis_types = [\"smotek_0\", \"smotek_1\", \"smotek_5\", \"smotek_15\"]\n",
    "custom_labels = [\"No SMOTE\", \"k = 1\", \"k = 5\", \"k = 15\"]\n",
    "x_pos_map = dict(zip(analysis_types, range(len(analysis_types))))\n",
    "df_long[\"x\"] = df_long[\"analysis_type\"].map(x_pos_map)\n",
    "\n",
    "# Plot with error bars\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "for metric in metrics:\n",
    "    for set_type in [\"Train\", \"Test\"]:\n",
    "        subset = df_long[(df_long[\"Metric\"] == metric) & (df_long[\"Set\"] == set_type)]\n",
    "        ax.plot(\n",
    "            subset[\"analysis_type\"],\n",
    "            subset[\"Score\"],\n",
    "            color=colors[metric],\n",
    "            linestyle=linestyles[set_type],\n",
    "            marker=markers[metric],\n",
    "            label=f\"{metric} ({set_type})\",  # Not used for legend directly\n",
    "            markersize=8,\n",
    "        )\n",
    "        # With Error Bars\n",
    "        # ax.errorbar(\n",
    "        #     subset[\"x\"],\n",
    "        #     subset[\"Score\"],\n",
    "        #     yerr=subset[\"SD\"],\n",
    "        #     color=colors[metric],\n",
    "        #     linestyle=linestyles[set_type],\n",
    "        #     marker=markers[metric],\n",
    "        #     markersize=8,\n",
    "        #     capsize=4,\n",
    "        # )\n",
    "\n",
    "# X-ticks and labels\n",
    "ax.set_xticks(range(len(custom_labels)))\n",
    "ax.set_xticklabels(custom_labels, fontsize=12)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n",
    "\n",
    "# Labels and style\n",
    "# ax.set_title(\"Train vs Test Performance by Analysis Type\")\n",
    "ax.set_xlabel(\"Analysis Type\", fontsize=1)\n",
    "ax.set_ylabel(\"Score\", fontsize=14)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Custom legend\n",
    "metric_handles = [\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        color=colors[m],\n",
    "        marker=markers[m],\n",
    "        linestyle=\"\",\n",
    "        markersize=8,\n",
    "        label=m,\n",
    "    )\n",
    "    for m in metrics\n",
    "]\n",
    "\n",
    "set_handles = [\n",
    "    Line2D([0], [0], color=\"gray\", linestyle=ls, lw=2, label=st)\n",
    "    for st, ls in linestyles.items()\n",
    "]\n",
    "\n",
    "ax.legend(\n",
    "    handles=metric_handles + set_handles,\n",
    "    title=\"\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.01, 0.5),\n",
    "    frameon=False,\n",
    "    borderaxespad=0,\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "plt.savefig(f\"{dir_patterns}/model_performance_smote_k.png\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis Distance of Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all final models\n",
    "runs_dir = \"./model_runs/all_runs\"\n",
    "if not os.path.exists(runs_dir):\n",
    "    runs_dir = \"/Volumes/SAMSUNG 1TB/all_runs\"\n",
    "    if not os.path.exists(runs_dir):\n",
    "        raise FileNotFoundError(f\"No runs found locally or on external drive.\")\n",
    "\n",
    "\n",
    "df_available = glob.glob(f\"{runs_dir}/run_*/*/final_model_performance.csv\")\n",
    "df_available = pd.DataFrame(df_available, columns=[\"file\"])\n",
    "df_available[\"species\"] = df_available[\"file\"].str.split(\"/\").str[-2]\n",
    "df_available[\"model\"] = df_available[\"file\"].str.split(\"/\").str[-3]\n",
    "df_available[\"seed\"] = df_available[\"model\"].str.split(\"_\").str[-1]\n",
    "df_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for additional analyses\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import string\n",
    "\n",
    "# Calculate Mahalanobis distances between train, SMOTE, and test sets\n",
    "results = []\n",
    "load_file = True\n",
    "filename = f\"{dir_patterns}/smote_analysis-mahalanobis_distances.csv\"\n",
    "\n",
    "if load_file and os.path.isfile(filename):\n",
    "    print(f\" - File {filename} already exists, loading existing results.\")\n",
    "    df_md = pd.read_csv(filename)\n",
    "    results = df_md.to_dict(\"records\")\n",
    "else:\n",
    "\n",
    "    # Loop through df_available (must already be loaded)\n",
    "    for i, row in (\n",
    "        df_available.sort_values([\"seed\", \"species\"]).reset_index(drop=True).iterrows()\n",
    "    ):\n",
    "        ifile = row.file\n",
    "        ispecies = row.species\n",
    "        iseed = row.seed\n",
    "\n",
    "        # Load data\n",
    "        try:\n",
    "            xtrain = pd.read_csv(\n",
    "                ifile.replace(\"final_model_performance.csv\", \"final_model/X_train.csv\")\n",
    "            ).drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "            ytrain = pd.read_csv(\n",
    "                ifile.replace(\"final_model_performance.csv\", \"final_model/y_train.csv\")\n",
    "            ).drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "            xtest = pd.read_csv(\n",
    "                ifile.replace(\"final_model_performance.csv\", \"final_model/X_test.csv\")\n",
    "            ).drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "            ytest = pd.read_csv(\n",
    "                ifile.replace(\"final_model_performance.csv\", \"final_model/y_test.csv\")\n",
    "            ).drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "            treeid = pd.read_csv(\n",
    "                ifile.replace(\n",
    "                    \"final_model_performance.csv\", \"treeid/X_train_treeid.csv\"\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {ispecies} seed {iseed} due to file error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Split original and SMOTE training data\n",
    "        test = pd.concat([ytest, xtest], axis=1)\n",
    "        train = pd.concat([ytrain, xtrain], axis=1)\n",
    "        train_org = train.iloc[: treeid.shape[0]]\n",
    "        train_smote = train.iloc[treeid.shape[0] :]\n",
    "\n",
    "        # Extract subsets\n",
    "        train_org = train_org.query(\"target == 1\").drop(columns=[\"target\"])\n",
    "        train_smote = train_smote.query(\"target == 1\").drop(columns=[\"target\"])\n",
    "        test = test.query(\"target == 1\").drop(columns=[\"target\"])\n",
    "\n",
    "        # Skip small sets\n",
    "        if train_org.shape[0] < 2 or train_smote.shape[0] < 2:\n",
    "            continue\n",
    "\n",
    "        # Use pooled covariance from all three sets\n",
    "        pooled = pd.concat([train_org, train_smote, test])\n",
    "        cov = np.cov(pooled.T)\n",
    "        try:\n",
    "            inv_cov = np.linalg.inv(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            continue\n",
    "\n",
    "        # Compute means\n",
    "        mean_org = train_org.mean().values\n",
    "        mean_smote = train_smote.mean().values\n",
    "        mean_test = test.mean().values\n",
    "\n",
    "        # Calculate Mahalanobis distances\n",
    "        md_train_smote = mahalanobis(mean_org, mean_smote, inv_cov)\n",
    "        md_train_test = mahalanobis(mean_org, mean_test, inv_cov)\n",
    "        md_smote_test = mahalanobis(mean_smote, mean_test, inv_cov)\n",
    "\n",
    "        print(\n",
    "            f\" - {i+1}/{df_available.shape[0]}: {ispecies:>20} \\t| {iseed}\"\n",
    "            f\" | Train–SMOTE: {md_train_smote:.3f}\"\n",
    "            f\" | Train–Test: {md_train_test:.3f}\"\n",
    "            f\" | SMOTE–Test: {md_smote_test:.3f}\"\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"species\": ispecies,\n",
    "                \"seed\": iseed,\n",
    "                \"md_train_smote\": md_train_smote,\n",
    "                \"md_train_test\": md_train_test,\n",
    "                \"md_smote_test\": md_smote_test,\n",
    "                \"file\": ifile,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # if i == 52:\n",
    "        # raise  # optional debugging break\n",
    "\n",
    "    # Create DataFrame and save\n",
    "    df_md = pd.DataFrame(results)\n",
    "    display(df_md.head())\n",
    "    df_md.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if files are available locally or on external drive\n",
    "test_file = df_md.file[0]\n",
    "if not os.path.isfile(test_file):\n",
    "    test_file = df_md.file[0].replace(\"./model_runs/\", \"/Volumes/SAMSUNG 1TB/\")\n",
    "    if not os.path.isfile(test_file):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Test file {test_file} not found. Please check the path.\"\n",
    "        )\n",
    "    df_md.file = df_md.file.str.replace(\"./model_runs/\", \"/Volumes/SAMSUNG 1TB/\")\n",
    "\n",
    "# Reshape for seaborn plot\n",
    "df_long = df_md.melt(\n",
    "    id_vars=\"species\",\n",
    "    value_vars=[\"md_train_smote\", \"md_train_test\", \"md_smote_test\"],\n",
    "    var_name=\"Distance Type\",\n",
    "    value_name=\"Mahalanobis Distance\",\n",
    ")\n",
    "\n",
    "# Shorten species names for better readability\n",
    "df_long[\"species\"] = df_long[\"species\"].apply(shorten_species_names)\n",
    "species_order = df_ntrees.species.tolist()\n",
    "species_order = [shorten_species_names(s) for s in species_order]\n",
    "\n",
    "# Remove Frangula alnus because massive outlier\n",
    "# df_long = df_long.query(\"species != 'Frangula alnus'\").reset_index(drop=True)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10), sharey=True)\n",
    "distance_types = [\"md_train_smote\", \"md_train_test\", \"md_smote_test\"]\n",
    "titles = [\"Train vs SMOTE\", \"Train vs Test\", \"SMOTE vs Test\"]\n",
    "\n",
    "for ax, dtype, title in zip(axes, distance_types, titles):\n",
    "    subset = df_long[df_long[\"Distance Type\"] == dtype]\n",
    "    # Add dotted grid lines\n",
    "    ax.grid(axis=\"y\", linestyle=\"dotted\", alpha=0.5)\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=subset,\n",
    "        y=\"species\",\n",
    "        x=\"Mahalanobis Distance\",\n",
    "        order=species_order,\n",
    "        ax=ax,\n",
    "        orient=\"h\",\n",
    "    )\n",
    "    ax.set_title(title, fontsize=14, weight=\"bold\")\n",
    "    ax.set_xlim(0, 5)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\n",
    "            \"Species (sorted by decreasing number of trees per species)\",\n",
    "            weight=\"bold\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "    # if ax != axes[0]:\n",
    "    # ax.set_ylabel(\"\")\n",
    "    # ax.set_yticks([])\n",
    "    # ax.set_xlabel(\"Distance\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(\n",
    "    f\"{dir_patterns}/smotek_md_boxplots.png\",\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best and worst model runs by Mahalanobis distance (do not use Fangula alnus because it is an outlier with very high distances)\n",
    "best_run = df_md.sort_values(\"md_train_smote\").reset_index(drop=True).iloc[0]\n",
    "worst_run = (\n",
    "    df_md.query(\"species != 'Frangula alnus'\")\n",
    "    .sort_values(\"md_smote_test\")\n",
    "    .reset_index(drop=True)\n",
    "    .iloc[-1]\n",
    ")\n",
    "selected_runs = [best_run, worst_run]\n",
    "\n",
    "# Print overview\n",
    "display(selected_runs)\n",
    "\n",
    "# Visualize distributions of predictors in train, SMOTE, and test sets\n",
    "# Customizable toggle\n",
    "include_histogram = False\n",
    "letters = [ch for ch in string.ascii_uppercase if ch != \"J\"]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=4,\n",
    "    ncols=6,\n",
    "    figsize=(22, 14),\n",
    "    gridspec_kw={\"hspace\": 0.5},\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "for run_idx, row in enumerate(selected_runs):\n",
    "    # Reload data\n",
    "    base_path = row[\"file\"].replace(\"final_model_performance.csv\", \"final_model/\")\n",
    "    xtrain = pd.read_csv(base_path + \"X_train.csv\").drop(\n",
    "        columns=[\"Unnamed: 0\"], errors=\"ignore\"\n",
    "    )\n",
    "    ytrain = pd.read_csv(base_path + \"y_train.csv\").drop(\n",
    "        columns=[\"Unnamed: 0\"], errors=\"ignore\"\n",
    "    )\n",
    "    xtest = pd.read_csv(base_path + \"X_test.csv\").drop(\n",
    "        columns=[\"Unnamed: 0\"], errors=\"ignore\"\n",
    "    )\n",
    "    ytest = pd.read_csv(base_path + \"y_test.csv\").drop(\n",
    "        columns=[\"Unnamed: 0\"], errors=\"ignore\"\n",
    "    )\n",
    "    treeid = pd.read_csv(base_path.replace(\"final_model/\", \"treeid/X_train_treeid.csv\"))\n",
    "\n",
    "    # Split original and SMOTE training data\n",
    "    test = pd.concat([ytest, xtest], axis=1)\n",
    "    train = pd.concat([ytrain, xtrain], axis=1)\n",
    "    train_org = train.iloc[: treeid.shape[0]]\n",
    "    train_smote = train.iloc[treeid.shape[0] :]\n",
    "\n",
    "    # Extract subsets\n",
    "    train_org = train_org.query(\"target == 1\").drop(columns=[\"target\"])\n",
    "    train_smote = train_smote.query(\"target == 1\").drop(columns=[\"target\"])\n",
    "    test = test.query(\"target == 1\").drop(columns=[\"target\"])\n",
    "\n",
    "    predictors = train_org.columns[:11]\n",
    "\n",
    "    # PCA\n",
    "    scaler = StandardScaler()\n",
    "    pooled = pd.concat(\n",
    "        [train_org[predictors], train_smote[predictors], test[predictors]],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    scaled = scaler.fit_transform(pooled)\n",
    "    pca = PCA(n_components=2)\n",
    "    pcs = pca.fit_transform(scaled)\n",
    "\n",
    "    pca_df = pd.DataFrame(pcs, columns=[\"PC1\", \"PC2\"])\n",
    "    pca_df[\"source\"] = (\n",
    "        [\"Train Data\"] * len(train_org)\n",
    "        + [\"Train SMOTE\"] * len(train_smote)\n",
    "        + [\"Test Data\"] * len(test)\n",
    "    )\n",
    "\n",
    "    # Histogram + KDE plots\n",
    "    for i, var in enumerate(predictors):\n",
    "        ax = axes[run_idx * 12 + i]\n",
    "\n",
    "        if include_histogram:\n",
    "            bins = np.histogram_bin_edges(train_org[var].dropna(), bins=\"auto\")\n",
    "            sns.histplot(\n",
    "                train_org[var],\n",
    "                bins=bins,\n",
    "                kde=True,\n",
    "                color=\"black\",\n",
    "                stat=\"density\",\n",
    "                ax=ax,\n",
    "                element=\"step\",\n",
    "                linewidth=1,\n",
    "                label=\"Train Data\",\n",
    "            )\n",
    "            sns.histplot(\n",
    "                train_smote[var],\n",
    "                bins=bins,\n",
    "                kde=True,\n",
    "                color=\"red\",\n",
    "                stat=\"density\",\n",
    "                ax=ax,\n",
    "                element=\"step\",\n",
    "                linewidth=1,\n",
    "                label=\"Train SMOTE\",\n",
    "            )\n",
    "            sns.histplot(\n",
    "                test[var],\n",
    "                bins=bins,\n",
    "                kde=True,\n",
    "                color=\"dodgerblue\",\n",
    "                stat=\"density\",\n",
    "                ax=ax,\n",
    "                element=\"step\",\n",
    "                linewidth=1,\n",
    "                label=\"Test Data\",\n",
    "            )\n",
    "        else:\n",
    "            sns.kdeplot(\n",
    "                train_org[var], color=\"black\", linewidth=1, ax=ax, label=\"Train Data\"\n",
    "            )\n",
    "            sns.kdeplot(\n",
    "                train_smote[var],\n",
    "                color=\"red\",\n",
    "                linewidth=1,\n",
    "                # linestyle=\"--\",\n",
    "                ax=ax,\n",
    "                label=\"Train SMOTE\",\n",
    "            )\n",
    "            sns.kdeplot(\n",
    "                xtest[var],\n",
    "                color=\"dodgerblue\",\n",
    "                linewidth=1,\n",
    "                # linestyle=\":\",\n",
    "                ax=ax,\n",
    "                label=\"Test Data\",\n",
    "            )\n",
    "\n",
    "        # Axis settings\n",
    "        ax.set_xlabel(var, fontsize=12)\n",
    "        ax.set_ylabel(\"Density\", fontsize=12)\n",
    "        if i % 6 != 0:\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"\")  # disable subplot title\n",
    "\n",
    "        if i == 0 and run_idx == 0:\n",
    "            ax.legend(fontsize=12, frameon=False)\n",
    "\n",
    "        # Add subplot letter\n",
    "        ax.text(\n",
    "            0.02,\n",
    "            0.98,\n",
    "            letters[run_idx * 12 + i],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "            ha=\"left\",\n",
    "        )\n",
    "\n",
    "    # PCA subplot (12th position)\n",
    "    ax = axes[run_idx * 12 + 11]\n",
    "    sns.kdeplot(\n",
    "        data=pca_df[pca_df[\"source\"] == \"Train Data\"],\n",
    "        x=\"PC1\",\n",
    "        y=\"PC2\",\n",
    "        levels=3,\n",
    "        color=\"black\",\n",
    "        linewidths=1,\n",
    "        label=\"Train Data\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=pca_df[pca_df[\"source\"] == \"Train SMOTE\"],\n",
    "        x=\"PC1\",\n",
    "        y=\"PC2\",\n",
    "        levels=3,\n",
    "        color=\"red\",\n",
    "        linewidths=1,\n",
    "        # linestyle=\"--\",\n",
    "        label=\"Train SMOTE\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=pca_df[pca_df[\"source\"] == \"Test Data\"],\n",
    "        x=\"PC1\",\n",
    "        y=\"PC2\",\n",
    "        levels=3,\n",
    "        color=\"dodgerblue\",\n",
    "        linewidths=1,\n",
    "        # linestyle=\":\",\n",
    "        label=\"Test Data\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    var_exp = pca.explained_variance_ratio_ * 100\n",
    "    ax.set_xlabel(f\"PC1 ({var_exp[0]:.0f}%)\", fontsize=12)\n",
    "    ax.set_ylabel(f\"PC2 ({var_exp[1]:.0f}%)\", fontsize=12, labelpad=-5)\n",
    "\n",
    "    # Add subplot letter for PCA\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        letters[run_idx * 12 + 11],\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "        ha=\"left\",\n",
    "    )\n",
    "\n",
    "    # Block title for each 2-row section\n",
    "    fig.text(\n",
    "        0.11,\n",
    "        0.9 if run_idx == 0 else 0.475,\n",
    "        f\"{row['species']} | seed={row['seed']} | MD Train-SMOTE  = {row['md_train_smote']:.2f} | MD Train-Test = {row['md_train_test']:.2f} | MD SMOTE-Test = {row['md_smote_test']:.2f}\",\n",
    "        # ha=\"left\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Remove top and right spines for all subplots\n",
    "for ax in axes:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    f\"{dir_patterns}/smotek_md_distr_pca.png\",\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE: N to keep\n",
    "\n",
    "- Analysis of influence of how many variables per category are kept before single best feature is isolated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Comparing performances of final models across different analyses\n",
    "# Load and compare final model performances\n",
    "files = glob.glob(\n",
    "    \"./extra_runs/rfe*/_analysis/_top9_species/*/pattern_analysis/by_mk/roc_0.6-min_group_share_0.6/tables/model_performance_summary.csv\"\n",
    ")\n",
    "\n",
    "analysis_type = [s.split(\"/\")[2] for s in files]\n",
    "\n",
    "files = pd.DataFrame(\n",
    "    {\n",
    "        \"file\": files,\n",
    "        \"analysis_type\": analysis_type,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Keep only most recent file for each analysis type\n",
    "files = files.sort_values([\"analysis_type\", \"file\"])\n",
    "files = files.groupby(\"analysis_type\").last().reset_index()\n",
    "\n",
    "df = []\n",
    "for i, row in files.iterrows():\n",
    "    tmp = pd.read_csv(row[\"file\"])\n",
    "    tmp = tmp.query(\"Species in ['Mean', 'SD']\")\n",
    "    tmp[\"analysis_type\"] = row[\"analysis_type\"]\n",
    "    tmp = tmp.rename(\n",
    "        columns={\n",
    "            \"Train Roc Auc\": \"Train ROC-AUC\",\n",
    "            \"Test Roc Auc\": \"Test ROC-AUC\",\n",
    "            \"Train Pr Auc\": \"Train PR-AUC\",\n",
    "            \"Test Pr Auc\": \"Test PR-AUC\",\n",
    "        }\n",
    "    )\n",
    "    df.append(tmp)\n",
    "\n",
    "df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "# Filter only 'Mean' rows\n",
    "df_mean = df[df[\"Species\"] == \"Mean\"]\n",
    "\n",
    "# Metrics and styles\n",
    "metrics = [\"ROC-AUC\", \"PR-AUC\", \"Precision\", \"Recall\", \"F1\"]\n",
    "linestyles = {\"Train\": \"--\", \"Test\": \"-\"}\n",
    "colors = {\n",
    "    \"ROC-AUC\": \"tab:blue\",\n",
    "    \"PR-AUC\": \"tab:orange\",\n",
    "    \"Precision\": \"tab:green\",\n",
    "    \"Recall\": \"tab:red\",\n",
    "    \"F1\": \"tab:purple\",\n",
    "}\n",
    "markers = {\n",
    "    \"ROC-AUC\": \"o\",\n",
    "    \"PR-AUC\": \"s\",\n",
    "    \"Precision\": \"D\",\n",
    "    \"Recall\": \"^\",\n",
    "    \"F1\": \"v\",\n",
    "}\n",
    "\n",
    "# Melt to long format\n",
    "train_cols = [f\"Train {m}\" for m in metrics]\n",
    "train_cols = [f\"Train {m}\" for m in metrics]\n",
    "test_cols = [f\"Test {m}\" for m in metrics]\n",
    "\n",
    "df_train = df_mean[[\"analysis_type\"] + train_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"Score\"\n",
    ")\n",
    "df_train[\"Set\"] = \"Train\"\n",
    "df_train[\"Metric\"] = df_train[\"Metric\"].str.replace(\"Train \", \"\")\n",
    "\n",
    "df_test = df_mean[[\"analysis_type\"] + test_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"Score\"\n",
    ")\n",
    "df_test[\"Set\"] = \"Test\"\n",
    "df_test[\"Metric\"] = df_test[\"Metric\"].str.replace(\"Test \", \"\")\n",
    "\n",
    "df_long = pd.concat([df_train, df_test])\n",
    "\n",
    "# Melt SD data\n",
    "df_sd = df[df[\"Species\"] == \"SD\"]\n",
    "\n",
    "df_train_sd = df_sd[[\"analysis_type\"] + train_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"SD\"\n",
    ")\n",
    "df_train_sd[\"Set\"] = \"Train\"\n",
    "df_train_sd[\"Metric\"] = df_train_sd[\"Metric\"].str.replace(\"Train \", \"\")\n",
    "\n",
    "df_test_sd = df_sd[[\"analysis_type\"] + test_cols].melt(\n",
    "    id_vars=\"analysis_type\", var_name=\"Metric\", value_name=\"SD\"\n",
    ")\n",
    "df_test_sd[\"Set\"] = \"Test\"\n",
    "df_test_sd[\"Metric\"] = df_test_sd[\"Metric\"].str.replace(\"Test \", \"\")\n",
    "\n",
    "df_sd_long = pd.concat([df_train_sd, df_test_sd])\n",
    "\n",
    "# Merge SDs with main data\n",
    "df_long = pd.merge(\n",
    "    df_long, df_sd_long, on=[\"analysis_type\", \"Metric\", \"Set\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Optional: numeric x positions (if needed for mapping)\n",
    "analysis_types = [\"rfe_nkeep_2\", \"rfe_nkeep_4\", \"rfe_nkeep_6\"]\n",
    "custom_labels = [\"k = 2\", \"k = 4\", \"k = 6\"]\n",
    "x_pos_map = dict(zip(analysis_types, range(len(analysis_types))))\n",
    "df_long[\"x\"] = df_long[\"analysis_type\"].map(x_pos_map)\n",
    "\n",
    "# Plot with error bars\n",
    "fig, ax = plt.subplots(figsize=(6.5, 5))\n",
    "for metric in metrics:\n",
    "    for set_type in [\"Train\", \"Test\"]:\n",
    "        subset = df_long[(df_long[\"Metric\"] == metric) & (df_long[\"Set\"] == set_type)]\n",
    "        ax.plot(\n",
    "            subset[\"analysis_type\"],\n",
    "            subset[\"Score\"],\n",
    "            color=colors[metric],\n",
    "            linestyle=linestyles[set_type],\n",
    "            marker=markers[metric],\n",
    "            label=f\"{metric} ({set_type})\",  # Not used for legend directly\n",
    "            markersize=8,\n",
    "        )\n",
    "        # With Error Bars\n",
    "        # ax.errorbar(\n",
    "        #     subset[\"x\"],\n",
    "        #     subset[\"Score\"],\n",
    "        #     yerr=subset[\"SD\"],\n",
    "        #     color=colors[metric],\n",
    "        #     linestyle=linestyles[set_type],\n",
    "        #     marker=markers[metric],\n",
    "        #     markersize=8,\n",
    "        #     capsize=4,\n",
    "        # )\n",
    "\n",
    "# X-ticks and labels\n",
    "ax.set_xticks(range(len(custom_labels)))\n",
    "ax.set_xticklabels(custom_labels, fontsize=12)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n",
    "\n",
    "# Labels and style\n",
    "# ax.set_title(\"Train vs Test Performance by Analysis Type\")\n",
    "ax.set_xlabel(\n",
    "    \"Number of features kept per category before selecting single best feature\",\n",
    "    fontsize=14,\n",
    ")\n",
    "ax.set_ylabel(\n",
    "    \"Score\",\n",
    "    fontsize=14,\n",
    ")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Custom legend\n",
    "metric_handles = [\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        color=colors[m],\n",
    "        marker=markers[m],\n",
    "        linestyle=\"\",\n",
    "        markersize=8,\n",
    "        label=m,\n",
    "    )\n",
    "    for m in metrics\n",
    "]\n",
    "\n",
    "set_handles = [\n",
    "    Line2D([0], [0], color=\"gray\", linestyle=ls, lw=2, label=st)\n",
    "    for st, ls in linestyles.items()\n",
    "]\n",
    "\n",
    "ax.legend(\n",
    "    handles=metric_handles + set_handles,\n",
    "    title=\"\",\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1.01, 0.5),\n",
    "    frameon=False,\n",
    "    borderaxespad=0,\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "plt.savefig(f\"{dir_patterns}/model_performance_rfe_n.png\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
