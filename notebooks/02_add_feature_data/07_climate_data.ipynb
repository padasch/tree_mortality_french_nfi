{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digitalis v3 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EPSG:2154 Coordinates\n",
    "coords = get_final_nfi_coordinates(\n",
    "    epsg=\"2154\", geojson_or_csv=\"csv\", noisy_or_corrected=\"noisy\"\n",
    ")\n",
    "display(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of available variables\n",
    "\n",
    "(1961 to 2020) x (12 Months + 5 Temporal Aggregates + Historical Climate + Contemporaty Climate)\n",
    "\n",
    "= 60years x 17 vars\n",
    "\n",
    "= 1044 max variables per dataset\n",
    "\n",
    "**Available Vars:**\n",
    "\n",
    "- bhc\n",
    "- deth\n",
    "- etp\n",
    "- prec\n",
    "- radneb\n",
    "- ruth\n",
    "- tmax\n",
    "- tmin\n",
    "- tmoy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all files are present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all files per year are present\n",
    "\n",
    "for dataset in [\n",
    "    \"tmoy\",\n",
    "    \"tmin\",\n",
    "    \"tmax\",\n",
    "    \"ruth\",\n",
    "    # \"deth\",\n",
    "    # \"radneb\",\n",
    "    # \"etp\",\n",
    "    # \"bhc\",\n",
    "    # \"prec\",\n",
    "]:\n",
    "\n",
    "    files = glob.glob(f\"../../data/raw/digitalis_v3/1km/{dataset}/*.tif\")\n",
    "    files = pd.DataFrame(files, columns=[\"files\"])\n",
    "    files[\"year\"] = files[\"files\"].apply(lambda x: int(x.split(\"_\")[-2].split(\"_\")[0]))\n",
    "\n",
    "    # Count files per year\n",
    "    print(\"\\n\")\n",
    "    print(\"-------------------\")\n",
    "    print(f\"⭐ {dataset}\")\n",
    "    print(\n",
    "        f\" - Total number of years: {files.year.value_counts().sort_index().__len__()}\\n - Total number of files: {files.year.value_counts().sort_index().sum()}\"\n",
    "    )\n",
    "    # Print groups with less than 17 files\n",
    "    print(\" - Dataset-Years with less than 17 files:\\n\")\n",
    "    print(\n",
    "        files.groupby(\"year\").count()[\"files\"][\n",
    "            files.groupby(\"year\").count()[\"files\"] != 17\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run zonal mean extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚨: If running all sites, it takes VERY long! Smaller coord dfs to sample are faster, so I could split it by year or so. But this requires more code to merge all sites together in the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data range\n",
    "data_range = \"only_2023\"  # all - until_2022 = only_2023\n",
    "\n",
    "# Get buffered coordinates\n",
    "buffer = gpd.read_file(\"../../data/final/nfi/700m_buffer_epsg2154.geojson\")\n",
    "\n",
    "# Filter first_year depending on data_range\n",
    "if data_range == \"until_2022\":\n",
    "    buffer = buffer[buffer[\"first_year\"] < 2018]\n",
    "elif data_range == \"only_2023\":\n",
    "    buffer = buffer[buffer[\"first_year\"] == 2018]\n",
    "elif data_range == \"all\":\n",
    "    data_range = \"all/direct\"\n",
    "else:\n",
    "    raise ValueError(\"data_range not recognized\")\n",
    "\n",
    "# Get files\n",
    "files = glob.glob(\"/Volumes/SAMSUNG 1TB/digitalis_v3/raw/1km/*/*.tif\")\n",
    "\n",
    "# ! Debug: I AM ONLY EXTRACTING MONTHLY VALUES, NO AGGREGATED VALUES\n",
    "# Aggregated values are: 6190, 9120, 6120, hi, pr, et, aut, 13 (or ann?)\n",
    "files = [\n",
    "    f\n",
    "    for f in files\n",
    "    if \"_1.tif\" in f\n",
    "    or \"_2.tif\" in f\n",
    "    or \"_3.tif\" in f\n",
    "    or \"_4.tif\" in f\n",
    "    or \"_5.tif\" in f\n",
    "    or \"_6.tif\" in f\n",
    "    or \"_7.tif\" in f\n",
    "    or \"_8.tif\" in f\n",
    "    or \"_9.tif\" in f\n",
    "    or \"_10.tif\" in f\n",
    "    or \"_11.tif\" in f\n",
    "    or \"_12.tif\" in f\n",
    "]\n",
    "\n",
    "# Remove annual aggregated files\n",
    "files = [f for f in files if \"6190\" not in f]\n",
    "files = [f for f in files if \"9120\" not in f]\n",
    "files = [f for f in files if \"6120\" not in f]\n",
    "\n",
    "# ! Debug: Remove bhc, deth, ruth, radneb for now\n",
    "files = [f for f in files if \"bhc\" not in f]\n",
    "files = [f for f in files if \"deth\" not in f]\n",
    "files = [f for f in files if \"ruth\" not in f]\n",
    "files = [f for f in files if \"radneb\" not in f]\n",
    "\n",
    "# Turn into dataframe\n",
    "df_files = pd.DataFrame(files, columns=[\"path\"])\n",
    "df_files[\"filename\"] = df_files[\"path\"].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
    "df_files[\"year\"] = df_files[\"filename\"].apply(lambda x: int(x.split(\"_\")[1]))\n",
    "df_files[\"dataset\"] = df_files[\"filename\"].apply(lambda x: x.split(\"_\")[0])\n",
    "df_files = df_files.sort_values(\"filename\")\n",
    "\n",
    "# Display files\n",
    "display(df_files.dataset.value_counts())\n",
    "display(df_files.head())\n",
    "list_files = split_df_into_list_of_group_or_ns(df_files, 10)\n",
    "\n",
    "run_mp(\n",
    "    extract_zonal_mean,\n",
    "    list_files,\n",
    "    buffer=buffer,\n",
    "    num_cores=10,\n",
    "    force_run=True,\n",
    "    save_dir=f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{data_range}/zonal_mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn raster extraction into long format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\n",
    "    # \"tmoy\",\n",
    "    # \"tmin\",\n",
    "    # \"tmax\",\n",
    "    # \"prec\",\n",
    "    \"etp\",\n",
    "    # \"bhc\",\n",
    "    # \"ruth\",\n",
    "    # \"deth\",\n",
    "    # \"radneb\",\n",
    "]:\n",
    "\n",
    "    # Load all files\n",
    "    files = glob.glob(\n",
    "        f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{data_range}/zonal_mean/*.feather\"\n",
    "    )\n",
    "\n",
    "    # Filter for dataset\n",
    "    files = [f for f in files if dataset in f]\n",
    "\n",
    "    if len(files) != 720:\n",
    "        print(\n",
    "            f\" 🚨 The number of files for '{dataset}' \\tis not 720 (60 years * 12 months) but: {len(files)}\"\n",
    "        )\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "    else:\n",
    "        print(\n",
    "            f\" ✅ The number of files for '{dataset}' \\tis 720 (60 years * 12 months)\"\n",
    "        )\n",
    "\n",
    "    # Loop through all files and attach to them by idp and first year\n",
    "    all_files = []\n",
    "    for file in files:\n",
    "        # Load file\n",
    "        df = pd.read_feather(file)\n",
    "        # Get filename\n",
    "        filename = df.columns[-1]\n",
    "        # Extract dataset and month\n",
    "        dataset = filename.split(\"_\")[0]\n",
    "        df[\"month\"] = filename.split(\"_\")[-1]\n",
    "        df[\"year\"] = filename.split(\"_\")[-2]\n",
    "        # Rename column to value\n",
    "        df = df.rename(columns={filename: \"value\"})\n",
    "        # Attach to list\n",
    "        all_files.append(df.reset_index(drop=True))\n",
    "\n",
    "    # Concatenate all files by row\n",
    "    df_all = (\n",
    "        pd.concat(all_files, axis=0)\n",
    "        .sort_values([\"year\", \"month\", \"idp\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df_all\n",
    "\n",
    "    # Save to feather\n",
    "    df_all.to_feather(\n",
    "        f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{data_range}/zonal_mean_long/{dataset}.feather\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge 2023 data into existing one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\n",
    "    \"tmoy\",\n",
    "    \"tmin\",\n",
    "    \"tmax\",\n",
    "    \"prec\",\n",
    "    \"etp\",\n",
    "    # \"bhc\",\n",
    "    # \"ruth\",\n",
    "    # \"deth\",\n",
    "    # \"radneb\",\n",
    "]:\n",
    "\n",
    "    # Verbose\n",
    "    display(f\" --- Dataset: {dataset} ---\")\n",
    "\n",
    "    # Set filename\n",
    "    old_file = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/until_2022/zonal_mean_long_{dataset}.feather\"\n",
    "    new_files = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/only_2023/zonal_mean_long/{dataset}.feather\"\n",
    "\n",
    "    # Check if files exists\n",
    "    if not os.path.exists(old_file):\n",
    "        print(f\" 🚨 File '{old_file}' does not exist\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\" ✅ File '{old_file}' exists\")\n",
    "\n",
    "    if not os.path.exists(new_files):\n",
    "        print(f\" 🚨 File '{new_files}' does not exist\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\" ✅ File '{new_files}' exists\")\n",
    "\n",
    "    # Load data\n",
    "    old_data = pd.read_feather(old_file)\n",
    "    new_data = pd.read_feather(new_files)\n",
    "\n",
    "    # Make sure none of these aggregations are present\n",
    "    old_data = old_data.query(\"year not in ['13', '6190', '9120', '6120']\")\n",
    "    new_data = new_data.query(\"year not in ['13', '6190', '9120', '6120']\")\n",
    "\n",
    "    # Combine data\n",
    "    df_all = pd.concat([old_data, new_data], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # ! Quality Checks\n",
    "    save_to_dump = False\n",
    "    # Print shape\n",
    "    print(f\" - Shape of old data: {old_data.shape}\")\n",
    "    print(f\" - Shape of new data: {new_data.shape}\")\n",
    "    print(f\" - Shape of combined data: {df_all.shape}\")\n",
    "\n",
    "    # Print sites\n",
    "    old_sites = old_data[\"idp\"].nunique()\n",
    "    new_sites = new_data[\"idp\"].nunique()\n",
    "    combined_sites = df_all[\"idp\"].nunique()\n",
    "    print(f\" - Number of sites in old data: {old_sites}\")\n",
    "    print(f\" - Number of sites in new data: {new_sites}\")\n",
    "    print(f\" - Number of sites in combined data: {combined_sites}\")\n",
    "\n",
    "    # To ensure that all year-months and no aggregates are present\n",
    "    # we will check the number of unique year-month combinations\n",
    "    actual_combinations = df_all[[\"year\", \"month\"]].drop_duplicates().shape[0]\n",
    "\n",
    "    if dataset == \"etp\":\n",
    "        expected_combinations = 719  # ETP is missing one month of data\n",
    "    elif dataset == \"radneb\":\n",
    "        expected_combinations = 717  # Radneb is missing 3 months of data\n",
    "    else:\n",
    "        expected_combinations = 720  # 60*12 = 720\n",
    "\n",
    "    if actual_combinations != expected_combinations:\n",
    "        save_to_dump = True\n",
    "        print(\n",
    "            f\" 🚨 Expected year-month combinations = {expected_combinations} but actual combinations = {actual_combinations}. Saving data to dump folder\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\" ✅ Expected and actual year-month combinations are the same: {actual_combinations} = {expected_combinations}\"\n",
    "        )\n",
    "\n",
    "    # To ensure all data is present, we calculate the expectd number of idp-year-month combinations. The df should have that many entries\n",
    "    expected = df_all.idp.nunique() * df_all.year.nunique() * df_all.month.nunique()\n",
    "    actual = df_all.shape[0]\n",
    "\n",
    "    if actual != expected:\n",
    "        save_to_dump = True\n",
    "        print(\n",
    "            f\" 🚨 Expected number of entries = {expected} but actual number of entries = {actual}. Saving data to dump folder\"\n",
    "        )\n",
    "\n",
    "    if save_to_dump:\n",
    "        final_file = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/all/merged/dump/{dataset}.feather\"\n",
    "\n",
    "    else:\n",
    "        final_file = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/all/merged/{dataset}.feather\"\n",
    "\n",
    "    # Save to feather\n",
    "    df_all.to_feather(final_file)\n",
    "\n",
    "    # Verbose\n",
    "    print(f\" 💾 Saved to: {final_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime.success(\"All done!\")\n",
    "print(\n",
    "    \"Reminder: Next, do some quality checks on the data and then calcualte the derivatives!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Final Derivatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚨 Set source, whether merged or directly extracted data\n",
    "source = \"merged\"  # merged or direct\n",
    "\n",
    "# Fix source path\n",
    "source = f\"all/{source}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\n",
    "    \"tmoy\",\n",
    "    \"tmin\",\n",
    "    \"tmax\",\n",
    "    \"prec\",\n",
    "    \"etp\",\n",
    "    \"bhc\",\n",
    "    \"ruth\",\n",
    "    \"deth\",\n",
    "    \"radneb\",\n",
    "]:\n",
    "\n",
    "    df = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/{dataset}.feather\"\n",
    "    if not os.path.exists(df):\n",
    "        print(f\" 🚨 File for {dataset} is missing\")\n",
    "    else:\n",
    "        df = pd.read_feather(df)\n",
    "        print(\n",
    "            f\"The shape of {dataset} is {df.shape} and the value column has {df['value'].isna().sum() / df.shape[0]:.2f}% missing values.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPEI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_etp = (\n",
    "    pd.read_feather(\n",
    "        f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/etp.feather\"\n",
    "    )\n",
    "    .rename(columns={\"value\": \"etp\"})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_prec = (\n",
    "    pd.read_feather(\n",
    "        f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/prec.feather\"\n",
    "    )\n",
    "    .rename(columns={\"value\": \"prec\"})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Merge data\n",
    "spei_data = pd.merge(\n",
    "    df_etp, df_prec, on=[\"idp\", \"first_year\", \"year\", \"month\"], how=\"outer\"\n",
    ")\n",
    "\n",
    "# Clean dates\n",
    "spei_data[\"date\"] = spei_data[\"year\"] + \"-\" + spei_data[\"month\"]\n",
    "spei_data[\"date\"] = pd.to_datetime(spei_data[\"date\"], format=\"%Y-%m\")\n",
    "spei_data = spei_data.drop(columns=[\"year\", \"month\"])\n",
    "spei_data = spei_data.sort_values(by=[\"idp\", \"date\"])\n",
    "spei_data = spei_data.reset_index(drop=True)\n",
    "\n",
    "# Saving to csv because of issues to load feather files in R... takes ~40s\n",
    "spei_data.to_csv(\n",
    "    f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/data_to_calculate_spei.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "# Display\n",
    "spei_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚨 FINISH SPEI CALCULATION IN R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature and Precipitation Anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\n",
    "    \"tmoy\",\n",
    "    \"tmin\",\n",
    "    \"tmax\",\n",
    "    # \"prec\",\n",
    "    # \"etp\",\n",
    "]:\n",
    "    print(f\"\\n--- Working on {dataset} ---\")\n",
    "    df = get_anomaly_metrics_loop_mp(\n",
    "        dataset, years_before_second_visit=7, source=source\n",
    "    )\n",
    "    df.to_feather(\n",
    "        f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/anomalies/digitalis_{dataset}.feather\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp. Anomaly Extraction Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "dataset = \"tmoy\"  # tmoy or spei\n",
    "source = \"all/merged\"  # Pick source\n",
    "iseed = 1  # Set seed\n",
    "myseason = \"JJA\"  # Set season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/{dataset}.feather\"\n",
    "ex = pd.read_feather(file)\n",
    "\n",
    "# Pick example site\n",
    "unique_sites = ex[\"idp\"].unique()\n",
    "example_site = pd.Series(unique_sites).sample(1, random_state=iseed).values[0]\n",
    "\n",
    "# ! Filter for example site\n",
    "# ex = ex.query(f\"idp == {example_site}\").copy()\n",
    "\n",
    "# ! Take the mean across all sites\n",
    "ex = ex.groupby([\"year\", \"month\"]).mean().reset_index()\n",
    "\n",
    "# Clean data\n",
    "ex[\"value\"] = ex[\"value\"] / 10\n",
    "ex = ex[pd.to_numeric(ex[\"month\"], errors=\"coerce\").notna()]\n",
    "ex = (\n",
    "    ex.query(\"month not in ['au', 'et', 'hi', 'pr', '13']\")\n",
    "    .assign(\n",
    "        year=lambda x: pd.to_numeric(x[\"year\"]),\n",
    "        month=lambda x: pd.to_numeric(x[\"month\"]).apply(lambda m: str(m).zfill(2)),\n",
    "        date=lambda x: x[\"year\"].astype(str) + \"-\" + x[\"month\"].astype(str) + \"-01\",\n",
    "        month_count=lambda x: ex[\"month\"].astype(int)\n",
    "        * (ex.year.astype(int) - 1960)\n",
    "        / 12,\n",
    "    )\n",
    "    .assign(date=lambda x: pd.to_datetime(x[\"date\"], errors=\"coerce\"))\n",
    "    .dropna()\n",
    "    .loc[lambda x: x[\"year\"].between(1961, 2020)]\n",
    ")\n",
    "ex[\"date\"] = pd.to_datetime(ex[\"date\"])\n",
    "ex[\"month\"] = ex[\"month\"].astype(int)\n",
    "\n",
    "ex = ex.sort_values(by=[\"date\"])\n",
    "\n",
    "# Change month to season string\n",
    "ex[\"month\"] = ex[\"month\"].map(\n",
    "    {\n",
    "        1: \"DJF\",\n",
    "        2: \"DJF\",\n",
    "        3: \"MAM\",\n",
    "        4: \"MAM\",\n",
    "        5: \"MAM\",\n",
    "        6: \"JJA\",\n",
    "        7: \"JJA\",\n",
    "        8: \"JJA\",\n",
    "        9: \"SON\",\n",
    "        10: \"SON\",\n",
    "        11: \"SON\",\n",
    "        12: \"DJF\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get data for season\n",
    "ex_season = ex.query(f\"month == '{myseason}'\")\n",
    "\n",
    "# Calculate anomalies\n",
    "ex_season_anom = ex_season.copy()\n",
    "ex_season_anom[\"value\"] = ex_season_anom[\"value\"] - ex_season_anom[\"value\"].mean()\n",
    "\n",
    "# Get data for season zoomed in\n",
    "ex_season_anom_zoomed = ex_season_anom.query(\n",
    "    \"date >= '2000-01-01' & date <= '2007-12-31'\"\n",
    ").copy()\n",
    "ex_zoomed = ex.query(\"date >= '2000-01-01' & date <= '2007-12-31'\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set season colors\n",
    "season_colors = {\n",
    "    \"DJF\": \"skyblue\",\n",
    "    \"MAM\": \"green\",\n",
    "    \"JJA\": \"yellow\",\n",
    "    \"SON\": \"orange\",\n",
    "}\n",
    "\n",
    "# fig, ax = plt.subplots(4, 1, figsize=(10, 10))\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 5))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# 1. Temperature Time Series\n",
    "# Add background for seasons\n",
    "current_season = ex[\"month\"].iloc[0]\n",
    "start_date = ex[\"date\"].iloc[0]\n",
    "\n",
    "for i in range(1, len(ex)):\n",
    "    if ex[\"month\"].iloc[i] != current_season:\n",
    "        # End of the current season\n",
    "        end_date = ex[\"date\"].iloc[i - 1]\n",
    "        ax[0].axvspan(\n",
    "            start_date, end_date, color=season_colors[current_season], alpha=0.25\n",
    "        )\n",
    "        # Start a new season span\n",
    "        current_season = ex[\"month\"].iloc[i]\n",
    "        start_date = ex[\"date\"].iloc[i]\n",
    "\n",
    "ax[0].plot(\n",
    "    ex[\"date\"],\n",
    "    ex[\"value\"],\n",
    "    label=\"Precipitation\",\n",
    "    color=\"black\",\n",
    ")\n",
    "ax[0].set_ylabel(\"Temperature (°C)\")\n",
    "ax[0].tick_params(axis=\"y\")\n",
    "ax[0].set_title(\n",
    "    f\"1.) Full Time Series of Temperature (colors = seasons)\",\n",
    "    fontweight=\"bold\",\n",
    "    loc=\"left\",\n",
    ")\n",
    "ax[0].set_ylim(-10, 30)\n",
    "\n",
    "# 2. Temperature Time Series of Season\n",
    "# Add background for seasons\n",
    "# Set all except chosen season to white\n",
    "# for season in [\"DJF\", \"MAM\", \"JJA\", \"SON\"]:\n",
    "#     if season != myseason:\n",
    "#         season_colors[season] = \"white\"\n",
    "\n",
    "current_season = ex[\"month\"].iloc[0]\n",
    "start_date = ex[\"date\"].iloc[0]\n",
    "\n",
    "for i in range(1, len(ex)):\n",
    "    if ex[\"month\"].iloc[i] != current_season:\n",
    "        # End of the current season\n",
    "        end_date = ex[\"date\"].iloc[i - 1]\n",
    "        ax[1].axvspan(\n",
    "            start_date, end_date, color=season_colors[current_season], alpha=0.25\n",
    "        )\n",
    "        # Start a new season span\n",
    "        current_season = ex[\"month\"].iloc[i]\n",
    "        start_date = ex[\"date\"].iloc[i]\n",
    "\n",
    "\n",
    "ax[1].plot(\n",
    "    ex_season[\"date\"],\n",
    "    ex_season[f\"value\"],\n",
    "    color=\"black\",\n",
    ")\n",
    "# ax[1].scatter(\n",
    "#     ex_season[\"date\"],\n",
    "#     ex_season[f\"value\"],\n",
    "#     color=\"white\",\n",
    "#     edgecolor=\"black\",\n",
    "#     label=\"Temperature\",\n",
    "# )\n",
    "ax[1].set_title(\n",
    "    f\"2.) Reduce time series to season: {myseason}\",\n",
    "    fontweight=\"bold\",\n",
    "    loc=\"left\",\n",
    ")\n",
    "ax[1].set_ylabel(\"Temperature (°C)\")\n",
    "ax[1].set_ylim(-10, 30)\n",
    "\n",
    "# 3. Temperature Time Series of Season with Anomalies\n",
    "# Add background for seasons\n",
    "current_season = ex[\"month\"].iloc[0]\n",
    "start_date = ex[\"date\"].iloc[0]\n",
    "\n",
    "for i in range(1, len(ex)):\n",
    "    if ex[\"month\"].iloc[i] != current_season:\n",
    "        # End of the current season\n",
    "        end_date = ex[\"date\"].iloc[i - 1]\n",
    "        ax[2].axvspan(\n",
    "            start_date, end_date, color=season_colors[current_season], alpha=0.25\n",
    "        )\n",
    "        # Start a new season span\n",
    "        current_season = ex[\"month\"].iloc[i]\n",
    "        start_date = ex[\"date\"].iloc[i]\n",
    "\n",
    "ax[2].axhline(0, color=\"black\", linestyle=\"-\")\n",
    "\n",
    "ax[2].plot(\n",
    "    ex_season_anom[\"date\"],\n",
    "    ex_season_anom[f\"value\"],\n",
    "    color=\"black\",\n",
    ")\n",
    "ax[2].scatter(\n",
    "    ex_season_anom[\"date\"],\n",
    "    ex_season_anom[f\"value\"],\n",
    "    color=\"white\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Temperature\",\n",
    ")\n",
    "ax[2].set_title(f\"3.) Calculate anomalies\", fontweight=\"bold\", loc=\"left\")\n",
    "ax[2].set_ylabel(\"Anomalies (°C)\")\n",
    "\n",
    "# 4. Temperature Time Series of Season Zoomed\n",
    "# Add background for seasons\n",
    "current_season = ex_zoomed[\"month\"].iloc[0]\n",
    "start_date = ex_zoomed[\"date\"].iloc[0]\n",
    "\n",
    "for i in range(1, len(ex_zoomed)):\n",
    "    if ex_zoomed[\"month\"].iloc[i] != current_season:\n",
    "        # End of the current season\n",
    "        end_date = ex_zoomed[\"date\"].iloc[i - 1]\n",
    "        ax[3].axvspan(\n",
    "            start_date, end_date, color=season_colors[current_season], alpha=0.25\n",
    "        )\n",
    "        # Start a new season span\n",
    "        current_season = ex_zoomed[\"month\"].iloc[i]\n",
    "        start_date = ex_zoomed[\"date\"].iloc[i]\n",
    "\n",
    "ax[3].axhline(0, color=\"black\", linestyle=\"-\")\n",
    "ax[3].scatter(\n",
    "    ex_season_anom_zoomed[\"date\"],\n",
    "    ex_season_anom_zoomed[f\"value\"],\n",
    "    color=\"white\",\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Temperature\",\n",
    ")\n",
    "\n",
    "# Plot min, max, and mean\n",
    "# Mark min, max and mean values\n",
    "min_temp = ex_season_anom_zoomed[\"value\"].min()\n",
    "max_temp = ex_season_anom_zoomed[\"value\"].max()\n",
    "mean_temp = ex_season_anom_zoomed[\"value\"].mean()\n",
    "\n",
    "# Add points for min and max\n",
    "ax[3].scatter(\n",
    "    ex_season_anom_zoomed.loc[ex_season_anom_zoomed[\"value\"] == min_temp, \"date\"],\n",
    "    min_temp,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    "    s=100,\n",
    "    marker=\"o\",\n",
    "    label=f\"Min: {min_temp:.2f}\",\n",
    ")\n",
    "\n",
    "ax[3].scatter(\n",
    "    ex_season_anom_zoomed.loc[ex_season_anom_zoomed[\"value\"] == max_temp, \"date\"],\n",
    "    max_temp,\n",
    "    color=\"red\",\n",
    "    edgecolor=\"black\",\n",
    "    s=100,\n",
    "    marker=\"o\",\n",
    "    label=f\"Max: {max_temp:.2f}\",\n",
    ")\n",
    "\n",
    "ax[3].axhline(\n",
    "    mean_temp,\n",
    "    color=\"brown\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean: {mean_temp:.2f}\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "ax[3].legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "ax[3].set_title(\n",
    "    f\"4.) Extract anomalies for {myseason} between 2 visits\",\n",
    "    fontweight=\"bold\",\n",
    "    loc=\"left\",\n",
    ")\n",
    "ax[3].set_ylabel(\"Anomalies (°C)\")\n",
    "\n",
    "ax[3].set_xlabel(\"Date\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(here(\"example_temperature_features.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp. trend across all sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"all/merged\"  # Pick source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "datasets = [\"tmoy\", \"tmax\", \"tmin\"]  # Pick dataset\n",
    "seasons_to_plot = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "iseed = 1  # Set seed\n",
    "\n",
    "\n",
    "# Load data\n",
    "df_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Get file\n",
    "    file = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/{dataset}.feather\"\n",
    "    # Load data\n",
    "    idf = pd.read_feather(file)\n",
    "    # Remove year 6120\n",
    "    idf = idf.query(\"year != '6120'\")\n",
    "    # Fix temperature value\n",
    "    idf[\"value\"] = idf[\"value\"] / 10\n",
    "    # Take mean across all sites\n",
    "    idf = idf.groupby([\"year\", \"month\"]).mean().reset_index()\n",
    "    # Attach dataset info\n",
    "    idf[\"tvar\"] = dataset\n",
    "    # Append to list\n",
    "    df_list.append(idf)\n",
    "\n",
    "# Concatenate\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "# Attach month based on season\n",
    "df[\"month\"] = df[\"month\"].astype(int)\n",
    "df[\"season\"] = df[\"month\"].map(\n",
    "    {\n",
    "        1: \"DJF\",\n",
    "        2: \"DJF\",\n",
    "        3: \"MAM\",\n",
    "        4: \"MAM\",\n",
    "        5: \"MAM\",\n",
    "        6: \"JJA\",\n",
    "        7: \"JJA\",\n",
    "        8: \"JJA\",\n",
    "        9: \"SON\",\n",
    "        10: \"SON\",\n",
    "        11: \"SON\",\n",
    "        12: \"DJF\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Attach date\n",
    "df[\"date\"] = pd.to_datetime(\n",
    "    df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str) + \"-01\"\n",
    ")\n",
    "# Attach numeric date\n",
    "df[\"date_num\"] = mdates.date2num(df[\"date\"])\n",
    "display(df)\n",
    "\n",
    "# Loop over every dataset and season, and calculate the anomalies trend for that season\n",
    "df_lms = []\n",
    "df_values = []\n",
    "\n",
    "for tvar in datasets:\n",
    "    for season in seasons_to_plot:\n",
    "        # Filter for dataset and season\n",
    "        idf = df.query(f\"tvar == '{tvar}' & season == '{season}'\").copy()\n",
    "        # Calculate anomalies\n",
    "        idf[\"value\"] = idf[\"value\"] - idf[\"value\"].mean()\n",
    "        # Regress anomalies on date_num\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(idf[[\"date_num\"]], idf[\"value\"])\n",
    "        df_lms.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"tvar\": tvar,\n",
    "                    \"season\": season,\n",
    "                    \"slope\": lm.coef_[0],\n",
    "                    \"intercept\": lm.intercept_,\n",
    "                },\n",
    "                index=[0],\n",
    "            )\n",
    "        )\n",
    "        # Calculate the values for the regression line\n",
    "        df_values.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"date_num\": idf[\"date_num\"],\n",
    "                    \"tvar\": tvar,\n",
    "                    \"season\": season,\n",
    "                    \"value\": lm.predict(idf[[\"date_num\"]]),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "df_lms = pd.concat(df_lms)\n",
    "df_values = pd.concat(df_values)\n",
    "df_values = df_values.replace({\"tmoy\": \"Tmean\", \"tmax\": \"Tmax\", \"tmin\": \"Tmin\"})\n",
    "df_values\n",
    "\n",
    "\n",
    "# Plot the regression lines, tvar is linetype, season is color shade\n",
    "tvar_linetypes = {\n",
    "    \"Tmean\": \"-\",\n",
    "    \"Tmax\": \"--\",\n",
    "    \"Tmin\": \"-.\",\n",
    "}\n",
    "\n",
    "season_colors = {\n",
    "    \"DJF\": \"darkblue\",\n",
    "    \"MAM\": \"green\",\n",
    "    \"JJA\": \"gold\",\n",
    "    \"SON\": \"darkorange\",\n",
    "}\n",
    "\n",
    "# Start plot\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "\n",
    "# Add zero line\n",
    "ax.axhline(0, color=\"black\", linestyle=\"-\")\n",
    "\n",
    "# Add lines\n",
    "for tvar in df_values[\"tvar\"].unique():\n",
    "    for season in seasons_to_plot:\n",
    "        idf = df_values.query(f\"tvar == '{tvar}' & season == '{season}'\")\n",
    "        ax.plot(\n",
    "            idf[\"date_num\"],\n",
    "            idf[\"value\"],\n",
    "            linestyle=tvar_linetypes[tvar],\n",
    "            color=season_colors[season],\n",
    "        )\n",
    "\n",
    "# Set the title and legend\n",
    "# ax.set_title(\n",
    "#     \"Trends for different temperature variables and seasons\", fontweight=\"bold\"\n",
    "# )\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Anomalies (°C)\")\n",
    "\n",
    "# Use equal ylim\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# Set x-axis to date format\n",
    "ax.set_xlim(-3500.0, 19000)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "# Show every 10th date to avoid overlapping\n",
    "ax.set_xticks(ax.get_xticks()[::20])\n",
    "\n",
    "# Create legend handles for tvar linetypes\n",
    "tvar_handles = [\n",
    "    mlines.Line2D([], [], color=\"black\", linestyle=linetype, label=tvar)\n",
    "    for tvar, linetype in tvar_linetypes.items()\n",
    "]\n",
    "\n",
    "# Create legend handles for season colors\n",
    "season_handles = [\n",
    "    mlines.Line2D([], [], color=color, linestyle=\"-\", label=season)\n",
    "    for season, color in season_colors.items()\n",
    "]\n",
    "\n",
    "# Combine both sets of handles\n",
    "fig.legend(\n",
    "    handles=season_handles + tvar_handles,  # Combine both sets of handles\n",
    "    loc=\"upper right\",  # Position at the center top\n",
    "    bbox_to_anchor=(1.6, 0.9),  # Move the legend below the plot\n",
    "    # loc=\"upper left\",  # Position at the center top\n",
    "    # bbox_to_anchor=(0.125, 0.9),  # with upper left and fig size 6,3\n",
    "    ncol=2,  # Arrange all items in one row\n",
    "    title=\"Season and Temperature\",  # Title for the legend\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "# Show the plot (optional, as this is just for the legend)\n",
    "# plt.tight_layout(rect=[0, 0, 0.4, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precip. trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "datasets = [\"prec\"]  # Pick dataset\n",
    "seasons_to_plot = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "iseed = 1  # Set seed\n",
    "\n",
    "\n",
    "# Load data\n",
    "df_list = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Get file\n",
    "    file = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/1km/{source}/{dataset}.feather\"\n",
    "    # Load data\n",
    "    idf = pd.read_feather(file)\n",
    "    # Remove year 6120\n",
    "    idf = idf.query(\"year != '6120'\")\n",
    "    # Fix temperature value\n",
    "    idf[\"value\"] = idf[\"value\"] / 10\n",
    "    # Take mean across all sites\n",
    "    idf = idf.groupby([\"year\", \"month\"]).mean().reset_index()\n",
    "    # Attach dataset info\n",
    "    idf[\"tvar\"] = dataset\n",
    "    # Append to list\n",
    "    df_list.append(idf)\n",
    "\n",
    "# Concatenate\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "# Attach month based on season\n",
    "df[\"month\"] = df[\"month\"].astype(int)\n",
    "df[\"season\"] = df[\"month\"].map(\n",
    "    {\n",
    "        1: \"DJF\",\n",
    "        2: \"DJF\",\n",
    "        3: \"MAM\",\n",
    "        4: \"MAM\",\n",
    "        5: \"MAM\",\n",
    "        6: \"JJA\",\n",
    "        7: \"JJA\",\n",
    "        8: \"JJA\",\n",
    "        9: \"SON\",\n",
    "        10: \"SON\",\n",
    "        11: \"SON\",\n",
    "        12: \"DJF\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Attach date\n",
    "df[\"date\"] = pd.to_datetime(\n",
    "    df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str) + \"-01\"\n",
    ")\n",
    "# Attach numeric date\n",
    "df[\"date_num\"] = mdates.date2num(df[\"date\"])\n",
    "display(df)\n",
    "\n",
    "# Loop over every dataset and season, and calculate the anomalies trend for that season\n",
    "df_lms = []\n",
    "df_values = []\n",
    "\n",
    "for tvar in datasets:\n",
    "    for season in seasons_to_plot:\n",
    "        # Filter for dataset and season\n",
    "        idf = df.query(f\"tvar == '{tvar}' & season == '{season}'\").copy()\n",
    "        # Calculate anomalies\n",
    "        idf[\"value\"] = idf[\"value\"] - idf[\"value\"].mean()\n",
    "        # Regress anomalies on date_num\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(idf[[\"date_num\"]], idf[\"value\"])\n",
    "        df_lms.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"tvar\": tvar,\n",
    "                    \"season\": season,\n",
    "                    \"slope\": lm.coef_[0],\n",
    "                    \"intercept\": lm.intercept_,\n",
    "                },\n",
    "                index=[0],\n",
    "            )\n",
    "        )\n",
    "        # Calculate the values for the regression line\n",
    "        df_values.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"date_num\": idf[\"date_num\"],\n",
    "                    \"tvar\": tvar,\n",
    "                    \"season\": season,\n",
    "                    \"value\": lm.predict(idf[[\"date_num\"]]),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "df_lms = pd.concat(df_lms)\n",
    "df_values = pd.concat(df_values)\n",
    "df_values = df_values.replace({\"tmoy\": \"Tmean\", \"tmax\": \"Tmax\", \"tmin\": \"Tmin\"})\n",
    "df_values\n",
    "\n",
    "# Plot the regression lines, tvar is linetype, season is color shade\n",
    "tvar_linetypes = {\n",
    "    \"Tmean\": \"-\",\n",
    "    \"Tmax\": \"--\",\n",
    "    \"Tmin\": \"-.\",\n",
    "}\n",
    "\n",
    "season_colors = {\n",
    "    \"DJF\": \"darkblue\",\n",
    "    \"MAM\": \"green\",\n",
    "    \"JJA\": \"gold\",\n",
    "    \"SON\": \"darkorange\",\n",
    "}\n",
    "\n",
    "# Start plot\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "\n",
    "# Add zero line\n",
    "ax.axhline(0, color=\"black\", linestyle=\"-\")\n",
    "\n",
    "# Add lines\n",
    "for tvar in df_values[\"tvar\"].unique():\n",
    "    for season in seasons_to_plot:\n",
    "        idf = df_values.query(f\"tvar == '{tvar}' & season == '{season}'\")\n",
    "        ax.plot(\n",
    "            idf[\"date_num\"],\n",
    "            idf[\"value\"],\n",
    "            # linestyle=tvar_linetypes[tvar],\n",
    "            color=season_colors[season],\n",
    "        )\n",
    "\n",
    "# Set the title and legend\n",
    "# ax.set_title(\n",
    "#     \"Trends for different temperature variables and seasons\", fontweight=\"bold\"\n",
    "# )\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Anomalies (mm)\")\n",
    "\n",
    "# Use equal ylim\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# Set x-axis to date format\n",
    "ax.set_xlim(-3500.0, 19000)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "# Show every 10th date to avoid overlapping\n",
    "ax.set_xticks(ax.get_xticks()[::20])\n",
    "\n",
    "# Create legend handles for tvar linetypes\n",
    "# tvar_handles = [\n",
    "#     mlines.Line2D([], [], color=\"black\", linestyle=linetype, label=tvar)\n",
    "#     for tvar, linetype in tvar_linetypes.items()\n",
    "# ]\n",
    "\n",
    "# Create legend handles for season colors\n",
    "season_handles = [\n",
    "    mlines.Line2D([], [], color=color, linestyle=\"-\", label=season)\n",
    "    for season, color in season_colors.items()\n",
    "]\n",
    "\n",
    "# Combine both sets of handles\n",
    "fig.legend(\n",
    "    handles=season_handles,  # + tvar_handles,  # Combine both sets of handles\n",
    "    loc=\"upper right\",  # Position at the center top\n",
    "    bbox_to_anchor=(1.6, 0.9),  # Move the legend below the plot\n",
    "    # loc=\"upper left\",  # Position at the center top\n",
    "    # bbox_to_anchor=(0.125, 0.9),  # with upper left and fig size 6,3\n",
    "    ncol=2,  # Arrange all items in one row\n",
    "    title=\"Season\",  # Title for the legend\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "# Show the plot (optional, as this is just for the legend)\n",
    "# plt.tight_layout(rect=[0, 0, 0.4, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate Evolution Maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rasterio.transform import Affine\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick aggregation factor\n",
    "agg_factor_km = 10\n",
    "agg_factor_m = agg_factor_km * 1000\n",
    "# Pick dataset\n",
    "dataset = \"tmoy\"\n",
    "# Pick time period\n",
    "first_year = 1980\n",
    "last_year = 2020\n",
    "range_years = np.arange(first_year, last_year + 1)\n",
    "# Pick season\n",
    "season = \"hi\"  # pr, et, au, hi, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files\n",
    "files_all = glob.glob(f\"/Volumes/SAMSUNG 1TB/digitalis_v3/raw/1km/{dataset}/*.tif\")\n",
    "files_all = pd.DataFrame(files_all, columns=[\"files\"])\n",
    "# Attach year\n",
    "files_all[\"year\"] = files_all[\"files\"].apply(lambda x: x.split(\"/\")[-1].split(\"_\")[1])\n",
    "files_all[\"year\"] = files_all[\"year\"].astype(int)\n",
    "# Attach month\n",
    "files_all[\"month\"] = files_all[\"files\"].apply(\n",
    "    lambda x: x.split(\"/\")[-1].split(\"_\")[2].split(\".\")[0]\n",
    ")\n",
    "# Remove 6120 files\n",
    "files_all = files_all.query(\"year != 6120\")\n",
    "# Sort by year and month\n",
    "files_all = files_all.sort_values(by=[\"year\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "# Folder to save aggregated files (not really needed because quick enough)\n",
    "agg_folder = f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/aggregated-to-{agg_factor_km}km/{dataset}\"\n",
    "os.makedirs(agg_folder, exist_ok=True)\n",
    "\n",
    "# Attach csv filenames, make loop to make it easier\n",
    "files_all[\"csv_files\"] = \"\"\n",
    "\n",
    "for i, row in files_all.iterrows():\n",
    "    files_all.loc[i, \"csv_files\"] = f\"{agg_folder}/{row.year}_{row.month}.csv\"\n",
    "\n",
    "# Filter for season\n",
    "files = files_all.query(f\"month == '{season}'\").reset_index(drop=True)\n",
    "# Filter for range of years\n",
    "files = files.query(\"year >= @first_year & year <= @last_year\").reset_index(drop=True)\n",
    "# Display\n",
    "files.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    df = aggregate_raster_to_csv(\n",
    "        input_raster_path=files.loc[i, \"files\"],\n",
    "        output_csv_path=files.loc[i, \"csv_files\"],\n",
    "        agg_factor_m=agg_factor_m,\n",
    "        save_file=False,\n",
    "    )\n",
    "\n",
    "    # Get year from filename\n",
    "    df[\"year\"] = files.loc[i, \"year\"]\n",
    "    df[\"month\"] = files.loc[i, \"month\"]\n",
    "\n",
    "    # Append to list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate\n",
    "df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "# Attach unique coordinates\n",
    "df[\"xy\"] = df[\"x\"].astype(str) + \"_\" + df[\"y\"].astype(str)\n",
    "\n",
    "# Divide values by 10 to get correct value\n",
    "df[\"value\"] = df[\"value\"] / 10\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At resolution of 1km, it takes about 40 minutes\n",
    "filename = (\n",
    "    agg_folder\n",
    "    + f\"/trend_per_pixel-months_{season}-from_{first_year}_to_{last_year}.feather\"\n",
    ")\n",
    "load_file = True\n",
    "\n",
    "# Load if file exists\n",
    "if os.path.exists(filename) and load_file:\n",
    "    df_mp = pd.read_feather(filename)\n",
    "    chime.info()\n",
    "    print(f\"File loaded from disk!: {filename}\")\n",
    "else:\n",
    "    df_mp = get_trend_per_pixel_loop_mp(\n",
    "        df.dropna(),\n",
    "        group_var=\"xy\",\n",
    "        date_var=\"year\",\n",
    "        value_var=\"value\",\n",
    "    )\n",
    "    chime.success()\n",
    "    df_mp.to_feather(filename)\n",
    "    print(f\"File saved to disk!: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mp was calculated with removing NA values, so we need to add these coordinates back\n",
    "df_lm = pd.merge(df[[\"xy\"]], df_mp, on=\"xy\", how=\"left\")\n",
    "# Attach x and y\n",
    "df_lm[\"x\"] = df_lm[\"xy\"].apply(lambda x: x.split(\"_\")[0]).astype(float)\n",
    "df_lm[\"y\"] = df_lm[\"xy\"].apply(lambda x: x.split(\"_\")[1]).astype(float)\n",
    "df_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the larger pixe_res, the longer it takes to calculate + the more granular the map becomes.\n",
    "make_map_for_temp_prec_cover(\n",
    "    df_lm,\n",
    "    \"tmoy\",\n",
    "    pixel_res=100j,\n",
    "    textsize=20,\n",
    "    contour_levels=5,\n",
    "    filepath=filename.replace(\".feather\", \".png\"),\n",
    ")\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rasterio.transform import Affine\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines\n",
    "from scipy.stats import mode\n",
    "\n",
    "from IPython.display import clear_output  # For clearing the output of a cell\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_season = {\n",
    "    \"13\": \"Annual\",\n",
    "    \"hi\": \"Winter\",\n",
    "    \"pr\": \"Spring\",\n",
    "    \"et\": \"Summer\",\n",
    "    \"au\": \"Fall\",\n",
    "}\n",
    "\n",
    "dict_dataset = {\n",
    "    \"tmoy\": \"Temperature\",\n",
    "    \"prec\": \"Precipitation\",\n",
    "    \"treecover\": \"Forest Cover\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_res = 500j\n",
    "agg_factor_km = 1\n",
    "\n",
    "# for d in [\"tmoy\", \"prec\"]:\n",
    "for d in [\"prec\"]:\n",
    "    for s in [\"13\", \"hi\", \"pr\", \"et\", \"au\"]:\n",
    "        for f in [1960, 1980, 2000]:\n",
    "            display(f\"Working on {d} for {s} over period {f} - 2020\")\n",
    "            print(\"... getting data\")\n",
    "            df_lm = produce_dfs_for_climate_evolution(\n",
    "                agg_factor_km=agg_factor_km,\n",
    "                dataset=d,\n",
    "                first_year=f,\n",
    "                last_year=2020,\n",
    "                season=s,\n",
    "                load_file=True,\n",
    "            )\n",
    "\n",
    "            print(\"... making map\")\n",
    "            make_map_for_temp_prec_cover(\n",
    "                df_lm,\n",
    "                d,\n",
    "                pixel_res=pixel_res,\n",
    "                textsize=20,\n",
    "                contour_levels=7,\n",
    "                filepath=f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/aggregated-to-{agg_factor_km}km/{d}/trend_per_pixel-months_{s}-from_{f}_to_{2020}-res_{pixel_res}.png\",\n",
    "            )\n",
    "        clear_output()\n",
    "        # display(\"Time: \" + datetime.datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! osascript -e 'tell app \"System Events\" to shut down'# Define parameters\n",
    "seasons = [\"13\", \"hi\", \"pr\", \"et\", \"au\"]  # Columns\n",
    "contour_levels = 11\n",
    "last_year = 2020\n",
    "\n",
    "for dataset in [\"tmoy\", \"prec\"]:\n",
    "\n",
    "    # Create a figure and grid of axes\n",
    "    fig, axs = plt.subplots(\n",
    "        1,\n",
    "        len(seasons),\n",
    "        figsize=(18, 3),\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Iterate through the grid\n",
    "    for i, season in enumerate(seasons):\n",
    "        # Get the axis for the current grid cell\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Plot the data\n",
    "        df_1960 = produce_dfs_for_climate_evolution(\n",
    "            agg_factor_km=1,\n",
    "            dataset=dataset,\n",
    "            first_year=1960,\n",
    "            last_year=2020,\n",
    "            season=season,\n",
    "            load_file=True,\n",
    "        )\n",
    "\n",
    "        df_1980 = produce_dfs_for_climate_evolution(\n",
    "            agg_factor_km=1,\n",
    "            dataset=dataset,\n",
    "            first_year=1980,\n",
    "            last_year=2020,\n",
    "            season=season,\n",
    "            load_file=True,\n",
    "        )\n",
    "\n",
    "        df_2000 = produce_dfs_for_climate_evolution(\n",
    "            agg_factor_km=1,\n",
    "            dataset=dataset,\n",
    "            first_year=2000,\n",
    "            last_year=2020,\n",
    "            season=season,\n",
    "            load_file=True,\n",
    "        )\n",
    "\n",
    "        # Plot the histograms over each other\n",
    "        df_1960.value.hist(\n",
    "            bins=60,\n",
    "            color=\"darkgreen\",\n",
    "            alpha=0.95,\n",
    "            edgecolor=\"darkgreen\",\n",
    "            density=True,\n",
    "            ax=ax,\n",
    "        )\n",
    "        df_1980.value.hist(\n",
    "            bins=60,\n",
    "            color=\"darkblue\",\n",
    "            alpha=0.95,\n",
    "            edgecolor=\"darkblue\",\n",
    "            density=True,\n",
    "            ax=ax,\n",
    "        )\n",
    "        df_2000.value.hist(\n",
    "            bins=60,\n",
    "            color=\"darkred\",\n",
    "            alpha=0.95,\n",
    "            edgecolor=\"darkred\",\n",
    "            density=True,\n",
    "            ax=ax,\n",
    "        )\n",
    "        plt.legend([\"1960-2020\", \"1980-2020\", \"2000-2020\"])\n",
    "        if dataset == \"tmoy\":\n",
    "            ax.set_xlabel(\"Temperature (°C/yr)\")\n",
    "        if dataset == \"prec\":\n",
    "            ax.set_xlabel(\"Precipitation (mm/yr$^{-2}$)\")\n",
    "\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        ax.set_title(f\"{dict_season[season]}\")\n",
    "\n",
    "    plt.suptitle(f\"Climate Evolution for {dict_dataset[dataset]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"../03_process_france_data/climate_evolution_figures/histograms_for_{dataset}.png\"\n",
    "    )\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Grid - Period ~ Season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "first_years = [1960, 1980, 2000]  # Rows\n",
    "seasons = [\"13\", \"hi\", \"pr\", \"et\", \"au\"]  # Columns\n",
    "contour_levels = 11\n",
    "last_year = 2020\n",
    "\n",
    "for dataset in [\"tmoy\", \"prec\"]:\n",
    "\n",
    "    # Create a figure and grid of axes\n",
    "    fig, axs = plt.subplots(\n",
    "        len(first_years),\n",
    "        len(seasons),\n",
    "        figsize=(15, 10),\n",
    "    )\n",
    "\n",
    "    # Iterate through the grid\n",
    "    for row_idx, first_year in enumerate(first_years):\n",
    "        for col_idx, season in enumerate(seasons):\n",
    "            # Get the axis for the current grid cell\n",
    "            ax = axs[row_idx, col_idx]\n",
    "\n",
    "            # Generate the dataframe dynamically\n",
    "            df_lm = produce_dfs_for_climate_evolution(\n",
    "                agg_factor_km=1,\n",
    "                dataset=dataset,\n",
    "                first_year=first_year,\n",
    "                last_year=last_year,\n",
    "                season=season,\n",
    "                load_file=True,\n",
    "            )\n",
    "\n",
    "            # Plot the map on the corresponding axis\n",
    "            make_map_for_temp_prec_cover(\n",
    "                df_lm,\n",
    "                dataset,\n",
    "                season,\n",
    "                pixel_res=500j,\n",
    "                textsize=10,\n",
    "                contour_levels=contour_levels,\n",
    "                cbar_pad=-0.02,\n",
    "                cbar_fraction=0.2,\n",
    "                cbar_shrink=0.6,\n",
    "                cbar_aspect=20,\n",
    "                tick_interval=2,\n",
    "                ax=ax,  # Dynamically assign the axis\n",
    "                filepath=f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/aggregated-to-{1}km/{dataset}/trend_per_pixel-months_{season}-from_{first_year}_to_{2020}-res_{500j}.png\",\n",
    "            )\n",
    "\n",
    "            # Set the title for each subplot based on `first_year` and `season`\n",
    "            ax.set_title(\n",
    "                f\"{dict_season[season]} ({first_year})\",\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Remove titles from all subplots\n",
    "    for ax in axs.flatten():\n",
    "        ax.set_title(\"\")\n",
    "\n",
    "    # Give the top row season names\n",
    "    for i, season in enumerate(seasons):\n",
    "        axs[0, i].set_title(f\"{dict_season[season]}\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Add y-axis by hand\n",
    "    plt.text(\n",
    "        0.1,\n",
    "        0.23,\n",
    "        \"2000-2020\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        transform=plt.gcf().transFigure,\n",
    "    )\n",
    "    plt.text(\n",
    "        0.1,\n",
    "        0.5,\n",
    "        \"1980-2020\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        transform=plt.gcf().transFigure,\n",
    "    )\n",
    "    plt.text(\n",
    "        0.1,\n",
    "        0.72,\n",
    "        \"1960-2020\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        transform=plt.gcf().transFigure,\n",
    "    )\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"../03_process_france_data/climate_evolution_figures/all_periods_for_{dataset}-{contour_levels}_contours.png\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    # Display the figure\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Period Grid - Metric ~ Season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "agg_factor_km = 1\n",
    "textsize = 15\n",
    "pixel_res = 500j\n",
    "# Local vars\n",
    "dir_fig = \"./climate_evolution_figures/\"\n",
    "os.makedirs(dir_fig, exist_ok=True)\n",
    "\n",
    "season_dic = {\n",
    "    \"13\": \"Annual\",\n",
    "    \"hi\": \"Winter\",\n",
    "    \"pr\": \"Spring\",\n",
    "    \"et\": \"Summer\",\n",
    "    \"au\": \"Autumn\",\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    \"tmoy\",\n",
    "    \"tmoy\",\n",
    "    \"tmoy\",\n",
    "    \"tmoy\",\n",
    "    \"tmoy\",\n",
    "    \"prec\",\n",
    "    \"prec\",\n",
    "    \"prec\",\n",
    "    \"prec\",\n",
    "    \"prec\",\n",
    "]\n",
    "seasons = [\"13\", \"hi\", \"pr\", \"et\", \"au\", \"13\", \"hi\", \"pr\", \"et\", \"au\"]\n",
    "\n",
    "for f in [1960, 1980, 2000]:\n",
    "\n",
    "    # Create 4x2 grid of plots\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(30, 15))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, s in enumerate(seasons):\n",
    "        df_lm = produce_dfs_for_climate_evolution(\n",
    "            agg_factor_km=agg_factor_km,\n",
    "            dataset=datasets[i],\n",
    "            first_year=f,\n",
    "            last_year=2020,\n",
    "            season=s,\n",
    "            load_file=True,\n",
    "        )\n",
    "\n",
    "        ax[i] = make_map_for_temp_prec_cover(\n",
    "            df_lm,\n",
    "            datasets[i],\n",
    "            season=s,\n",
    "            pixel_res=pixel_res,\n",
    "            textsize=textsize,\n",
    "            contour_levels=7,\n",
    "            filepath=f\"/Volumes/SAMSUNG 1TB/digitalis_v3/processed/aggregated-to-{agg_factor_km}km/{datasets[i]}/trend_per_pixel-months_{seasons[i]}-from_{f}_to_{2020}-res_{pixel_res}.png\",\n",
    "            ax=ax[i],\n",
    "        )\n",
    "        # Fix title\n",
    "        ax[i].set_title(\n",
    "            season_dic[s], loc=\"left\", fontweight=\"bold\", fontsize=20, pad=0\n",
    "        )\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"../03_process_france_data/climate_evolution_figures/climate_evolution-{f}_to_2020_agg-{agg_factor_km}km-res_{pixel_res}.png\"\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOS\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
