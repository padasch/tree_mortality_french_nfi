{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Mortality Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()\n",
    "\n",
    "from IPython.display import clear_output  # For clearing the output of a cell\n",
    "import json\n",
    "\n",
    "\n",
    "# List available data\n",
    "tmp = list_predictor_datasets(return_list=False)\n",
    "display(\"--------\")\n",
    "print(\"\\nList of available species and their percentages\")\n",
    "tmp = get_final_nfi_data_for_analysis(verbose=False).query(\n",
    "    \"tree_state_change in ['alive_alive', 'alive_dead']\"\n",
    ")\n",
    "# Get normalized and non normalized counts\n",
    "species = tmp[\"species_lat2\"].value_counts()\n",
    "species_norm = tmp[\"species_lat2\"].value_counts(normalize=True)\n",
    "for i in species.index:\n",
    "    print(f\"{i:25} {species[i]:<30} {species_norm[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(species, user_input, base_dir=None):\n",
    "    # NFI file name\n",
    "    # Folder prefix\n",
    "    folder_suffix = \"\"  # None for no prefix\n",
    "    user_input[\"subset_group\"] = [species]\n",
    "    user_input[\"predictor_datasets\"] = [\n",
    "        # \"agroparistech_soil\",\n",
    "        # \"apt\",\n",
    "        # \"cs_disturbances\",\n",
    "        \"digitalis_tmax\",\n",
    "        \"digitalis_tmin\",\n",
    "        \"digitalis_tmoy\",\n",
    "        # \"edo\",\n",
    "        \"esa_landcover_percentages\",\n",
    "        \"forest_biodiversity\",\n",
    "        \"forest_competition\",\n",
    "        \"forest_gini\",\n",
    "        # \"forest_health\",\n",
    "        # \"forest_structure_idp\",\n",
    "        \"france_dem\",\n",
    "        \"human_activity\",\n",
    "        # \"ndvi\",\n",
    "        # \"nfi_site_information\",\n",
    "        # \"safran\",\n",
    "        # \"soil\",\n",
    "        \"spei_minmean\",\n",
    "        \"spei_trend\",\n",
    "        # \"treecover\",\n",
    "        # \"metrics_of_change\",  # > Growth / Mortality / Recruitment / Logging -> Specify variables below\n",
    "    ]\n",
    "\n",
    "    # ! Get current directory\n",
    "    if base_dir is None:\n",
    "        user_input[\"current_dir\"] = create_new_run_folder_treemort(\n",
    "            user_input[\"subset_group\"][0]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Attach / if not present\n",
    "        if base_dir[-1] != \"/\":\n",
    "            base_dir += \"/\"\n",
    "\n",
    "        user_input[\"current_dir\"] = base_dir + species + \"/\"\n",
    "        os.makedirs(user_input[\"current_dir\"], exist_ok=True)\n",
    "\n",
    "    current_dir = user_input[\"current_dir\"]\n",
    "\n",
    "    # Write to file\n",
    "    file_path = f\"{current_dir}/__user_input.txt\"\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for key, value in user_input.items():\n",
    "            if isinstance(value, list):\n",
    "                file.write(f\"{key}:\")\n",
    "                for v in value:\n",
    "                    file.write(f\"\\n - {v}\")\n",
    "                file.write(\"\\n\\n\")\n",
    "            else:\n",
    "                file.write(f\"{key}:\\n - {value}\\n\\n\")\n",
    "\n",
    "    # Add a few quick_see_files\n",
    "    write_txt(\n",
    "        f\"{current_dir}/_{user_input['subset_group'][0]}_{user_input['subset'][0]}.txt\"\n",
    "    )\n",
    "\n",
    "    if user_input[\"do_smote_test_validation\"]:\n",
    "        write_txt(f\"{user_input['current_dir']}/🚨 SMOTE for RFE-CV val set.txt\")\n",
    "\n",
    "    if user_input[\"do_smote_test_final\"]:\n",
    "        write_txt(f\"{user_input['current_dir']}/🚨 SMOTE for final test set.txt\")\n",
    "\n",
    "    write_txt(\n",
    "        f\"{user_input['current_dir']}/🚨 best model based on {user_input['best_model_decision']}.txt\"\n",
    "    )\n",
    "\n",
    "    if user_input[\"description_file\"] is not None:\n",
    "        write_txt(f\"{user_input['current_dir']}/_{user_input['description_file']}.txt\")\n",
    "\n",
    "    # write_txt(\n",
    "    #     f\"{current_dir}/_SUBSET-REGION_{user_input['region_subset']}-{user_input['region_subset_group']}.txt\"\n",
    "    # )\n",
    "    ## Load Data\n",
    "\n",
    "    # ! Load Cleaned Dataset\n",
    "    df_raw = pd.read_feather(here(\"data/final/nfi/nfi_ready_for_analysis.feather\"))\n",
    "    ## Apriori Filter\n",
    "\n",
    "    df_filter = df_raw.copy()\n",
    "    print(f\" - Initial shape of nfi data: {df_filter.shape}\")\n",
    "\n",
    "    # # Keep only alive_alive and alive_dead\n",
    "    # df_filter = df_filter.query(\"tree_state_change in ['alive_alive', 'alive_dead']\")\n",
    "\n",
    "    # ## Filters following work by A. Taccoen\n",
    "    # # No forest edges: plisi 1,2 indicate forest edges in the plot\n",
    "    # df_filter = df_filter.query(\"plisi not in [1, 2]\")\n",
    "\n",
    "    # # No groves\n",
    "    # df_filter = df_filter.query(\n",
    "    #     \"utip_1 != 'V' and utip_2 != 'V' and csa_1 != 2 and csa_2 != 2\"\n",
    "    # )\n",
    "\n",
    "    # # No natural incidences\n",
    "    # df_filter = df_filter.query(\n",
    "    #     \"nincid_1 in ['0', 'Missing'] and nincid_2 in ['0', 'Missing']\"\n",
    "    # )\n",
    "\n",
    "    # # No broken [T] or fallen/windthrown [Z, A, 1] trees (or cut [6, 7])\n",
    "    # df_filter = df_filter.query(\"veget5 not in ['Z', '1', 'T', 'A', '6', '7']\")\n",
    "\n",
    "    # # No sites with less than 10% of trees alive at first visit\n",
    "    # df_filter = df_filter.query(\"share_alive > 0.1\")\n",
    "\n",
    "    # # No sites with less than 10% of trees DBH >= 7.5cm\n",
    "    # df_filter = df_filter.query(\"share_larger75dbh > 0.1\")\n",
    "    # # df_filter = df_filter.query(\"share_smaller75dbh < 0.9\")\n",
    "\n",
    "    ## Verbose\n",
    "    # filter_report(\"\", df_raw, df_filter)\n",
    "    ### Target\n",
    "\n",
    "    # Load Tree Level Data\n",
    "    df_subset = df_filter.copy()\n",
    "\n",
    "    # Filter out trees that do not belong to the desired group\n",
    "    # Check if subset variables are in the dataset\n",
    "    for subset in user_input[\"subset\"]:\n",
    "        if subset not in df_subset.columns:\n",
    "            raise KeyError(f\"{subset} not in columns\")\n",
    "\n",
    "    df_before = df_subset.copy()\n",
    "\n",
    "    for subset in user_input[\"subset\"]:\n",
    "        df_subset = df_subset[df_subset[subset].isin(user_input[\"subset_group\"])]\n",
    "        # print(\n",
    "        #     f\"Kept {len(df_subset)} trees based on {subset}: {user_input['subset_group']}\"\n",
    "        # )\n",
    "\n",
    "    # Filter out irrelevant trees\n",
    "    df_subset = df_subset.query(\n",
    "        \"tree_state_change == 'alive_alive' or tree_state_change == 'alive_dead'\"\n",
    "    )\n",
    "    # Encode target\n",
    "    df_subset[\"target\"] = (\n",
    "        df_subset[\"tree_state_change\"]\n",
    "        .copy()\n",
    "        .apply(lambda x: 1 if x == \"alive_dead\" else 0)\n",
    "    )\n",
    "\n",
    "    # Clean df a bit\n",
    "    df_subset = move_vars_to_front(df_subset, [\"idp\", \"tree_id\", \"target\"])\n",
    "\n",
    "    # Keep target dataset separately\n",
    "    df_target = df_subset[[\"idp\", \"tree_id\", \"target\"]]\n",
    "    # display(df_target)\n",
    "    # display(df_target.target.value_counts())\n",
    "    # display(df_target.target.value_counts(normalize=True))\n",
    "    # df_target.target.value_counts(normalize=True).plot(kind=\"bar\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Break function if only alive trees\n",
    "    if df_target.target.value_counts().shape[0] == 1:\n",
    "        display(df_target.target.value_counts())\n",
    "        print(f\" - Skipping because too few dead trees\")\n",
    "        write_txt(f\"{current_dir}/⚠️ too few dead trees.txt\")\n",
    "        return None\n",
    "\n",
    "    # Break function if too little dead trees\n",
    "    if df_target.target.value_counts()[1] < 35:\n",
    "        display(df_target.target.value_counts())\n",
    "        print(f\" - Skipping because too few dead trees\")\n",
    "        write_txt(f\"{current_dir}/⚠️ too few dead trees.txt\")\n",
    "        return None\n",
    "\n",
    "    print(f\" - Shape of target dataset: {df_target.shape}\")\n",
    "\n",
    "    ### ! Predictors ----------------------------------------------------------------\n",
    "    # #! Initiate dictionary and df\n",
    "    dict_preds = {}\n",
    "    df_preds = df_subset.copy()[[\"idp\", \"tree_id\"]]\n",
    "\n",
    "    #! Tree Properties\n",
    "    # Using df_subset from above to pick variables\n",
    "    voi = [\"htot_final\", \"c13_rel\", \"c13_1\"]\n",
    "    df_tree = df_subset[[\"idp\", \"tree_id\"] + voi]\n",
    "    df_preds = df_preds.merge(df_tree, on=[\"idp\", \"tree_id\"], how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Tree\", df_tree, dict_preds)\n",
    "\n",
    "    #! Stand Properties\n",
    "    # Using df_subset from above to pick variables\n",
    "    df_stand = df_subset[[\"idp\", \"tree_id\", \"social_status\"]]\n",
    "\n",
    "    # Using separately calculated metrics\n",
    "    df_stand = (\n",
    "        df_stand.merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_competition\"),\n",
    "            on=[\"idp\", \"tree_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_biodiversity\"),\n",
    "            on=[\"idp\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .merge(\n",
    "            attach_or_load_predictor_dataset(\"forest_gini\"),\n",
    "            on=[\"idp\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    df_preds = df_preds.merge(df_stand, on=[\"idp\", \"tree_id\"], how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Stand\", df_stand, dict_preds)\n",
    "\n",
    "    #! Carrying Capacity\n",
    "    df_cc = attach_or_load_predictor_dataset(\"forest_carrying_capacity\")\n",
    "    df_preds = df_preds.merge(df_cc, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Carrying Capacity\", df_cc, dict_preds)\n",
    "\n",
    "    #! Topography\n",
    "    df_topo = attach_or_load_predictor_dataset(\"france_dem\")\n",
    "    # Keep only variables at 1000m resolution (we will use this as the main resolution)\n",
    "    df_topo = df_topo[[\"idp\"] + [var for var in df_topo.columns if \"1000\" in var]]\n",
    "    # Remove dem1000_ and _mean from variable names\n",
    "    df_topo.columns = [\"idp\"] + [\n",
    "        var.replace(\"dem1000_\", \"\").replace(\"_mean\", \"\") for var in df_topo.columns[1:]\n",
    "    ]\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_topo, on=\"idp\", how=\"left\")\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Topography\", df_topo, dict_preds)\n",
    "\n",
    "    #! Soil Conditions\n",
    "    df_soil = attach_or_load_predictor_dataset(\"agroparistech_soil\")\n",
    "    # Clean variable names\n",
    "    df_soil.columns = [var.replace(\"soil_\", \"\") for var in df_soil.columns]\n",
    "    df_soil = df_soil.drop(columns=[\"first_year\"])\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_soil, on=\"idp\", how=\"left\")\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Soil\", df_soil, dict_preds)\n",
    "\n",
    "    #! Temperature\n",
    "    drop_cols = [\"idp\", \"first_year\", \"yrs_before_second_visit\"]\n",
    "    df_temp = pd.concat(\n",
    "        [\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmoy\"),\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmin\").drop(columns=drop_cols),\n",
    "            attach_or_load_predictor_dataset(\"digitalis_tmax\").drop(columns=drop_cols),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # > Note: Removed annual metrics because they are dominated by summer or winter temperatures anyways\n",
    "    for col in df_temp.columns:\n",
    "        if \"ann\" in col:\n",
    "            df_temp = df_temp.drop(columns=col)\n",
    "        elif \"trend\" in col:\n",
    "            df_temp = df_temp.drop(columns=col)\n",
    "        elif \"slope\" in col:\n",
    "            df_temp = df_temp.drop(columns=col)\n",
    "    display(\"🟣🟣🟣 CURRENTLY NOT USING Temperature TREND VARIABLES 🟣🟣🟣\")\n",
    "\n",
    "    # Attach to df_preds\n",
    "    df_preds = df_preds.merge(df_temp, on=\"idp\", how=\"left\")\n",
    "\n",
    "    # Save variables to dictionary\n",
    "    dict_preds = add_vars_to_dict(\"Temperature\", df_temp, dict_preds)\n",
    "\n",
    "    #! SPEI\n",
    "    df_spei = pd.merge(\n",
    "        attach_or_load_predictor_dataset(\"spei_trend\"),\n",
    "        attach_or_load_predictor_dataset(\"spei_minmean\"),\n",
    "        on=\"idp\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    df_spei = attach_or_load_predictor_dataset(\"spei_minmean\")\n",
    "    display(\"🟣🟣🟣 CURRENTLY NOT USING SPEI TREND VARIABLES 🟣🟣🟣\")\n",
    "\n",
    "    # Rename columns from numbers to months\n",
    "    df_spei.columns = [\n",
    "        var.replace(\"-1_\", \"-jan_\")\n",
    "        .replace(\"-2_\", \"-feb_\")\n",
    "        .replace(\"-3_\", \"-mar_\")\n",
    "        .replace(\"-4_\", \"-apr_\")\n",
    "        .replace(\"-5_\", \"-may_\")\n",
    "        .replace(\"-6_\", \"-jun_\")\n",
    "        .replace(\"-7_\", \"-jul_\")\n",
    "        .replace(\"-8_\", \"-aug_\")\n",
    "        .replace(\"-9_\", \"-sep_\")\n",
    "        .replace(\"-10_\", \"-oct_\")\n",
    "        .replace(\"-11_\", \"-nov_\")\n",
    "        .replace(\"-12_\", \"-dec_\")\n",
    "        .replace(\"-13_\", \"-ann_\")\n",
    "        for var in df_spei.columns\n",
    "    ]\n",
    "\n",
    "    # From df_spei select only variables with the following patterns:\n",
    "    # > Note: Removed \"ann\" to focus on seasons\n",
    "    spei_durations = [f\"spei{i}-\" for i in [1, 3, 6, 9, 12, 15, 18, 21, 24]]\n",
    "    spei_months = [f\"*-{i}_*\" for i in [\"feb\", \"may\", \"aug\", \"nov\"]]\n",
    "    spei_subset = match_variables(df_spei, spei_durations)\n",
    "    spei_subset = match_variables(df_spei[spei_subset], spei_months)\n",
    "\n",
    "    df_spei = df_spei[[\"idp\"] + spei_subset]\n",
    "\n",
    "    df_preds = df_preds.merge(df_spei, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"SPEI\", df_spei, dict_preds)\n",
    "\n",
    "    #! Management\n",
    "    df_human = attach_or_load_predictor_dataset(\"human_activity\")\n",
    "    df_preds = df_preds.merge(df_human, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"Management\", df_human, dict_preds)\n",
    "\n",
    "    #! Landcover\n",
    "    # df_lc = attach_or_load_predictor_dataset(\"esa_landcover_percentages\")\n",
    "    # df_preds = df_preds.merge(df_lc, on=\"idp\", how=\"left\")\n",
    "    # dict_preds = add_vars_to_dict(\"Land Cover\", df_lc, dict_preds)\n",
    "\n",
    "    #! NDVI\n",
    "    df_ndvi = attach_or_load_predictor_dataset(\"ndvi\")\n",
    "    df_preds = df_preds.merge(df_ndvi, on=\"idp\", how=\"left\")\n",
    "    dict_preds = add_vars_to_dict(\"NDVI\", df_ndvi, dict_preds)\n",
    "\n",
    "    # ! Align direction of variables\n",
    "    # Increasing distance to road should mean more management\n",
    "    df_preds.dist_road = df_preds.dist_road.replace({0: 4, 1: 3, 3: 1, 4: 0})\n",
    "\n",
    "    # ! Update dictionary --------------------------------------------------------------------------------\n",
    "    dict_preds_org = dict_preds.copy()\n",
    "    dict_preds_org\n",
    "    dict_preds = dict_preds_org.copy()\n",
    "    dict_preds.pop(\"Tree\", None)\n",
    "    dict_preds.pop(\"Stand\", None)\n",
    "    dict_preds.pop(\"Soil\", None)\n",
    "    dict_preds.pop(\"Carrying Capacity\", None)\n",
    "\n",
    "    dict_preds[\"Tree Size\"] = [\n",
    "        \"htot_final\",\n",
    "        \"c13_1\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Light Competition\"] = [\n",
    "        \"c13_rel\",\n",
    "        \"social_status\",\n",
    "        \"competition_larger\",\n",
    "        \"competition_larger_rel\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Species Competition\"] = [\n",
    "        \"competition_same_species\",\n",
    "        \"competition_same_species_rel\",\n",
    "        \"competition_other_species\",\n",
    "        \"competition_other_species_rel\",\n",
    "        \"belongs_to_dom_spec\",\n",
    "        \"num_species\",\n",
    "        \"simpson_species\",\n",
    "        \"shannon_species\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Stand Structure\"] = [\n",
    "        \"num_trees\",\n",
    "        \"gini_ba_1\",\n",
    "        \"mean_dbh\",\n",
    "        \"carrying_capacity\",\n",
    "        \"competition_total\",\n",
    "    ]\n",
    "    # dict_preds[\"stand_diversity\"] = [\"num_species\", \"simpson_species\", \"shannon_species\"]\n",
    "    dict_preds[\"Soil Fertility\"] = [\n",
    "        # \"waterlogging_temp\",\n",
    "        # \"waterlogging_perm\",\n",
    "        # \"swhc\",\n",
    "        \"CN\",\n",
    "        \"pH\",\n",
    "    ]\n",
    "\n",
    "    dict_preds[\"Soil Water Conditions\"] = [\n",
    "        \"waterlogging_temp\",\n",
    "        \"waterlogging_perm\",\n",
    "        \"swhc\",\n",
    "    ]\n",
    "\n",
    "    # dict_preds[\"soil_conditions\"] = []\n",
    "\n",
    "    # Save dictionary to file\n",
    "    with open(f\"{current_dir}/dict_preds.json\", \"w\") as f:\n",
    "        json.dump(dict_preds, f)\n",
    "\n",
    "    # ! DATA PREPARATION --------------------------------------------------------------------------------\n",
    "\n",
    "    #### Removing NAs\n",
    "    df_before = df_preds.copy()\n",
    "\n",
    "    # Get Na percentages per columns\n",
    "    l_nas = round(\n",
    "        df_before.drop(columns=[\"idp\", \"tree_id\"])\n",
    "        .isna()\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "        / len(df_before)\n",
    "        * 100,\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    # print(\"Percentages of NA values in the predictor dataset:\")\n",
    "    # for i in range(0, len(l_nas)):\n",
    "    # print(f\"- {l_nas.index[i]:25}: {l_nas[i]}%\")\n",
    "    # display()\n",
    "\n",
    "    # Drop where more than 10% of values are missing\n",
    "    dropped_vars = []\n",
    "    for var in l_nas[l_nas > 10].index:\n",
    "        # print(f\"Dropping '{var}'\")\n",
    "        dropped_vars = dropped_vars.append(var)\n",
    "        df_before = df_before.drop(columns=var)\n",
    "    # print(f\"\\nDropping variables with more than 10% missing values: {dropped_vars}\")\n",
    "\n",
    "    # Impute mean values where less than 10% of values are missing\n",
    "    l_imp = l_nas.copy()\n",
    "    l_imp = l_imp[l_imp > 00]\n",
    "    l_imp = l_imp[l_imp <= 10]\n",
    "    imp_vars = l_imp.index\n",
    "    # display()\n",
    "    # print(\n",
    "    #     f\"\\nImputing mean values for variables with less than 10% missing values: {imp_vars}\"\n",
    "    # )\n",
    "    for var in l_imp.index:\n",
    "        # print(f\"Imputing mean for '{var}'\")\n",
    "        df_before[var] = df_before[var].fillna(df_before[var].mean())\n",
    "\n",
    "    df_nonas = df_before.copy()\n",
    "    #### OHE\n",
    "\n",
    "    # Get temporary df\n",
    "    df_ohe = df_nonas.copy()\n",
    "\n",
    "    # Get all variables names before one-hot encoding\n",
    "    all_var_names_before_ohe = sorted(df_ohe.columns.to_list())\n",
    "\n",
    "    # Set variables to not ohe:\n",
    "    my_vars_not_to_ohe = [\"test_train_strata\", \"target\", \"idp\", \"tree_id\"]\n",
    "\n",
    "    # Do the OHE\n",
    "    df_ohe = do_ohe(df_ohe, my_vars_not_to_ohe, verbose=False)\n",
    "\n",
    "    # Get all variables names after one-hot encoding\n",
    "    all_var_names_after_ohe = sorted(df_ohe.columns.to_list())\n",
    "\n",
    "    # Get variable dictionary\n",
    "    var_ohe_dict = {}\n",
    "    for var in all_var_names_before_ohe:\n",
    "        sub_vars = []\n",
    "\n",
    "        if var in all_var_names_after_ohe:\n",
    "            # If the variable was not ohe, it stays the same\n",
    "            var_ohe_dict[var] = [var]\n",
    "            continue\n",
    "        else:\n",
    "            # If the variable was ohe, search for pattern and add it\n",
    "            pattern = r\"^\" + var + r\"_.*\"\n",
    "            for sub_var in all_var_names_after_ohe:\n",
    "                # print(pattern, sub_var, re.match(pattern, sub_var))\n",
    "                if re.match(pattern, sub_var):\n",
    "                    sub_vars.append(sub_var)\n",
    "        var_ohe_dict[var] = sub_vars\n",
    "\n",
    "    # Print which variable has how many sub-variables\n",
    "    # print(\"\\n---- Variable OHE Count ----\")\n",
    "    # for k, v in var_ohe_dict.items():\n",
    "    #     if len(v) > 1:\n",
    "    # print(f\" - {k}: {len(v)}\")\n",
    "    ### Final Dataset\n",
    "\n",
    "    # Get final dataset from above\n",
    "    df_predictors_final = df_ohe.copy()\n",
    "\n",
    "    # Raise error if target and predictor df have not same number of rows\n",
    "    if df_target.shape[0] != df_predictors_final.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Target and predictor datasets have different number of rows: {df_target.shape[0]} vs {df_predictors_final.shape[0]}\"\n",
    "        )\n",
    "\n",
    "    # Merge to get correct order\n",
    "    df_target_pred_final = pd.merge(\n",
    "        df_target, df_predictors_final, on=[\"idp\", \"tree_id\"], how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_target_pred_final = df_target_pred_final.drop(\n",
    "        columns=[\"idp\", \"tree_id\", \"first_year\"], errors=\"ignore\"\n",
    "    )\n",
    "    # df_target_pred_final.to_csv(\"df_final_target_predictors.csv\", index=False)\n",
    "    #### Test/Train Split\n",
    "\n",
    "    # Get df\n",
    "    df_for_splitting = df_target_pred_final.copy()\n",
    "    print(f\" - Shape of df before splitting: \\t {df_for_splitting.shape}\")\n",
    "\n",
    "    X = df_for_splitting.drop(\"target\", axis=1)\n",
    "    y = df_for_splitting[\"target\"]\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=user_input[\"test_split\"],\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    Xy_train = pd.concat([y_train, X_train], axis=1).reset_index(drop=True)\n",
    "    Xy_test = pd.concat([y_test, X_test], axis=1).reset_index(drop=True)\n",
    "\n",
    "    # print(f\"Shape of Xy_train:\\t\\t {Xy_train.shape}\")\n",
    "    # print(f\"Shape of Xy_test:\\t\\t {Xy_test.shape}\")\n",
    "    Xy_train.target.value_counts()\n",
    "    Xy_test.target.value_counts()\n",
    "    #### Debug for removing vars\n",
    "\n",
    "    # print(f\"Shape of Xy_train before removal: \\t {Xy_train.shape}\")\n",
    "    # print(f\"Shape of Xy_test before removal: \\t {Xy_test.shape}\")\n",
    "\n",
    "    # SPEI\n",
    "    if \"SPEI\" in dict_preds:\n",
    "\n",
    "        spei_train = Xy_train[\n",
    "            [var for var in dict_preds[\"SPEI\"] if var in Xy_train.columns]\n",
    "        ]\n",
    "        spei_test = Xy_test[\n",
    "            [var for var in dict_preds[\"SPEI\"] if var in Xy_test.columns]\n",
    "        ]\n",
    "\n",
    "        # Xy_train = Xy_train.drop(columns=dict_preds[\"SPEI\"], errors=\"ignore\")\n",
    "        # Xy_test = Xy_test.drop(columns=dict_preds[\"SPEI\"], errors=\"ignore\")\n",
    "\n",
    "        # dict_preds.pop(\"SPEI\", None)\n",
    "\n",
    "    # TEMPERATURE\n",
    "    # if \"Temperature\" in dict_preds:\n",
    "    #     Xy_train = Xy_train.drop(columns=dict_preds[\"Temperature\"], errors=\"ignore\")\n",
    "    #     Xy_test = Xy_test.drop(columns=dict_preds[\"Temperature\"], errors=\"ignore\")\n",
    "    #     dict_preds.pop(\"Temperature\", None)\n",
    "\n",
    "    # print(f\"Shape of Xy_train:\\t\\t\\t {Xy_train.shape}\")\n",
    "    # print(f\"Shape of Xy_test:\\t\\t\\t {Xy_test.shape}\")\n",
    "\n",
    "    print(f\" - Shape of Xy_train:\\t\\t\\t {Xy_train.shape}\")\n",
    "    print(f\" - Shape of Xy_test:\\t\\t\\t {Xy_test.shape}\")\n",
    "\n",
    "    # Keep original dfs for saving tree ID further below\n",
    "    df_target_for_treeid = df_target.copy()\n",
    "    df_predictors_final_for_treeid = df_predictors_final.copy()\n",
    "\n",
    "    # ! RFE ------------------------------------------------------------------------------\n",
    "    display(\" --- FEATURE ELIMINATION ---\")\n",
    "    rfecv_params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 8,\n",
    "        \"max_features\": 0.01,\n",
    "        \"bootstrap\": True,\n",
    "        \"criterion\": \"gini\",\n",
    "    }\n",
    "\n",
    "    df_cvmetrics_per_nfeatures = run_rfecv_treemort(\n",
    "        dict_categories=dict_preds.copy(),\n",
    "        var_ohe_dict=var_ohe_dict.copy(),\n",
    "        Xy_train_for_rfe=Xy_train.copy(),\n",
    "        user_input=user_input,\n",
    "        rfecv_params=rfecv_params,\n",
    "        debug_stop=False,\n",
    "        debug_stop_after_n_iterations=10,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    #! Report best variables ----------------------------------------------------------------\n",
    "    display(\" --- BEST FEATURES ---\")\n",
    "    if user_input[\"method_validation\"] == \"oob\":\n",
    "        user_input[\"best_model_metric\"] = \"oob\"\n",
    "\n",
    "    if user_input[\"best_model_decision\"] == \"best_metric\":\n",
    "\n",
    "        ohed_variables_in_final_model = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[\"ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        non_ohed_variables_in_final_model = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[\"non_ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        best_score = (\n",
    "            df_cvmetrics_per_nfeatures.sort_values(\n",
    "                by=user_input[\"best_model_metric\"], ascending=False\n",
    "            )\n",
    "            .head(1)[user_input[\"best_model_metric\"]]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "    elif user_input[\"best_model_decision\"] == \"best_per_category\":\n",
    "        dict_len = len(dict_preds)\n",
    "\n",
    "        ohed_variables_in_final_model = df_cvmetrics_per_nfeatures.query(\n",
    "            \"n_features == @dict_len\"\n",
    "        )[\"ohe_vars_in_model\"].values[0]\n",
    "\n",
    "        non_ohed_variables_in_final_model = df_cvmetrics_per_nfeatures.query(\n",
    "            \"n_features == @dict_len\"\n",
    "        )[\"non_ohe_vars_in_model\"].values[0]\n",
    "\n",
    "        best_score = df_cvmetrics_per_nfeatures.query(\"n_features == @dict_len\")[\n",
    "            user_input[\"best_model_metric\"]\n",
    "        ].values[0]\n",
    "\n",
    "    elif user_input[\"best_model_decision\"] == \"best_metric_max1\":\n",
    "        dict_len = len(dict_preds)\n",
    "\n",
    "        max1cat = df_cvmetrics_per_nfeatures.query(\"n_features <= @dict_len\")\n",
    "\n",
    "        non_ohed_variables_in_final_model = (\n",
    "            max1cat.sort_values(by=user_input[\"best_model_metric\"], ascending=False)\n",
    "            .head(1)[\"non_ohe_vars_in_model\"]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "        best_score = (\n",
    "            max1cat.sort_values(by=user_input[\"best_model_metric\"], ascending=False)\n",
    "            .head(1)[user_input[\"best_model_metric\"]]\n",
    "            .values[0]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid selection for final model decision!: {user_input['best_model_decision']}\"\n",
    "        )\n",
    "\n",
    "    txt_best_var = f\"\"\"\n",
    "    - Best score: {user_input['best_model_metric']} = {round(best_score,3)} based on model selecting by '{user_input['best_model_decision']}\n",
    "    \n",
    "    - Variables in best model (ohe):\\t{ohed_variables_in_final_model}\n",
    "    \n",
    "    - Variables in best model (non-ohe):\\t{sorted(non_ohed_variables_in_final_model)}\n",
    "        \"\"\"\n",
    "\n",
    "    # print(txt_best_var)\n",
    "    with open(f\"{current_dir}/final_model_variables.txt\", \"w\") as f:\n",
    "        f.write(txt_best_var)\n",
    "\n",
    "    # ! Select variables of best model\n",
    "    Xy_train_best_preds = Xy_train.copy()[[\"target\"] + ohed_variables_in_final_model]\n",
    "\n",
    "    #! Plot ----------------------------------------------------------------\n",
    "    # display()\n",
    "    # Get max number of features\n",
    "    x_max = df_cvmetrics_per_nfeatures[\"n_features\"].max()\n",
    "\n",
    "    # Get list of variables to plot\n",
    "    all_metrics = [\n",
    "        # \"balanced_accuracy\",\n",
    "        \"roc_auc\",\n",
    "        \"accuracy\",\n",
    "        \"f1\",\n",
    "        \"recall\",\n",
    "        \"precision\",\n",
    "    ]\n",
    "\n",
    "    if user_input[\"method_validation\"] == \"oob\":\n",
    "        all_metrics = [\"oob\"] + all_metrics\n",
    "\n",
    "    # Start figure\n",
    "    fig, axs = plt.subplots(len(all_metrics), 1, figsize=(7, 10))\n",
    "\n",
    "    # Loop over every variable\n",
    "    for i, metric in enumerate(all_metrics):\n",
    "\n",
    "        # Get max score\n",
    "        max_score = df_cvmetrics_per_nfeatures.sort_values(by=metric, ascending=False)[\n",
    "            metric\n",
    "        ].iloc[0]\n",
    "\n",
    "        # Get n_features at max accuracy\n",
    "        max_score_features = df_cvmetrics_per_nfeatures.sort_values(\n",
    "            by=metric, ascending=False\n",
    "        ).iloc[0, 0]\n",
    "\n",
    "        # Add lines\n",
    "        axs[i].plot(\n",
    "            df_cvmetrics_per_nfeatures[\"n_features\"],\n",
    "            df_cvmetrics_per_nfeatures[metric],\n",
    "        )\n",
    "\n",
    "        # Add error bands\n",
    "        axs[i].fill_between(\n",
    "            df_cvmetrics_per_nfeatures[\"n_features\"],\n",
    "            df_cvmetrics_per_nfeatures[metric]\n",
    "            - df_cvmetrics_per_nfeatures[f\"{metric}_sd\"],\n",
    "            df_cvmetrics_per_nfeatures[metric]\n",
    "            + df_cvmetrics_per_nfeatures[f\"{metric}_sd\"],\n",
    "            alpha=0.3,\n",
    "        )\n",
    "\n",
    "        if i == len(all_metrics) - 1:\n",
    "            axs[i].set_xlabel(\"Number of Features\")\n",
    "        else:\n",
    "            axs[i].set_xlabel(\"\")\n",
    "\n",
    "        # axs[i].set_ylabel(\"Accuracy Score\")\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_title(\"\")\n",
    "        axs[i].set_xlim(x_max, 0)\n",
    "        axs[i].set_ylim((max_score * 0.75), (max_score * 1.15))\n",
    "\n",
    "        # Add red vertical line for highest accuracy score\n",
    "        axs[i].axvline(x=max_score_features, color=\"red\")\n",
    "\n",
    "        # Add text of max_score_features to axs in red\n",
    "        axs[i].text(\n",
    "            x_max - 5,\n",
    "            max_score * 0.875,\n",
    "            f\"Optimal Nr. of Features: {int(max_score_features)} at {metric} = {round(max_score,3)}\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    # LAYOUT\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{current_dir}/fig_refcv_results.png\")\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "    # ! Correlation Removal ----------------------------------------------------------------\n",
    "    # First get feature importance of the best model\n",
    "    if user_input[\"method_validation\"] == \"cv\":\n",
    "        rf, sco, rf_vi = SMOTE_cv(\n",
    "            Xy_all=Xy_train_best_preds,\n",
    "            var_ohe_dict=var_ohe_dict,\n",
    "            rf_params=rfecv_params,\n",
    "            method_importance=user_input[\"method_importance\"],\n",
    "            smote_on_test=user_input[\"do_smote_test_validation\"],\n",
    "            rnd_seed=user_input[\"seed_nr\"],\n",
    "            verbose=False,\n",
    "            save_directory=None,\n",
    "        )\n",
    "    elif user_input[\"method_validation\"] == \"oob\":\n",
    "        rf, sco, rf_vi = SMOTE_oob(\n",
    "            Xy_all=Xy_train_best_preds,\n",
    "            var_ohe_dict=var_ohe_dict,\n",
    "            rf_params=rfecv_params,\n",
    "            method_importance=user_input[\"method_importance\"],\n",
    "            smote_on_test=user_input[\"do_smote_test_validation\"],\n",
    "            rnd_seed=user_input[\"seed_nr\"],\n",
    "            verbose=False,\n",
    "            save_directory=None,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Failed during RFE - Invalid method_validation! Got: {user_input['method_validation']}\"\n",
    "        )\n",
    "\n",
    "    # Get order of features (note that they are NOT ohe'd, so I have to first decode the dataframe, before selection. As done below.)\n",
    "    order_of_features = rf_vi.Feature.to_list()\n",
    "    final_vars = remove_correlation_based_on_vi(\n",
    "        Xy_train_best_preds,\n",
    "        var_ohe_dict,\n",
    "        rf_vi,\n",
    "        threshold=user_input[\"correlation_threshold\"],\n",
    "        make_heatmaps=False,\n",
    "        return_only_top_n=user_input[\"n_features_in_final_model\"],\n",
    "        save_directory=current_dir,\n",
    "    )\n",
    "\n",
    "    # ! SET FINAL FEATURES ----------------------------------------------------------------\n",
    "    Xy_train_final = Xy_train_best_preds.copy()[[\"target\"] + final_vars]\n",
    "    Xy_test_final = Xy_test.copy()[[\"target\"] + final_vars]\n",
    "\n",
    "    # ! TUNING -----------------------------------------------------------------------\n",
    "    # ! Prescribed Gridsearch\n",
    "    display(\" --- GRID SEARCH ---\")\n",
    "    # Get dataframe\n",
    "    Xy_train_for_tuning = Xy_train_final.copy()\n",
    "\n",
    "    # Split into response and predictors\n",
    "    Xy = Xy_train_for_tuning.copy()\n",
    "    X = Xy.drop(\n",
    "        columns=[\"target\", \"test_train_strata\", \"tree_id\", \"idp\"], errors=\"ignore\"\n",
    "    )\n",
    "    y = Xy[\"target\"]\n",
    "\n",
    "    # Build model\n",
    "    oversample = SMOTE(random_state=user_input[\"seed_nr\"])\n",
    "    model = RandomForestClassifier(random_state=user_input[\"seed_nr\"], n_jobs=-1)\n",
    "\n",
    "    # Apply oversampling to train set\n",
    "    X_train_over, y_train_over = oversample.fit_resample(X, y)\n",
    "\n",
    "    # Create Stratified K-fold cross validation\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=3, n_repeats=1, random_state=user_input[\"seed_nr\"]\n",
    "    )\n",
    "\n",
    "    # Get parameter grid\n",
    "    # param_grid = get_tune_grid_classification()\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [100, 300],  # Has minor influence\n",
    "        \"max_depth\": [1, 3, 12, 18],  # [1, 5, 8, 12, 15, 18]\n",
    "        # 'min_samples_split': [2, 5, 10],\n",
    "        # 'min_samples_leaf': [1, 2, 4],\n",
    "        \"max_features\": [0.01, 0.1, \"sqrt\"],  # Minor influence\n",
    "        # 'bootstrap': [True],\n",
    "        \"criterion\": [\"gini\"],  # 'gini',  entropy worked better on test\n",
    "    }\n",
    "\n",
    "    # Set the grid search model\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        return_train_score=True,\n",
    "        scoring=user_input[\"gsc_metric\"],\n",
    "    )\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(\n",
    "        X,\n",
    "        y,\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    display(\"\")\n",
    "    print(\"--- FINAL RESULTS ---\")\n",
    "    print(\"Parameter grid:\")\n",
    "    for key, value in param_grid.items():\n",
    "        print(f\" - {key}: {value}\")\n",
    "\n",
    "    print(\"\\nBest parameters:\")\n",
    "    for key, value in grid_search.best_params_.items():\n",
    "        print(f\" - {key}: {value}\")\n",
    "    print(\n",
    "        f\"\\nBest {user_input['best_model_metric']}: {round(grid_search.best_score_, 2)}\"\n",
    "    )\n",
    "\n",
    "    # Get best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    # Visualize tuning\n",
    "    plot_grid_search_results(\n",
    "        grid_search, \"prescribed\", save_directory=current_dir, show=False\n",
    "    )\n",
    "\n",
    "    # ! Final Model ------------------------------------------------------------------------\n",
    "    display(\" --- FINAL MODEL RUN ---\")\n",
    "\n",
    "    # Setup model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        n_jobs=-1,\n",
    "        **best_params,\n",
    "    )\n",
    "\n",
    "    # Split response and predictors\n",
    "    X_train_final = Xy_train_final.drop(columns=[\"target\"], errors=\"ignore\")\n",
    "    y_train_final = Xy_train_final[\"target\"]\n",
    "\n",
    "    X_test_final = Xy_test_final.drop(columns=[\"target\"], errors=\"ignore\")\n",
    "    y_test_final = Xy_test_final[\"target\"]\n",
    "\n",
    "    # Apply SMOTE to train data\n",
    "    sm = SMOTE(random_state=user_input[\"seed_nr\"])\n",
    "    X_train_final, y_train_final = sm.fit_resample(X_train_final, y_train_final)\n",
    "    if user_input[\"do_smote_test_final\"]:\n",
    "        X_test_final, y_test_final = sm.fit_resample(X_test_final, y_test_final)\n",
    "\n",
    "    # Fit model\n",
    "    rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Feature importance\n",
    "    # * 2024-10-20: Disabled permutation\n",
    "    # rf_vi = assessing_top_predictors(\n",
    "    #     vi_method=\"permutation\",\n",
    "    #     rf_in=rf_model,\n",
    "    #     X_train_in=X_train_final,\n",
    "    #     X_test_in=X_test_final,\n",
    "    #     y_test_in=y_test_final,\n",
    "    #     dict_ohe_in=var_ohe_dict,\n",
    "    #     with_aggregation=True,\n",
    "    #     n_predictors=20,\n",
    "    #     random_state=user_input[\"seed_nr\"],\n",
    "    #     verbose=False,\n",
    "    #     save_directory=user_input[\"current_dir\"],\n",
    "    # )\n",
    "\n",
    "    # Feature importance\n",
    "    rf_vi = assessing_top_predictors(\n",
    "        vi_method=\"impurity\",\n",
    "        rf_in=rf_model,\n",
    "        X_train_in=X_train_final,\n",
    "        X_test_in=X_test_final,\n",
    "        y_test_in=y_test_final,\n",
    "        dict_ohe_in=var_ohe_dict,\n",
    "        with_aggregation=True,\n",
    "        n_predictors=20,\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        verbose=False,\n",
    "        save_directory=user_input[\"current_dir\"],\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    model_evaluation_classification(\n",
    "        rf_model=rf_model,\n",
    "        X_train=X_train_final,\n",
    "        y_train=y_train_final,\n",
    "        X_test=X_test_final,\n",
    "        y_test=y_test_final,\n",
    "        prob_threshold=user_input[\"prob_threshold\"],\n",
    "        save_directory=user_input[\"current_dir\"],\n",
    "        metric=\"f1-score\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # y_pred = rf_model.predict(X_test_final)\n",
    "    # y_pred = pd.Series(y_pred, index=y_test_final.index)\n",
    "    # rf_score = bootstrap_classification_metric(\n",
    "    #     y_test_final,\n",
    "    #     y_pred,\n",
    "    #     metrics=[\"accuracy\", \"precision\", \"recall\", \"roc_auc\"],\n",
    "    #     n_bootstraps=100,\n",
    "    # )\n",
    "\n",
    "    # rf_score.to_csv(\n",
    "    #     f\"{current_dir}/final_model_scores_from_binary_values.csv\", index=False\n",
    "    # )\n",
    "\n",
    "    # display(rf_score)\n",
    "\n",
    "    # ! DEBUG\n",
    "    # print(\"\\n\\n\\n xxxxxxxxxxxx QUICK EXIT: NOT RUNNING SHAP! xxxxxxxxxxxx\")\n",
    "    # chime.warning()\n",
    "    # return None\n",
    "\n",
    "    # ! DEBUG TREE_ID: JUST TO SAVE LINK BETWEEN TREE_ID AND PREDICTORS\n",
    "    # * 2024-10-20: Moved tree_id debugging down here, after the final dataset is saved in the evaluation_classification function\n",
    "\n",
    "    # display(\" --- DEBUG: SAVING TREE_ID SEPARATELY ---\")\n",
    "    # Get final predictors and save a reduced X_test for safety check\n",
    "    final_predictors = (\n",
    "        pd.read_csv(f\"{current_dir}/final_model/X_test.csv\")\n",
    "        .drop(columns=[\"Unnamed: 0\"])\n",
    "        .columns.to_list()\n",
    "    )\n",
    "\n",
    "    final_predictors = []\n",
    "\n",
    "    df_targted_treeid = pd.merge(\n",
    "        df_target_for_treeid,\n",
    "        df_predictors_final_for_treeid,\n",
    "        on=[\"idp\", \"tree_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    print(f\" - Shape of df_targted_treeid: {df_targted_treeid.shape}\")\n",
    "    print(f\" - Shape of df_targted_treeid target: {df_targted_treeid['target'].shape}\")\n",
    "\n",
    "    X_train_treeid, X_test_treeid, y_train_treeid, y_test_treeid = train_test_split(\n",
    "        df_targted_treeid,\n",
    "        df_targted_treeid[\"target\"],\n",
    "        test_size=user_input[\"test_split\"],\n",
    "        random_state=user_input[\"seed_nr\"],\n",
    "        stratify=df_targted_treeid[\"target\"],\n",
    "    )\n",
    "\n",
    "    dir_treeid = f\"{current_dir}/treeid\"\n",
    "    os.makedirs(dir_treeid, exist_ok=True)\n",
    "\n",
    "    X_train_treeid[[\"tree_id\"] + final_predictors].to_csv(\n",
    "        f\"{dir_treeid}/X_train_treeid.csv\", index=True\n",
    "    )\n",
    "    X_test_treeid[[\"tree_id\"] + final_predictors].to_csv(\n",
    "        f\"{dir_treeid}/X_test_treeid.csv\", index=True\n",
    "    )\n",
    "\n",
    "    y_train_treeid.to_csv(f\"{dir_treeid}/y_train_treeid.csv\", index=True)\n",
    "    y_test_treeid.to_csv(f\"{dir_treeid}/y_test_treeid.csv\", index=True)\n",
    "\n",
    "    # display(\"Saved treeid files to:\", dir_treeid)\n",
    "    # return\n",
    "    # ! DEBUG TREE_ID ---\n",
    "\n",
    "    # * ----------------------------------------------------------------------------------------------------------------\n",
    "    # * 2024-10-20: Uncommented creation of SHAP plots and PDP plots because they are not used in the final report\n",
    "\n",
    "    # # ! SHAP ANALYSIS ------------------------------------------------------------------------\n",
    "    # display(\" --- SHAP ANALYSIS ---\")\n",
    "\n",
    "    # # Get list of features\n",
    "    # X_train_shap = Xy_train_final.drop(columns=\"target\")\n",
    "    # X_test_shap = Xy_test_final.drop(columns=\"target\")\n",
    "\n",
    "    # features = list(X_train_shap)\n",
    "\n",
    "    # # ! Pick which dataset to use\n",
    "    # test_or_train = user_input[\"shap_on_test_or_train\"]\n",
    "    # print(f\" - Using {test_or_train} set for SHAP analysis\")\n",
    "\n",
    "    # if test_or_train == \"train\":\n",
    "    #     X_shap = X_train_shap.copy()\n",
    "    # else:\n",
    "    #     X_shap = X_test_shap.copy()\n",
    "\n",
    "    # # ! Take at least 100 samples but max x% of the dataset\n",
    "    # max_perc = 0.15\n",
    "    # min_samples = 200\n",
    "    # max_samples = 800\n",
    "\n",
    "    # X_shap_len = X_shap.shape[0]\n",
    "    # # min_samples = min(min_samples, X_shap_len)\n",
    "    # # max_samples = max(max_samples, int(round(X_shap_len * max_perc)))\n",
    "    # # n_shap_samples = max(min_samples, max_samples)\n",
    "\n",
    "    # if int(round(X_shap_len * max_perc)) > max_samples:\n",
    "    #     n_shap_samples = max_samples\n",
    "    # else:\n",
    "    #     n_shap_samples = min(min_samples, X_shap_len)\n",
    "\n",
    "    # # > DEBUG option to use all data available! (takes much longer when SMOTE on test set is on!)\n",
    "    # if user_input[\"use_all_shap_data\"]:\n",
    "    #     n_shap_samples = X_shap_len\n",
    "    # else:\n",
    "    #     print(\"🔴🔴🔴 NOT USING ALL DATA TO RUN SHAP 🔴🔴🔴\")\n",
    "\n",
    "    # print(\n",
    "    #     f\" - Using {n_shap_samples} samples for SHAP analysis ({max_perc*100}% of {X_shap_len} samples = {int(round(X_shap_len * max_perc))}, min = {min_samples}, max = {max_samples})\"\n",
    "    # )\n",
    "\n",
    "    # # Run explainer\n",
    "    # # Subset and save dataset used for SHAP\n",
    "    # np.random.seed(user_input[\"seed_nr\"])\n",
    "    # X_shap = X_shap.sample(n_shap_samples, random_state=user_input[\"seed_nr\"])\n",
    "    # X_shap.to_feather(f\"{current_dir}/X_shap_{test_or_train}.feather\")\n",
    "\n",
    "    # # Get one-way SHAP values and save them\n",
    "    # print(\" - Calculating SHAP values\")\n",
    "    # explainer = shap.TreeExplainer(rf_model, X_shap)\n",
    "    # shap_values_extended = explainer(X_shap, check_additivity=False)\n",
    "\n",
    "    # # Save values\n",
    "    # with open(f\"{current_dir}/shap_values_{test_or_train}.pkl\", \"wb\") as file:\n",
    "    #     pickle.dump(shap_values_extended, file)\n",
    "\n",
    "    # # Get interaction values and save them for later\n",
    "    # if user_input[\"run_shap_interaction\"]:\n",
    "    #     print(\" - Calculating SHAP interaction values\")\n",
    "    #     interaction_values = shap.TreeExplainer(rf_model).shap_interaction_values(\n",
    "    #         X_shap\n",
    "    #     )\n",
    "    #     with open(\n",
    "    #         f\"{current_dir}/shap_values_interaction_{test_or_train}.pkl\", \"wb\"\n",
    "    #     ) as file:\n",
    "    #         pickle.dump(interaction_values, file)\n",
    "\n",
    "    # print(\" - Creating final plots\")\n",
    "\n",
    "    # # Keep values leading to mortality (1)\n",
    "    # shap_values = shap_values_extended.values[:, :, 1]\n",
    "    # # Model importances\n",
    "    # feature_importances = rf_model.feature_importances_\n",
    "    # importances = pd.DataFrame(index=features)\n",
    "    # importances[\"importance\"] = feature_importances\n",
    "    # importances[\"rank\"] = importances[\"importance\"].rank(ascending=False).values\n",
    "    # # display(importances.sort_values(\"rank\").head())\n",
    "\n",
    "    # # Calculate mean Shapley value for each feature in training set\n",
    "    # importances[\"mean_shapley_values\"] = np.mean(shap_values, axis=0)\n",
    "\n",
    "    # # Calculate mean absolute Shapley value for each feature in training set\n",
    "    # # This will give us the average importance of each feature\n",
    "    # importances[\"mean_abs_shapley_values\"] = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "    # # Add Shapley values to coefficient table.\n",
    "    # importances.sort_values(by=\"importance\", ascending=False).head()\n",
    "\n",
    "    # # Get top n features\n",
    "    # top_n = len(final_vars)\n",
    "\n",
    "    # importance_top_n = (\n",
    "    #     importances.sort_values(by=\"importance\", ascending=False).head(top_n).index\n",
    "    # )\n",
    "    # shapley_top_n = (\n",
    "    #     importances.sort_values(by=\"mean_abs_shapley_values\", ascending=False)\n",
    "    #     .head(top_n)\n",
    "    #     .index\n",
    "    # )\n",
    "\n",
    "    # # Add to DataFrame\n",
    "    # top_n_features = pd.DataFrame()\n",
    "    # top_n_features[\"importances\"] = importance_top_n.values\n",
    "    # top_n_features[\"Shapley\"] = shapley_top_n.values\n",
    "\n",
    "    # # ! Wrangle dataset for bar plots\n",
    "    # Take mean of absolute shap values\n",
    "    # xxx = pd.DataFrame(shap_values[0].tolist()).T\n",
    "    # xxx.columns = X_shap.columns\n",
    "\n",
    "    # for i in range(1, len(shap_values)):\n",
    "    #     iii = pd.DataFrame(shap_values[i].tolist()).T\n",
    "    #     iii.columns = X_shap.columns\n",
    "    #     xxx = pd.concat([xxx, iii], axis=0, ignore_index=True)\n",
    "\n",
    "    # # Take mean of all variables\n",
    "    # rrr = xxx.abs().mean().sort_values(ascending=False)\n",
    "    # rrr = rrr / rrr.sum()\n",
    "    # rrr = pd.DataFrame(rrr)\n",
    "    # rrr.columns = [\"Importance\"]\n",
    "    # rrr.Importance = rrr.Importance * 100\n",
    "    # rrr[\"Feature\"] = rrr.index\n",
    "    # rrr.reset_index(drop=True, inplace=True)\n",
    "    # # Link feature variable to predictor dataset in new column\n",
    "    # for f in rrr.Feature:\n",
    "    #     for key, value in dict_preds.items():\n",
    "    #         if f in value:\n",
    "    #             rrr.loc[rrr.Feature == f, \"dataset\"] = key\n",
    "\n",
    "    # # Sum up the VI for each dataset\n",
    "    # rrr_of_dataset = (\n",
    "    #     rrr[[\"Importance\", \"dataset\"]]\n",
    "    #     .groupby(\"dataset\")\n",
    "    #     .sum()\n",
    "    #     .reset_index()\n",
    "    #     .rename({\"Importance\": \"dataset_imp\"}, axis=1)\n",
    "    # )\n",
    "\n",
    "    # rrr_of_dataset.dataset_imp = (\n",
    "    #     rrr_of_dataset.dataset_imp / rrr_of_dataset.dataset_imp.sum() * 100\n",
    "    # )\n",
    "\n",
    "    # # Attach dataset label with percentages\n",
    "    # for i, row in rrr_of_dataset.iterrows():\n",
    "    #     # rrr_of_dataset.loc[i, \"dataset_label\"] = (\n",
    "    #     #     rrr_of_dataset.loc[i, \"dataset\"]\n",
    "    #     #     + \"  (\"\n",
    "    #     #     + str(round(rrr_of_dataset.loc[i, \"dataset_imp\"]))\n",
    "    #     #     + \"%)\"\n",
    "    #     # )\n",
    "    #     rrr_of_dataset.loc[i, \"dataset_label\"] = (\n",
    "    #         str(round(rrr_of_dataset.loc[i, \"dataset_imp\"]))\n",
    "    #         + \"%: \"\n",
    "    #         + rrr_of_dataset.loc[i, \"dataset\"]\n",
    "    #     )\n",
    "\n",
    "    # rrr = rrr.merge(rrr_of_dataset, on=\"dataset\", how=\"left\")\n",
    "\n",
    "    # # Display\n",
    "    # # display(importances, rrr)\n",
    "    # # #! Figures --------------------------------------------------------\n",
    "\n",
    "    # # > Beeswarm plot\n",
    "    # fig = plt.figure(figsize=(6, 6))\n",
    "    # shap.summary_plot(\n",
    "    #     shap_values=shap_values,\n",
    "    #     features=X_shap.values,\n",
    "    #     feature_names=X_shap.columns.values,\n",
    "    #     plot_type=\"violin\",\n",
    "    #     max_display=15,\n",
    "    #     show=False,\n",
    "    # )\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{current_dir}/fig_shap_beeswarm.png\")\n",
    "    # plt.close()\n",
    "    # # plt.show()\n",
    "\n",
    "    # # ! PDP plot ------------------------------------------------------------\n",
    "    # feat_to_show = shapley_top_n[0:]\n",
    "    # num_features = len(feat_to_show)\n",
    "    # num_cols = 7  # Number of subplots per row\n",
    "    # num_rows = 2  # Calculate number of rows\n",
    "    # if n_shap_samples > 5000:\n",
    "    #     point_density = 0.25\n",
    "    # elif n_shap_samples > 2500:\n",
    "    #     point_density = 0.5\n",
    "    # elif n_shap_samples > 2500:\n",
    "    #     point_density = 0.5\n",
    "    # else:\n",
    "    #     point_density = 0.75\n",
    "\n",
    "    # fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(25, 8))\n",
    "\n",
    "    # for i, feat in enumerate(feat_to_show):\n",
    "    #     row = i // num_cols  # Calculate row index\n",
    "    #     col = i % num_cols  # Calculate column index\n",
    "\n",
    "    #     shap.plots.scatter(\n",
    "    #         shap_values_extended[:, feat][:, 1],\n",
    "    #         x_jitter=0,\n",
    "    #         alpha=point_density,\n",
    "    #         ax=axes[row, col],\n",
    "    #         show=False,\n",
    "    #     )\n",
    "    #     axes[row, col].set_xlabel(f\"{feat}\")\n",
    "    #     if col == 0:  # For the first column\n",
    "    #         axes[row, col].set_ylabel(\"SHAP Value\")\n",
    "    #     else:\n",
    "    #         axes[row, col].set_ylabel(\"SHAP VAlue\")\n",
    "    #     # axes[row, col].set_ylim(-0.2, 0.2)\n",
    "\n",
    "    # # Remove extra subplots\n",
    "    # for i in range(num_features, num_rows * num_cols):\n",
    "    #     fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    # fig.set_tight_layout(True)\n",
    "    # fig.savefig(f\"{current_dir}/fig_shap_scatter.png\")\n",
    "    # plt.close()\n",
    "\n",
    "    # # ! My own bar plot --------------------------------------------------------\n",
    "    # # Make a barplot of feature against importance, color by dataset, add percentage of total importance of dataset in the legend\n",
    "    # # Get palette first\n",
    "    # palette = sns.color_palette(\"tab20\", len(rrr_of_dataset))\n",
    "    # dict_hue = dict(zip(rrr_of_dataset.dataset, palette))\n",
    "    # rrr_of_dataset[\"color\"] = rrr_of_dataset.dataset.map(dict_hue)\n",
    "\n",
    "    # # * By Feature Alone --------------------------------------------------------\n",
    "    # plt.figure(figsize=(8, max(math.ceil(rrr.shape[0] / 4), 4)))\n",
    "    # sns.barplot(\n",
    "    #     x=\"Importance\",\n",
    "    #     y=\"Feature\",\n",
    "    #     data=rrr.sort_values(by=\"Importance\", ascending=False),\n",
    "    #     hue=\"dataset_label\",\n",
    "    #     dodge=False,\n",
    "    #     palette=palette,\n",
    "    # )\n",
    "    # plt.xlabel(\"Relative Importance\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "    # plt.title(\n",
    "    #     f\"Feature Importance for {user_input['subset_group'][0]}\\n\",\n",
    "    #     fontsize=12,\n",
    "    #     fontdict={\"fontweight\": \"bold\"},\n",
    "    # )\n",
    "    # plt.savefig(user_input[\"current_dir\"] + \"/fig-vip-shap-by_feature.png\")\n",
    "    # plt.close()\n",
    "    # # plt.show()\n",
    "\n",
    "    # # * By Dataset Alone --------------------------------------------------------\n",
    "    # plt.figure(figsize=(8, max(math.ceil(rrr.shape[0] / 4), 4)))\n",
    "    # sns.barplot(\n",
    "    #     x=\"dataset_imp\",\n",
    "    #     y=\"dataset_label\",\n",
    "    #     data=rrr_of_dataset.sort_values(by=\"dataset_imp\", ascending=False),\n",
    "    #     hue=\"dataset_label\",\n",
    "    #     dodge=False,\n",
    "    #     palette=palette,\n",
    "    # )\n",
    "\n",
    "    # plt.xlabel(\"Relative Importance\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), frameon=False)\n",
    "    # plt.title(\n",
    "    #     f\"Dataset Importance for {user_input['subset_group'][0]}\\n\",\n",
    "    #     fontsize=12,\n",
    "    #     fontdict={\"fontweight\": \"bold\"},\n",
    "    # )\n",
    "    # plt.savefig(user_input[\"current_dir\"] + \"/fig-vip-shap-by_dataset.png\")\n",
    "    # plt.close()\n",
    "    # # plt.show()\n",
    "\n",
    "    # # * Both --------------------------------------------------------\n",
    "    # # Create a figure with two side-by-side subplots\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(16, max(math.ceil(rrr.shape[0] / 4), 4)))\n",
    "    # palette = sns.color_palette(\"tab20\", len(rrr[\"dataset_label\"].unique()))\n",
    "\n",
    "    # # Plot the first barplot\n",
    "    # sns.barplot(\n",
    "    #     x=\"Importance\",\n",
    "    #     y=\"Feature\",\n",
    "    #     data=rrr.sort_values(by=\"Importance\", ascending=False),\n",
    "    #     hue=\"dataset_label\",\n",
    "    #     dodge=False,\n",
    "    #     palette=palette,\n",
    "    #     ax=axs[0],  # Plot on the first subplot\n",
    "    # )\n",
    "    # axs[0].set_xlabel(\"Relative Importance\")\n",
    "    # axs[0].set_ylabel(\"\")\n",
    "    # # axs[0].legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    # axs[0].legend([], [], frameon=False)\n",
    "    # axs[0].set_title(\n",
    "    #     f\"Final Predictors\",\n",
    "    #     fontsize=12,\n",
    "    #     # fontweight=\"bold\",\n",
    "    # )\n",
    "\n",
    "    # # Plot the second barplot\n",
    "    # sns.barplot(\n",
    "    #     x=\"dataset_imp\",\n",
    "    #     y=\"dataset_label\",\n",
    "    #     data=rrr_of_dataset.sort_values(by=\"dataset_imp\", ascending=False),\n",
    "    #     hue=\"color\",\n",
    "    #     dodge=False,\n",
    "    #     palette=palette,\n",
    "    #     ax=axs[1],  # Plot on the second subplot\n",
    "    # )\n",
    "\n",
    "    # axs[1].set_xlabel(\"Relative Importance\")\n",
    "    # axs[1].set_ylabel(\"\")\n",
    "    # axs[1].legend([], [], frameon=False)\n",
    "    # axs[1].set_title(\n",
    "    #     f\"Datasets\",\n",
    "    #     fontsize=12,\n",
    "    #     # fontweight=\"bold\",\n",
    "    # )\n",
    "\n",
    "    # # Adjust layout and display the figure\n",
    "    # fig.suptitle(f\"{user_input['subset_group'][0]}\", fontsize=14, fontweight=\"bold\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(user_input[\"current_dir\"] + \"/fig-vip-shap-both.png\")\n",
    "    # plt.close()\n",
    "    # # plt.show()\n",
    "    # * ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # ! SAVE IT ALL --------------------------------------------------------\n",
    "    # General information\n",
    "    df_save = pd.DataFrame(\n",
    "        {\n",
    "            \"subset\": [user_input[\"subset\"][0]],\n",
    "            \"subset_group\": [user_input[\"subset_group\"][0]],\n",
    "            \"created\": [datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")],\n",
    "            \"best_model_decision\": [user_input[\"best_model_decision\"]],\n",
    "            \"N_died\": [df_target.target.sum()],\n",
    "            \"N_surv\": [df_target.shape[0] - df_target.target.sum()],\n",
    "            \"dir\": [user_input[\"current_dir\"]],\n",
    "            \"oversampled_cv\": [user_input[\"do_smote_test_validation\"]],\n",
    "            \"oversampled_test\": [user_input[\"do_smote_test_final\"]],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Final model metrics\n",
    "    df_save = pd.concat(\n",
    "        [df_save, pd.read_csv(f\"{current_dir}/classification_metrics.csv\")], axis=1\n",
    "    )\n",
    "\n",
    "    # Feature information\n",
    "    # for dataset in rrr.dataset.unique():\n",
    "    #     df_save[f\"{dataset} - Importance\"] = rrr.loc[\n",
    "    #         rrr.dataset == dataset, \"dataset_imp\"\n",
    "    #     ].values[0]\n",
    "    #     df_save[f\"{dataset} - Metrics\"] = [\n",
    "    #         rrr.loc[rrr.dataset == dataset, \"Feature\"].values\n",
    "    #     ]\n",
    "    #     df_save[f\"{dataset} - Values\"] = [\n",
    "    #         rrr.loc[rrr.dataset == dataset, \"Importance\"].values\n",
    "    #     ]\n",
    "\n",
    "    df_save.to_csv(f\"{current_dir}/final_model_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {}\n",
    "\n",
    "# ! TRAINING -----------------------------------------------------------------------\n",
    "# General\n",
    "user_input[\"seed_nr\"] = 42\n",
    "user_input[\"test_split\"] = 0.2\n",
    "user_input[\"current_dir\"] = None\n",
    "# Feature Elimination\n",
    "user_input[\"do_ref\"] = False\n",
    "user_input[\"cv_folds\"] = 5\n",
    "user_input[\"method_validation\"] = \"oob\"  # none | cv | oob\n",
    "user_input[\"method_importance\"] = \"impurity\"  # permutation | impurity\n",
    "user_input[\"do_tuning\"] = False  # Tune during rfe validation?\n",
    "user_input[\"correlation_threshold\"] = 0.8  # 🚨 CURRENTLY AT 1 == NO REMOVAL!\n",
    "user_input[\"n_features_in_final_model\"] = 15\n",
    "# SMOTE\n",
    "user_input[\"do_smote_test_validation\"] = False\n",
    "user_input[\"do_smote_test_final\"] = False\n",
    "# Tuning\n",
    "user_input[\"do_prescribed_search\"] = True\n",
    "user_input[\"do_random_search\"] = False\n",
    "user_input[\"gsc_metric\"] = \"roc_auc\"  # grid search metric\n",
    "# ! Final model ---------------------------------------------------------------------------------\n",
    "# best_per_category | best_metric | best_metric_max1\n",
    "user_input[\"best_model_decision\"] = \"best_per_category\"\n",
    "user_input[\"best_model_metric\"] = \"roc_auc\"\n",
    "user_input[\"prob_threshold\"] = 0.4  # Classification threshold\n",
    "# ! SHAP ---------------------------------------------------------------------------------\n",
    "user_input[\"shap_on_test_or_train\"] = \"test\"  # test | train\n",
    "user_input[\"use_all_shap_data\"] = False  # Whether to use all shap data\n",
    "user_input[\"run_shap_interaction\"] = True  # Whether to run shap interactions\n",
    "# ! Output ---------------------------------------------------------------------------------\n",
    "user_input[\"dir_suffix\"] = None  # None or string\n",
    "user_input[\"description_file\"] = \"ADD_DESCRIPTION\"  # None or string\n",
    "user_input[\"subset\"] = [\"species_lat2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Code to find where error occured in loop\n",
    "# for ii, ss in enumerate(species.index):\n",
    "#     if ss in [\n",
    "#         # \"Fagus sylvatica\",\n",
    "#         \"Alnus incana\",\n",
    "#         # \"Buxus sempervirens\",\n",
    "#         # \"Pinus sylvestris\",\n",
    "#         # \"Fraxinus excelsior\",\n",
    "#         # \"Betula pendula\",\n",
    "#         # \"Quercus robur\",\n",
    "#         # \"Quercus pubescens\",\n",
    "#     ]:\n",
    "#         display(f\"🟡🟡🟡 Running {ii}: {ss} 🟡🟡🟡\")\n",
    "#         run_all(ss, user_input)\n",
    "#         # chime.success()\n",
    "#         break\n",
    "#     # clear_output(wait=True)\n",
    "# # # chime.success()\n",
    "# # # chime.success()\n",
    "# # # chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Settings\n",
    "# run_name = \"oob + impurity + seed 24\"\n",
    "\n",
    "# user_input[\"seed_nr\"] = 24\n",
    "# user_input[\"method_importance\"] = \"impurity\"  # permutation | impurity\n",
    "# user_input[\"shap_on_test_or_train\"] = \"test\"  # test | train\n",
    "# user_input[\"use_all_shap_data\"] = False\n",
    "\n",
    "# # Run loop\n",
    "# base_dir = create_new_run_folder_treemort_fullrun(run_name)\n",
    "# st = start_time()\n",
    "# for i, s in enumerate(species.index):\n",
    "#     display(f\"🟡🟡🟡 {i}/{len(species)}: {s} 🟡🟡🟡\")\n",
    "#     if i > 20:\n",
    "#         print(f\" - Skipping {i}\")\n",
    "#         continue\n",
    "#     ist = start_time()\n",
    "#     run_all(s, user_input, base_dir=base_dir)\n",
    "#     clear_output(wait=True)\n",
    "#     end_time(ist, None)\n",
    "\n",
    "# end_time(st, user_input[\"current_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs\n",
    "all_seeds = pd.read_csv(\"allOriginalSeeds.csv\").seed.tolist()\n",
    "all_species = species.index.tolist()\n",
    "all_runs = pd.DataFrame(\n",
    "    list(itertools.product(all_seeds, all_species)),\n",
    "    columns=[\"seed\", \"species\"],\n",
    ")\n",
    "all_runs[\"dir\"] = \"\"\n",
    "all_runs[\"done\"] = False\n",
    "\n",
    "\n",
    "# Loop over all runs and check if that run has been completed\n",
    "for i, row in all_runs.iterrows():\n",
    "    # Get folder matching the seed\n",
    "    seed = row.seed\n",
    "    base_dir = glob.glob(f\"./model_runs/_fullruns/* {seed} +*\")\n",
    "\n",
    "    if len(base_dir) == 0:\n",
    "        print(f\" - No folder found for seed: {seed}\")\n",
    "        continue\n",
    "    else:\n",
    "        base_dir = base_dir[0]\n",
    "        all_runs.loc[i, \"dir\"] = base_dir\n",
    "\n",
    "    if os.path.isfile(f\"./{base_dir}/{row.species}/final_model_performance.csv\"):\n",
    "        all_runs.loc[i, \"done\"] = True\n",
    "    elif os.path.isfile(f\"{base_dir}/{row.species}/⚠️ too few dead trees.txt\"):\n",
    "        all_runs.loc[i, \"done\"] = True\n",
    "    else:\n",
    "        all_runs.loc[i, \"done\"] = False\n",
    "\n",
    "# Get missing runs\n",
    "runs_to_run = (\n",
    "    all_runs.query(\"done == False\")\n",
    "    .sort_values([\"species\", \"seed\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ! Debug for running on multiple notebooks\n",
    "l_runs_to_run = split_df_into_list_of_group_or_ns(runs_to_run, \"seed\", 5)\n",
    "runs_to_run = l_runs_to_run[0].reset_index(drop=True)\n",
    "\n",
    "# Loop over all runs and check if that run has been completed\n",
    "for i, row in runs_to_run.iterrows():\n",
    "\n",
    "    iseed = row.seed\n",
    "    ispecies = row.species\n",
    "    idir = row.dir\n",
    "\n",
    "    if idir == \"\":\n",
    "        # Create folder for run\n",
    "        run_name = f\"impurity + slow selection + {iseed} + correlation removal\"\n",
    "        idir = create_new_run_folder_treemort_fullrun(run_name)\n",
    "        all_runs.loc[i, \"dir\"] = idir\n",
    "\n",
    "    # Start run\n",
    "    user_input[\"seed_nr\"] = iseed\n",
    "    display(\"\")\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        --------------------------------------------------------------------------------\n",
    "        Run {i}/{runs_to_run.shape[0]}\n",
    "        Seed: {iseed}\n",
    "        Species: {ispecies}\n",
    "        Dir: {idir}\n",
    "        Started: {datetime.datetime.now().strftime('%Y-%m-%d @ %H:%M:%S')}\n",
    "        --------------------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "    )\n",
    "    ist = start_time(False)\n",
    "    run_all(ispecies, user_input, base_dir=idir)\n",
    "    clear_output(wait=True)\n",
    "    end_time(ist, None, ring=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! osascript -e 'tell app \"System Events\" to shut down'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! Code snippet for parallel processing. Reduce allOriginals to non-completed seeds\n",
    "# # ! --------------------------------------------\n",
    "# # Remove completed seeds\n",
    "# completed_seeds = [\n",
    "#     1220,\n",
    "#     1221,\n",
    "#     1222,\n",
    "#     1223,\n",
    "#     1224,\n",
    "#     1225,\n",
    "#     1226,\n",
    "#     1227,\n",
    "#     1228,\n",
    "#     19991,\n",
    "#     19992,\n",
    "#     19993,\n",
    "#     19994,\n",
    "#     542,\n",
    "#     569,\n",
    "#     61,\n",
    "#     610000,\n",
    "#     612,\n",
    "#     642,\n",
    "#     669,\n",
    "#     91,\n",
    "#     910000,\n",
    "#     912,\n",
    "#     91996,\n",
    "#     942,\n",
    "#     969,\n",
    "#     9991,\n",
    "#     9992,\n",
    "#     9993,\n",
    "#     9994,\n",
    "#     9995,\n",
    "#     9996,\n",
    "#     9997,\n",
    "#     9998,\n",
    "# ]\n",
    "# allOriginalSeeds = [x for x in allOriginalSeeds if x not in completed_seeds]\n",
    "# allOriginalSeeds = allOriginalSeeds[0:3]\n",
    "# # ! --------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SHAP Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "from imports import *\n",
    "\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models and species\n",
    "final_species = get_species_with_models(\"list\")\n",
    "\n",
    "top9 = final_species[:9]\n",
    "\n",
    "base_dir = \"./model_runs/_fullruns/\"\n",
    "models_dir = os.listdir(base_dir)\n",
    "models_dir = [m for m in models_dir if \"impurity\" in m]\n",
    "models_dir = sorted(models_dir)\n",
    "\n",
    "# Merge species and model lists into one df\n",
    "models_species = list(itertools.product(models_dir, final_species))\n",
    "df_in = pd.DataFrame(models_species, columns=[\"model\", \"species\"])\n",
    "df_in[\"seed\"] = df_in[\"model\"].apply(lambda x: x.split(\" + \")[2].split(\" +\")[0])\n",
    "display(df_in)\n",
    "\n",
    "print(f\"Number of models: {df_in.model.nunique()}\")\n",
    "print(f\"Number of species: {df_in.species.nunique()}\")\n",
    "print(f\"Number of seeds: {df_in.seed.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which runs are actually missing\n",
    "missing_runs = []\n",
    "for i, row in df_in.iterrows():\n",
    "    idir = f\"./model_runs/_fullruns/{row['model']}/{row['species']}/final_model_performance.csv\"\n",
    "    if not os.path.isfile(idir):\n",
    "        # print(idir)\n",
    "        missing_runs.append(row)\n",
    "\n",
    "df_missing = pd.DataFrame(missing_runs)\n",
    "print(f\"{df_missing.shape[0]} missing runs\")\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_run_new_loop_mp(\n",
    "    df_in,\n",
    "    run_interaction=False,\n",
    "    approximate=True,\n",
    "    test_or_train=\"test\",\n",
    "    force_run=True,\n",
    "    verbose=False,\n",
    "    num_cores=9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! osascript -e 'tell app \"System Events\" to shut down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP - Variable Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos\n",
    "# - ADD SKIP IF MODEL PERFORMANCE FILE IS NOT AVAILABLE! SIMPLE CHECK IF MODEL HAS BEEN RUN\n",
    "# - Could add possibility to use train data instead of test but we are generally focusing on test anyways...\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Load the models and species\n",
    "final_species = get_species_with_models(\"list\")\n",
    "\n",
    "top9 = final_species[:9]\n",
    "\n",
    "base_dir = \"./model_runs/_fullruns/\"\n",
    "models_dir = os.listdir(base_dir)\n",
    "models_dir = [m for m in models_dir if \"impurity\" in m]\n",
    "models_dir = sorted(models_dir)\n",
    "\n",
    "# Merge species and model lists into one df\n",
    "models_species = list(itertools.product(models_dir, final_species))\n",
    "df_in = pd.DataFrame(models_species, columns=[\"model\", \"species\"])\n",
    "\n",
    "df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over runs and species and calculate mean absolute SHAP values\n",
    "for i, row in tqdm(df_in.iterrows(), total=df_in.shape[0]):\n",
    "    # Get predictor data\n",
    "    ipreds = f\"./model_runs/_fullruns/{row.model}/{row.species}/final_model/X_test.csv\"\n",
    "    ipreds = pd.read_csv(ipreds, index_col=[0])\n",
    "\n",
    "    # Get SHAP data\n",
    "    ishap = f\"./model_runs/_fullruns/{row.model}/{row.species}/new_shap/approximated/shap_values_test.pkl\"\n",
    "    if not os.path.exists(ishap):\n",
    "        raise ValueError(\n",
    "            f\" 🚨 Skipping {row.model}/{row.species} because no SHAP values calculated yet!\"\n",
    "        )\n",
    "    ishap = load_shap(ishap)\n",
    "\n",
    "    # Extract SHAP values per prediction (saved in third dimension)\n",
    "    ishap = ishap.values[:, :, 1]\n",
    "\n",
    "    # Get the row of SHAP values to have a basis to add to\n",
    "    ishapAll = pd.DataFrame(ishap[0].tolist()).T\n",
    "\n",
    "    # Give the df the correct predictor names\n",
    "    ishapAll.columns = ipreds.columns\n",
    "\n",
    "    # Loop over all SHAP predictions and concatenate\n",
    "    for j in range(1, len(ishap)):\n",
    "        iii = pd.DataFrame(ishap[j].tolist()).T\n",
    "        iii.columns = ipreds.columns\n",
    "        ishapAll = pd.concat([ishapAll, iii], axis=0, ignore_index=True)\n",
    "\n",
    "    # Safety check: Shape of predictors should be the same as for SHAP values\n",
    "    if ipreds.shape != ishapAll.shape:\n",
    "        print(\n",
    "            f\" - Issue: The shape of the predictor data should equal the shape of the concatenated SHAP values!\"\n",
    "        )\n",
    "\n",
    "    # Take mean of SHAP values across all variables\n",
    "    ishapMean_org = ishapAll.abs().mean().sort_values(ascending=False)\n",
    "    ishapMean = ishapMean_org / ishapMean_org.sum()\n",
    "    ishapMean = pd.DataFrame(ishapMean)\n",
    "    ishapMean.columns = [\"Importance\"]\n",
    "    ishapMean.Importance = ishapMean.Importance * 100\n",
    "    ishapMean[\"Feature\"] = ishapMean.index\n",
    "    ishapMean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Link feature variable to predictor dataset in new column\n",
    "    # Load predictor dictionary\n",
    "    dict_preds = json.load(open(f\"./model_analysis/dict_preds.json\"))\n",
    "    for f in ishapMean.Feature:\n",
    "        for key, value in dict_preds.items():\n",
    "            if f in value:\n",
    "                ishapMean.loc[ishapMean.Feature == f, \"dataset\"] = key\n",
    "\n",
    "    # Sum up the VI for each dataset\n",
    "    ishapMean_of_dataset = (\n",
    "        ishapMean[[\"Importance\", \"dataset\"]]\n",
    "        .groupby(\"dataset\")\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename({\"Importance\": \"dataset_imp\"}, axis=1)\n",
    "    )\n",
    "\n",
    "    ishapMean_of_dataset.dataset_imp = (\n",
    "        ishapMean_of_dataset.dataset_imp / ishapMean_of_dataset.dataset_imp.sum() * 100\n",
    "    )\n",
    "\n",
    "    ishapMean[\"mean_abs_shap_org\"] = ishapMean_org.values\n",
    "\n",
    "    # Attach dataset label with percentages\n",
    "    for j, jrow in ishapMean_of_dataset.iterrows():\n",
    "        # ishapMean_of_dataset.loc[j, \"dataset_label\"] = (\n",
    "        #     ishapMean_of_dataset.loc[j, \"dataset\"]\n",
    "        #     + \"  (\"\n",
    "        #     + str(round(ishapMean_of_dataset.loc[j, \"dataset_imp\"]))\n",
    "        #     + \"%)\"\n",
    "        # )\n",
    "        ishapMean_of_dataset.loc[j, \"dataset_label\"] = (\n",
    "            str(round(ishapMean_of_dataset.loc[j, \"dataset_imp\"]))\n",
    "            + \"%: \"\n",
    "            + ishapMean_of_dataset.loc[j, \"dataset\"]\n",
    "        )\n",
    "\n",
    "    ishapMean = ishapMean.merge(ishapMean_of_dataset, on=\"dataset\", how=\"left\")\n",
    "\n",
    "    # Save SHAP data\n",
    "    ishapMean.to_csv(\n",
    "        f\"./model_runs/_fullruns/{row.model}/{row.species}/shap_variable_importance.csv\"\n",
    "    )\n",
    "\n",
    "    # Load final model performance\n",
    "    ifinalOrg = f\"./model_runs/_fullruns/{row.model}/{row.species}/final_model_performance_org.csv\"\n",
    "    ifinalNew = (\n",
    "        f\"./model_runs/_fullruns/{row.model}/{row.species}/final_model_performance.csv\"\n",
    "    )\n",
    "\n",
    "    # If the original file has not yet been backuped, save it!\n",
    "    if not os.path.exists(ifinalOrg):\n",
    "        shutil.copy2(ifinalNew, ifinalOrg)\n",
    "\n",
    "    # Load model performance file, attach SHAP information and save it again\n",
    "    ifinalNewDf = pd.read_csv(ifinalNew)\n",
    "\n",
    "    for dataset in ishapMean.dataset.unique():\n",
    "        ifinalNewDf[f\"{dataset} - Importance\"] = ishapMean.loc[\n",
    "            ishapMean.dataset == dataset, \"dataset_imp\"\n",
    "        ].values[0]\n",
    "        ifinalNewDf[f\"{dataset} - Metrics\"] = [\n",
    "            ishapMean.loc[ishapMean.dataset == dataset, \"Feature\"].values\n",
    "        ]\n",
    "        ifinalNewDf[f\"{dataset} - Values\"] = [\n",
    "            ishapMean.loc[ishapMean.dataset == dataset, \"Importance\"].values\n",
    "        ]\n",
    "\n",
    "    ifinalNewDf.to_csv(ifinalNew, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! osascript -e 'tell app \"System Events\" to shut down'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
